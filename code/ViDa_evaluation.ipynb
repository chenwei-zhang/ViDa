{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pc\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import phate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepared embedded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMS_T (621984,)\n",
      "SIMS_HT (621984,)\n",
      "SIMS_HT_uniq (46606,)\n",
      "SIMS_adj_uniq (46606, 50, 50)\n",
      "SIMS_scar_uniq (46606, 4000)\n",
      "SIMS_G_uniq (46606,)\n",
      "SIMS_pair_uniq (46606,)\n",
      "SIMS_dict (621984, 5)\n",
      "SIMS_dict_uniq (46606, 4)\n",
      "coord_id_S (621984,)\n",
      "indices_S (46606,)\n",
      "trj_id (100,)\n",
      "occ_density_S (46606,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load saved trajectories data for npz file\n",
    "\"\"\"\n",
    "SEQ = \"PT4\"\n",
    "# SEQ = \"PT4_hairpin\"\n",
    "\n",
    "# laod pre-training data\n",
    "fnpz_data = \"./data/pretraining/pretraining_{}.npz\".format(SEQ)\n",
    "\n",
    "data_npz = np.load(fnpz_data)\n",
    "\n",
    "# asssign data to variables\n",
    "for var in data_npz.files:\n",
    "     locals()[var] = data_npz[var]\n",
    "     print(var, locals()[var].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MDS data\n",
    "mds_data = \"./data/vida_data/PT4_mds.npz\"\n",
    "mds_coords = np.load(mds_data)[\"mds_coords\"]\n",
    "mds_all_coords = np.load(mds_data)[\"mds_all_coords\"]\n",
    "mds_coords.shape, mds_all_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j = np.array(np.load(f'./data/graph/{SEQ}/shortestpath_knn=100.npz',allow_pickle=True)[\"X_j\"], dtype=int)\n",
    "D_ij = np.array(np.load(f'./data/graph/{SEQ}/shortestpath_knn=100.npz',allow_pickle=True)[\"D_ij\"], dtype=float)\n",
    "print(\"X_j\", X_j.shape)\n",
    "print(\"D_ij\", D_ij.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability of being visited during a simulated trajectory \n",
    "# from the initial state\n",
    "split_id = trj_id + 1 # index for split to each trajectory\n",
    "P_tot = np.zeros(len(SIMS_dict_uniq))\n",
    "\n",
    "for i in range(len(split_id)):\n",
    "    if i == 0:\n",
    "        trj = set(SIMS_dict[0:split_id[i],4].astype(int))\n",
    "    else:\n",
    "        trj = set(SIMS_dict[split_id[i-1]:split_id[i],4].astype(int))\n",
    "\n",
    "    P_tot[list(trj)] += 1\n",
    "\n",
    "P_tot = P_tot / 100\n",
    "\n",
    "print(\"P_tot\", P_tot.shape, P_tot.max(), P_tot.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "# load embedding WITHOUT vida plot data\n",
    "# SEQ = \"PT0\"\n",
    "fnpz_noViDa = f\"./data/vida_data/noViDa-noEnergy/{SEQ}_noViDa.npz\"\n",
    "data_noViDa = np.load(fnpz_noViDa,allow_pickle=True)\n",
    "for var in data_noViDa.files:\n",
    "    globals()[var] = data_noViDa[var]\n",
    "    print(var, globals()[var].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_embed (46606, 25)\n",
      "pca_coords (46606, 3)\n",
      "pca_all_coords (621984, 3)\n",
      "phate_coords (46606, 2)\n",
      "phate_all_coords (621984, 2)\n",
      "data_embed: 14.681322 -8.626003 3.143037e-10 0.9999999\n"
     ]
    }
   ],
   "source": [
    "# load ViDa embedding plot data\n",
    "\n",
    "# fnpz_data_embed = f\"./data/vida_data/{SEQ}.npz\"\n",
    "fnpz_data_embed = f\"./data/vida_data/{SEQ}_0823-0138.npz\"\n",
    "# fnpz_data_embed = f\"./data/vida_data/{SEQ}_usePT4_03040216.npz\"\n",
    "data_npz_embed = np.load(fnpz_data_embed,allow_pickle=True)\n",
    "\n",
    "# asssign data to variables\n",
    "for var in data_npz_embed.files:\n",
    "    globals()[var] = data_npz_embed[var]\n",
    "    print(var, globals()[var].shape)\n",
    "print(\"data_embed:\", data_embed.max(), data_embed.min(), data_embed.mean(), data_embed.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ViDa embedding plot data from Fist Step data\n",
    "\n",
    "fnpz_data = \"./data/jordan/pretraining/npstates5m_39.npz\"\n",
    "data_npz = np.load(fnpz_data)\n",
    "\n",
    "fnpz_data_embed = f\"./data/jordan/plotdata/plot_0817-2136_npstates5m_39.npz\"\n",
    "data_npz_embed = np.load(fnpz_data_embed,allow_pickle=True)\n",
    "\n",
    "# asssign data to variables\n",
    "for var in data_npz_embed.files:\n",
    "    globals()[var] = data_npz_embed[var]\n",
    "    print(var, globals()[var].shape)\n",
    "print(\"data_embed:\", data_embed.max(), data_embed.min(), data_embed.mean(), data_embed.std())\n",
    "\n",
    "pca_all_coords = pca_coords[coord_id_S]\n",
    "phate_all_coords = phate_coords[coord_id_S]\n",
    "\n",
    "pca_all_coords.shape, phate_all_coords.shape, coord_id_S.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMS_T (621984,)\n",
      "SIMS_HT (621984,)\n",
      "SIMS_HT_uniq (46606,)\n",
      "SIMS_adj_uniq (46606, 50, 50)\n",
      "SIMS_scar_uniq (46606, 4000)\n",
      "SIMS_G_uniq (46606,)\n",
      "SIMS_pair_uniq (46606,)\n",
      "SIMS_dict (621984, 5)\n",
      "SIMS_dict_uniq (46606, 4)\n",
      "coord_id_S (621984,)\n",
      "indices_S (46606,)\n",
      "trj_id (100,)\n",
      "data_embed (46606, 25)\n",
      "occ_density_S (46606,)\n",
      "pca_coords (46606, 3)\n",
      "pca_all_coords (621984, 3)\n",
      "phate_coords (46606, 2)\n",
      "phate_all_coords (621984, 2)\n",
      "umap_coord_2d (46606, 2)\n",
      "umap_all_coord_2d (621984, 2)\n",
      "umap_coord_3d (46606, 3)\n",
      "umap_all_coord_3d (621984, 3)\n",
      "tsne_coord_2d (46606, 2)\n",
      "tsne_all_coord_2d (621984, 2)\n",
      "tsne_coord_3d (46606, 3)\n",
      "tsne_all_coord_3d (621984, 3)\n",
      "data_embed 29.09028 -15.852685 -0.025475224 1.352891\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "# load GSAE embedding plot data\n",
    "\n",
    "fnpz_data_embed = f\"./data/gsae_data/helix_assoc_{SEQ}.npz\"\n",
    "# fnpz_data_embed = f\"./data/vida_data/noViDa-noEnergy/{SEQ}_noViDa.npz\"\n",
    "\n",
    "\n",
    "data_npz_embed = np.load(fnpz_data_embed,allow_pickle=True)\n",
    "# asssign data to variables\n",
    "for var in data_npz_embed.files:\n",
    "    globals()[var] = data_npz_embed[var]\n",
    "    print(var, globals()[var].shape)\n",
    "print(\"data_embed\", data_embed.max(), data_embed.min(), data_embed.mean(), data_embed.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_1trj_distance(arr):\n",
    "    total_distance = []\n",
    "    \n",
    "    for i in range(1, len(arr)):\n",
    "        total_distance.append(np.sqrt(np.sum((arr[i] - arr[i-1])**2)))\n",
    "        \n",
    "    return total_distance\n",
    "\n",
    "\n",
    "# get all edges and distance for each trajectory, and flatten the list\n",
    "def get_edge_dist(trj_id, SIMS_dict, embedding):\n",
    "    # trajec_embed = []\n",
    "    TRJ_ID = trj_id+1\n",
    "    all_edges = []\n",
    "    all_dist = []\n",
    "    scaler = MinMaxScaler()\n",
    "    embedding = scaler.fit_transform(embedding)\n",
    "    \n",
    "    for i in range(len(TRJ_ID)):\n",
    "        if i == 0:\n",
    "            s = 0\n",
    "            s_prime = TRJ_ID[i]\n",
    "        elif i == len(trj_id):\n",
    "            s = TRJ_ID[i-1]\n",
    "            s_prime = len(embedding)\n",
    "        else:\n",
    "            s = TRJ_ID[i-1]\n",
    "            s_prime = TRJ_ID[i]\n",
    "        \n",
    "        single_trajec_embed = embedding[s:s_prime][:,:2]\n",
    "        distance_list = calculate_1trj_distance(single_trajec_embed)\n",
    "        \n",
    "        # trajec_embed.append(single_trajec_embed)\n",
    "        \n",
    "        nodes = np.array(SIMS_dict[s:s_prime][:,-1], dtype=int)\n",
    "        edges_single = []\n",
    "        for previous, current in zip(nodes, nodes[1:]):\n",
    "            edges_single.append([previous, current])   \n",
    "        \n",
    "        edges_dist_single = [a + [b] for a, b in zip(edges_single, distance_list)]\n",
    "\n",
    "        all_edges.append(edges_dist_single)\n",
    "        \n",
    "        flatten_edges = [item for sublist in all_edges for item in sublist]\n",
    "    \n",
    "    return flatten_edges\n",
    "    \n",
    "\n",
    "# remove duplicate edges and calculate the average smoothness\n",
    "def smoothness(trj_id, SIMS_dict, phate_all_coords):\n",
    "    \n",
    "    flatten_edges = get_edge_dist(trj_id, SIMS_dict, phate_all_coords)\n",
    "    \n",
    "    # unique_set = set(map(tuple, flatten_edges))\n",
    "    # unique_list = [list(item) for item in unique_set]\n",
    "\n",
    "    # # Create a dictionary to store the values\n",
    "    # uni_values = {}\n",
    "\n",
    "    # for sublist in unique_list:\n",
    "    #     key = tuple(sorted(sublist[:2]))\n",
    "    #     # if key not in uni_values or sublist[2] < uni_values[key][2]:\n",
    "    #     if key not in uni_values:\n",
    "    #         uni_values[key] = sublist\n",
    "\n",
    "    # cleaned_list = list(uni_values.values())\n",
    "    # len(cleaned_list)\n",
    "    \n",
    "    unique_set = set(map(tuple, flatten_edges))\n",
    "    unique_list = [list(item) for item in unique_set]\n",
    "\n",
    "    total_sum = sum(sublist[-1] for sublist in unique_list)\n",
    "    average = total_sum / len(unique_list)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViDa PCA: 0.043\n",
      "ViDa PHATE: 0.020\n"
     ]
    }
   ],
   "source": [
    "print(f\"ViDa PCA: {smoothness(trj_id, SIMS_dict, pca_all_coords):.3f}\")\n",
    "print(f\"ViDa PHATE: {smoothness(trj_id, SIMS_dict, phate_all_coords):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSAE PCA: 0.046\n",
      "GSAE PHATE: 0.021\n"
     ]
    }
   ],
   "source": [
    "print(f\"GSAE PCA: {smoothness(trj_id, SIMS_dict, pca_all_coords):.3f}\")\n",
    "print(f\"GSAE PHATE: {smoothness(trj_id, SIMS_dict, phate_all_coords):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MDS: {smoothness(trj_id, SIMS_dict, mds_all_coords):.3f}\")\n",
    "print(f\"Direct PCA: {smoothness(trj_id, SIMS_dict, pca_all_coords_direct):.3f}\")\n",
    "print(f\"Direct PHATE: {smoothness(trj_id, SIMS_dict, phate_all_coords_direct):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_list = flatten_edges\n",
    "# unique_set = set()\n",
    "# deleted_indices = []\n",
    "\n",
    "# for i, sublist in enumerate(original_list):\n",
    "#     sorted_sublist = tuple(sorted(sublist))\n",
    "#     if sorted_sublist in unique_set:\n",
    "#         deleted_indices.append(i)\n",
    "#     else:\n",
    "#         unique_set.add(sorted_sublist)\n",
    "\n",
    "# unique_list = [list(sublist) for sublist in unique_set]\n",
    "\n",
    "# print(\"Unique List:\", unique_list)\n",
    "# print(\"Deleted Indices:\", deleted_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freq_weighted_distortion(embedding, trj_id):\n",
    "\n",
    "    def split_embedding(trj_id, embedding):\n",
    "        traj_embed = []\n",
    "        split_indx = trj_id+1\n",
    "        \n",
    "        for i in range(len(split_indx)):\n",
    "            if i == 0:\n",
    "                s = 0\n",
    "                s_prime = split_indx[i]\n",
    "            elif i == len(trj_id):\n",
    "                s = split_indx[i-1]\n",
    "                s_prime = len(embedding)\n",
    "            else:\n",
    "                s = split_indx[i-1]\n",
    "                s_prime = split_indx[i]\n",
    "            \n",
    "            traj_embed.append(embedding[s:s_prime][:,:2])\n",
    "        \n",
    "        return traj_embed\n",
    "    \n",
    "    \n",
    "    def calculate_1trj_distance(arr):\n",
    "        total_distance = []\n",
    "        \n",
    "        for i in range(1, len(arr)):\n",
    "            total_distance.append(np.sqrt(np.sum((arr[i] - arr[i-1])**2)))\n",
    "            \n",
    "        return np.sum(total_distance)\n",
    "    \n",
    "    \n",
    "    # normalize embedding to [0,1]\n",
    "    norm_embedding = (embedding - np.min(embedding)) / (np.max(embedding) - np.min(embedding))\n",
    "    \n",
    "    # get split embeddings\n",
    "    traj_embed = split_embedding(trj_id, norm_embedding)\n",
    "    \n",
    "    # calculate distortion\n",
    "    total_distance = 0\n",
    "    \n",
    "    for i in range(len(traj_embed)):\n",
    "        total_distance += calculate_1trj_distance(traj_embed[i])        \n",
    "    \n",
    "    distortion = total_distance / (trj_id[-1]+1)\n",
    "    \n",
    "    return distortion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajec_embed(trj_id, embedding):\n",
    "    trajec_embed = []\n",
    "    TRJ_ID = trj_id+1\n",
    "    \n",
    "    for i in range(len(TRJ_ID)):\n",
    "        if i == 0:\n",
    "            s = 0\n",
    "            s_prime = TRJ_ID[i]\n",
    "        elif i == len(trj_id):\n",
    "            s = TRJ_ID[i-1]\n",
    "            s_prime = len(embedding)\n",
    "        else:\n",
    "            s = TRJ_ID[i-1]\n",
    "            s_prime = TRJ_ID[i]\n",
    "        \n",
    "        trajec_embed.append(embedding[s:s_prime][:,:2])\n",
    "    \n",
    "    return trajec_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_1trj_distance(arr):\n",
    "    total_distance = []\n",
    "    \n",
    "    for i in range(1, len(arr)):\n",
    "        total_distance.append(np.sqrt(np.sum((arr[i] - arr[i-1])**2)))\n",
    "        \n",
    "    return np.sum(total_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_total_distance(embedding, trj_id):\n",
    "# calculate the expected total distance over all trajectories\n",
    "    scaler = MinMaxScaler()\n",
    "    std_embedding = scaler.fit_transform(embedding) \n",
    "    \n",
    "    trajec_embed = get_trajec_embed(trj_id, std_embedding)\n",
    "    \n",
    "    total_distance = 0\n",
    "    \n",
    "    for i in range(len(trajec_embed)):\n",
    "        total_distance += calculate_1trj_distance(trajec_embed[i])        \n",
    "        \n",
    "    return total_distance / (trj_id[-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViDa PCA: 0.038\n",
      "ViDa PHATE: 0.019\n",
      "ViDa PCA: 0.029\n",
      "ViDa PHATE: 0.018\n"
     ]
    }
   ],
   "source": [
    "print(f\"ViDa PCA: {calculate_avg_total_distance(pca_all_coords, trj_id):.3f}\")\n",
    "print(f\"ViDa PHATE: {calculate_avg_total_distance(phate_all_coords, trj_id):.3f}\")\n",
    "\n",
    "print(f\"ViDa PCA: {freq_weighted_distortion(pca_all_coords, trj_id):.3f}\")\n",
    "print(f\"ViDa PHATE: {freq_weighted_distortion(phate_all_coords, trj_id):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MDS: {calculate_avg_total_distance(mds_all_coords, trj_id):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSAE PCA: 0.035\n",
      "GSAE PHATE: 0.030\n"
     ]
    }
   ],
   "source": [
    "print(f\"GSAE PCA: {calculate_avg_total_distance(pca_all_coords, trj_id):.3f}\")\n",
    "print(f\"GSAE PHATE: {calculate_avg_total_distance(phate_all_coords, trj_id):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Direct PCA: {calculate_avg_total_distance(pca_all_coords_direct, trj_id):.3f}\")\n",
    "print(f\"Direct PHATE: {calculate_avg_total_distance(phate_all_coords_direct, trj_id):.3f}\")\n",
    "# print(f\"Direct MDS: {calculate_avg_total_distance(mds_all_coords, trj_id):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_step_length(trj_id, embedding):\n",
    "    trajec_embed = get_trajec_embed(trj_id, embedding)\n",
    "    step_length = []\n",
    "    \n",
    "    for  i in range(len(trajec_embed)):\n",
    "        arr = trajec_embed[i]\n",
    "        for j in range(1, len(arr)):\n",
    "            step_length.append(np.sqrt(np.sum((arr[j] - arr[j-1])**2)))\n",
    "    return np.array(step_length)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_pdf(steps, num_bins):\n",
    "#     # Sort the data\n",
    "#     data = np.sort(steps)\n",
    "    \n",
    "#     # Group the data into the bins   \n",
    "#     counts, bin_edges = np.histogram(data, bins=num_bins)\n",
    "    \n",
    "#     # Calculate the density of data points in each bin\n",
    "#     density = counts / sum(counts)\n",
    "\n",
    "#     # Plot the PDF\n",
    "#     bin_widths = np.diff(bin_edges)\n",
    "#     bin_centers = bin_edges[:-1] + bin_widths / 2\n",
    "#     bin_centers = bin_centers / bin_centers.max() * 100\n",
    "\n",
    "#     plt.plot(bin_centers, density)\n",
    "#     plt.xticks(np.arange(0, 101, 10))\n",
    "#     plt.xlabel('Step length percentage (%)')\n",
    "#     plt.ylabel('Density')\n",
    "#     plt.title('PDF of step length for embedding')\n",
    "#     return plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pdf(steps, num_bins):\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    norm_steps = scaler.fit_transform(steps.reshape(-1,1))\n",
    "    \n",
    "    # Sort the data\n",
    "    data = np.sort(norm_steps)\n",
    "    \n",
    "    # Group the data into the bins   \n",
    "    make_pdf.counts, make_pdf.bin_edges = np.histogram(data, bins=num_bins)\n",
    "    \n",
    "    # Calculate the density of data points in each bin\n",
    "    make_pdf.density = make_pdf.counts / sum(make_pdf.counts)\n",
    "\n",
    "    # Plot the PDF\n",
    "    bin_widths = np.diff(make_pdf.bin_edges)\n",
    "    make_pdf.bin_centers = make_pdf.bin_edges[:-1] + bin_widths / 2\n",
    "    make_pdf.bin_centers = make_pdf.bin_centers / make_pdf.bin_centers.max() * 100\n",
    "\n",
    "    plt.plot(make_pdf.bin_centers, make_pdf.density)\n",
    "    plt.xticks(np.arange(0, 101, 10))\n",
    "    plt.xlabel('Step length percentage (%)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('PDF of step length for embedding')\n",
    "    return plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = calculate_step_length(trj_id, phate_all_coords)\n",
    "# steps = calculate_step_length(trj_id, phate_all_coords_direct)\n",
    "\n",
    "make_pdf(steps, num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pdf.bin_edges, make_pdf.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsae_phate = make_pdf.counts\n",
    "vida_pca, vida_phate, direct_pca, direct_phate, gsae_pca, gsae_phate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vida_pca, vida_phate, direct_pca, direct_phate, gsae_pca, gsae_phate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_widths = np.diff(make_pdf.bin_edges)\n",
    "bin_centers = make_pdf.bin_edges[:-1] + bin_widths / 2\n",
    "bin_centers = make_pdf.bin_centers / make_pdf.bin_centers.max() * 100\n",
    "bin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# save for plotting\n",
    "fnpz_data = f\"data/vida_data/pdf_plot_{SEQ}.npz\"\n",
    "with open(fnpz_data, 'wb') as f:\n",
    "    np.savez(f,\n",
    "            bin_centers=bin_centers,\n",
    "            vida_pca=vida_pca,\n",
    "            vida_phate=vida_phate,\n",
    "            gsae_pca=gsae_pca,\n",
    "            gsae_phate=gsae_phate,\n",
    "            direct_pca=direct_pca,\n",
    "            direct_phate=direct_phate,\n",
    "            # direct_mds=direct_mds,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a whole plot\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize =(15, 8))\n",
    "\n",
    "# Plot the lines\n",
    "ax.plot(bin_centers, vida_pca/sum(vida_pca), label='ViDa PCA')\n",
    "ax.plot(bin_centers, vida_phate/sum(vida_phate), label='ViDa PHATE')\n",
    "ax.plot(bin_centers, gsae_pca/sum(gsae_pca), label='GSAE PCA')\n",
    "ax.plot(bin_centers, gsae_phate/sum(gsae_phate), label='GSAE PHATE')\n",
    "ax.plot(bin_centers, direct_pca/sum(direct_pca), label='Direct PCA')\n",
    "ax.plot(bin_centers, direct_phate/sum(direct_phate), label='Direct PHATE')\n",
    "# ax.plot(bin_centers, direct_mds/sum(direct_mds), label='Direct MDS')\n",
    "\n",
    "# Set the legend\n",
    "ax.legend()\n",
    "\n",
    "plt.xticks(np.arange(0, 101, 10))\n",
    "plt.xlabel('Step length percentage (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f'PDF of step length for {SEQ}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = calculate_step_length(trj_id, pca_all_coords_direct)\n",
    "steps = calculate_step_length(trj_id, pca_all_coords)\n",
    "\n",
    "make_pdf(steps, num_bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsd(X_j, D_ij, P_tot, z):\n",
    "    \"\"\"\n",
    "    Metric to calculate the distance \n",
    "    \"\"\"\n",
    "    z_re = z.reshape(-1,1,z.shape[-1])\n",
    "    zj = z[X_j]\n",
    "    global l2_zizj\n",
    "    l2_zizj = np.sqrt(np.sum((z_re-zj)**2, axis=-1))\n",
    "    \n",
    "    # # normalize the distance\n",
    "    # scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "    # l2_zizj = scaler.fit_transform(l2_zizj)\n",
    "    # D_ij = scaler.fit_transform(D_ij)\n",
    "    \n",
    "    dist_diff = (l2_zizj - D_ij)**2\n",
    "    root = \n",
    "    wij = (P_tot.reshape(-1,1) * P_tot[X_j])\n",
    "    # dist_loss = np.sum(wij * dist_diff)\n",
    "    dist_loss = ((wij * dist_diff) * P_tot.reshape(-1,1) / P_tot.sum()).sum()\n",
    "    \n",
    "    \n",
    "    return dist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_dist(X_j, D_ij, P_tot, z):\n",
    "    \"\"\"\n",
    "    Metric to calculate the distance \n",
    "    \"\"\"\n",
    "    z_re = z.reshape(-1,1,z.shape[-1])\n",
    "    zj = z[X_j]\n",
    "    global l2_zizj\n",
    "    l2_zizj = np.sqrt(np.sum((z_re-zj)**2, axis=-1))\n",
    "    \n",
    "    # # normalize the distance\n",
    "    # scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "    # l2_zizj = scaler.fit_transform(l2_zizj)\n",
    "    # D_ij = scaler.fit_transform(D_ij)\n",
    "    \n",
    "    dist_diff = (l2_zizj - D_ij)**2\n",
    "    wij = (P_tot.reshape(-1,1) * P_tot[X_j])\n",
    "    # dist_loss = np.sum(wij * dist_diff)\n",
    "    dist_loss = ((wij * dist_diff) * P_tot.reshape(-1,1) / P_tot.sum()).sum()\n",
    "    \n",
    "    \n",
    "    return dist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Dij = np.log(D_ij)+np.abs(np.log(D_ij).min())\n",
    "print(\"log_Dij\", log_Dij.shape, log_Dij.max(), log_Dij.min())\n",
    "\n",
    "pca_dist = metric_dist(X_j, log_Dij, P_tot, pca_coords[:,:2])\n",
    "phate_dist = metric_dist(X_j, log_Dij, P_tot, phate_coords)\n",
    "print (f'ViDa PCA distance loss: {pca_dist:.4f}')\n",
    "print (f'ViDa PHATE distance loss: {phate_dist:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Dij = np.log(D_ij)+np.abs(np.log(D_ij).min())\n",
    "\n",
    "pca_dist = metric_dist(X_j, log_Dij, P_tot, pca_coords_direct[:,:2])\n",
    "phate_dist = metric_dist(X_j, log_Dij, P_tot, phate_coords_direct)\n",
    "print (f'Direct PCA distance loss: {pca_dist:.4f}')\n",
    "print (f'Direct PHATE distance loss: {phate_dist:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Dij = np.log(D_ij)+np.abs(np.log(D_ij).min())\n",
    "\n",
    "pca_dist = metric_dist(X_j, log_Dij, P_tot, pca_coords[:,:2])\n",
    "phate_dist = metric_dist(X_j, log_Dij, P_tot, pca_coords)\n",
    "print (f'GSAE PCA distance loss: {pca_dist:.4f}')\n",
    "print (f'GSAE Direct PHATE distance loss: {phate_dist:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighboring preservation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def neighboring_preservation_rate(X, X_j, P_tot, k):\n",
    "    \"\"\"\n",
    "    Metric to calculate the neighboring preservation rate \n",
    "    \"\"\"\n",
    "    # Compute the k-nearest neighbors for both X and Y.\n",
    "    nn_X = NearestNeighbors(n_neighbors=k+1).fit(X) # k+1 because we don't want to include the point itself\n",
    "    indices_X = nn_X.kneighbors(X,return_distance=False)[:,1:] # exclude the point itself\n",
    "    \n",
    "    # compute the rate of each point\n",
    "    rate_list = []\n",
    "    for i in range(len(indices_X)):\n",
    "        count = len(np.intersect1d(indices_X[i], X_j[i,:k]))\n",
    "        rate_i = count/k\n",
    "        rate_list.append(rate_i)\n",
    "    \n",
    "    # Compute the overall neighbsoring preservation rate\n",
    "    \n",
    "    # rate = np.mean(rate_list) # average\n",
    "    rate = (rate_list * P_tot / P_tot.sum()).sum() # weighted average\n",
    "    \n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViDa embedding\n",
    "knn = 100\n",
    "print(\"ViDa PCA rate: {:.4f}\".format(neighboring_preservation_rate(pca_coords[:,:2], X_j_all, P_tot, k=knn)))\n",
    "print(\"ViDa PHATE rate: {:.4f}\".format(neighboring_preservation_rate(phate_coords, X_j_all, P_tot, k=knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained ViDa embedding\n",
    "knn = 10000\n",
    "print(\"Pretrained ViDa PCA rate: {:.4f}\".format(neighboring_preservation_rate(pca_coords[:,:2], X_j_all, P_tot, k=knn)))\n",
    "print(\"Pretrained ViDa PHATE rate: {:.4f}\".format(neighboring_preservation_rate(phate_coords, X_j_all, P_tot, k=knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSAE embedding\n",
    "knn = 10000\n",
    "print(\"GSAE PCA rate: {:.4f}\".format(neighboring_preservation_rate(pca_coords[:,:2], X_j_all, P_tot, k=knn)))\n",
    "print(\"GSAE PHATE rate: {:.4f}\".format(neighboring_preservation_rate(phate_coords, X_j_all, P_tot, k=knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct embedding\n",
    "knn = 100\n",
    "print(\"Direct PCA rate: {:.4f}\".format(neighboring_preservation_rate(pca_coords_direct[:,:2], X_j, P_tot, k=knn)))\n",
    "print(\"Direct PHATE rate: {:.4f}\".format(neighboring_preservation_rate(phate_coords_direct, X_j, P_tot, k=knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_ij_all = []\n",
    "# X_j_all = []\n",
    "\n",
    "# for i in range(len(SIMS_HT_uniq)):\n",
    "#     dij = np.array(list(np.load(f'./data/graph/{SEQ}/allpath_{SEQ}/path_{i}.npy',allow_pickle=True)[0].values()), dtype=float)\n",
    "#     xj = np.array(list(np.load(f'./data/graph/{SEQ}/allpath_{SEQ}/path_{i}.npy',allow_pickle=True)[0].keys()), dtype=int)\n",
    "    \n",
    "#     D_ij_all.append(dij)\n",
    "#     X_j_all.append(xj)\n",
    "\n",
    "# D_ij_all = np.stack(D_ij_all)\n",
    "# X_j_all = np.stack(X_j_all)\n",
    "\n",
    "# # save npz file for shortest path\n",
    "# with open(f'./data/graph/{SEQ}_allpath.npz', 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#              X_j_all = np.stack(X_j_all),\n",
    "#              D_ij_all = np.stack(D_ij_all),\n",
    "#          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = PCA(n_components=25)\n",
    "cm.fit(data_embed)\n",
    "\n",
    "PC_values = np.arange(cm.n_components_) + 1\n",
    "plt.plot(PC_values, np.cumsum(cm.explained_variance_ratio_), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot: PCA')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "# plt.xticks(np.arange(0, data_embed.shape[-1]+1, 1))\n",
    "plt.show()\n",
    "\n",
    "print(np.cumsum(cm.explained_variance_ratio_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS for distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = \"PT4\"\n",
    "# PT4_hairpin all path\n",
    "X_j_all = np.array(np.load(f'./data/graph/{SEQ}/{SEQ}_allpath.npz',allow_pickle=True)[\"X_j\"])\n",
    "D_ij_all = np.array(np.load(f'./data/graph/{SEQ}/{SEQ}_allpath.npz',allow_pickle=True)[\"D_ij\"], dtype=np.float16)\n",
    "print(\"X_j_all\", X_j_all.shape)\n",
    "print(\"D_ij_all\", D_ij_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make precomputed distance matrix for MDS with \n",
    "MDS_dist = np.ones((D_ij_all.shape[0],D_ij_all.shape[0]))\n",
    "for i in range(len(D_ij_all)):\n",
    "    MDS_dist[i,X_j_all[i]] = D_ij_all[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDS_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary if the distance matrix includes all the neighbors\n",
    "def makeSymmetric(mat):\n",
    "    # Loop to traverse lower triangular\n",
    "    # elements of the given matrix\n",
    "    for i in range(0, len(mat)):\n",
    "        for j in range(0, len(mat)):\n",
    "            if (j < i):\n",
    "                mat[i][j] = mat[j][i] = min(mat[i][j], mat[j][i])\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDS_dist_symm = makeSymmetric(MDS_dist)\n",
    "MDS_dist_symm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed')\n",
    "mds_coords = mds.fit_transform(MDS_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_all_coords = mds_coords[coord_id_S]\n",
    "fnpz_data = f\"data/vida_data/{SEQ}_mds.npz\"\n",
    "with open(fnpz_data, 'wb') as f:\n",
    "    np.savez(f,\n",
    "            mds_coords=mds_coords, mds_all_coords=mds_all_coords,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save for python\n",
    "# pca_all_coords_direct = pca_coords_direct[coord_id_S]  \n",
    "# phate_all_coords_direct = phate_coords_direct[coord_id_S]\n",
    "# mds_all_coords = mds_coords[coord_id_S]\n",
    "\n",
    "# fnpz_data = f\"data/vida_data/noViDa-noEnergy/{SEQ}_noViDa.npz\"\n",
    "# with open(fnpz_data, 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#             pca_coords_direct=pca_coords_direct, pca_all_coords_direct=pca_all_coords_direct,\n",
    "#             phate_coords_direct=phate_coords_direct, phate_all_coords_direct=phate_all_coords_direct,\n",
    "#             mds_coords=mds_coords, mds_all_coords=mds_all_coords,\n",
    "#             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "X = pca_coords[:,0]\n",
    "Y = pca_coords[:,1]\n",
    "Z = pca_coords[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[int(SIMS_dict[-1,-1])]]\n",
    "y = [Y[0],Y[int(SIMS_dict[-1,-1])]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try use PCA directly without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "pca_coords_direct = PCA(n_components=3).fit_transform(SIMS_scar_uniq)   # multiple trj\n",
    "\n",
    "X = pca_coords_direct[:,0]\n",
    "Y = pca_coords_direct[:,1]\n",
    "Z = pca_coords_direct[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=20,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = PCA(n_components=25)\n",
    "cm.fit(SIMS_scar_uniq)\n",
    "\n",
    "PC_values = np.arange(cm.n_components_) + 1\n",
    "plt.plot(PC_values, np.cumsum(cm.explained_variance_ratio_), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot: PCA')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "# plt.xticks(np.arange(0, data_embed.shape[-1]+1, 1))\n",
    "plt.show()\n",
    "\n",
    "print(np.cumsum(cm.explained_variance_ratio_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHATE Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phate = phate_coords[:,0]\n",
    "Y_phate = phate_coords[:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X_phate,Y_phate,\n",
    "                c=SIMS_G_uniq,            \n",
    "                cmap='plasma',\n",
    "               )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X_phate[0],X_phate[int(SIMS_dict[-1,-1])]]\n",
    "y = [Y_phate[0],Y_phate[int(SIMS_dict[-1,-1])]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PHATE without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "phate_coords_direct = phate_operator.fit_transform(SIMS_scar_uniq)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(phate_coords_direct[:,0],\n",
    "          phate_coords_direct[:,1],\n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [phate_coords_direct[:,0][0],phate_coords_direct[:,0][-1]]\n",
    "y = [phate_coords_direct[:,1][0],phate_coords_direct[:,1][-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS with distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnpz_data = \"./data/vida_data/PT4_mds.npz\"\n",
    "mds_coords = np.load(fnpz_data)[\"mds_coords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mds_coords[:,0]\n",
    "Y = mds_coords[:,1]\n",
    "cmap = plt.cm.plasma\n",
    "cmap_r = plt.cm.get_cmap('plasma_r')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c = SIMS_G_uniq,\n",
    "          cmap=cmap,\n",
    "          s=10\n",
    "        )\n",
    " \n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[int(SIMS_dict[-1,-1])]]\n",
    "y = [Y[0],Y[int(SIMS_dict[-1,-1])]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering to Find Kinetic Traps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_idx = 0\n",
    "# initial_idx = np.where(SIMS_dict_uniq[:,0] == '.........................+.........................')[0][0]\n",
    "final_idx = np.where(SIMS_dict_uniq[:,0] == '(((((((((((((((((((((((((+)))))))))))))))))))))))))')[0][0]\n",
    "print(initial_idx, final_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine all features into one array\n",
    "pos_data = data_embed\n",
    "energy_data = SIMS_G_uniq\n",
    "time_data = SIMS_HT_uniq\n",
    "frq_data = P_tot\n",
    "\n",
    "# # normalize different features to the same scale\n",
    "scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "\n",
    "norm_pos_data = scaler.fit_transform(pos_data)\n",
    "norm_energy_data = scaler.fit_transform(energy_data.reshape(-1,1))\n",
    "norm_time_data = scaler.fit_transform(time_data.reshape(-1,1))\n",
    "\n",
    "combined_data = np.concatenate((norm_pos_data, norm_energy_data, norm_time_data, frq_data.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out data points with low frequency\n",
    "\n",
    "## eps = 0.07, min_samples = 4 for PT0 with filter=0.13\n",
    "## eps = 0.14, min_samples = 4 for PT3 with filter=0.15\n",
    "## eps = 0.18, min_samples = 4 for PT4 with filter=0.2\n",
    "## eps = 0.08, min_samples = 4 for PT3_hairpin with filter=0.2\n",
    "## eps = 0.15, min_samples = 4 for PT4_hairpin with filter=0.5\n",
    "\n",
    "if SEQ == \"PT0\": \n",
    "    filter_threshold = 0.13\n",
    "elif SEQ == \"PT3\":\n",
    "    filter_threshold = 0.15\n",
    "elif SEQ == \"PT4\":\n",
    "    filter_threshold = 0.2 \n",
    "    \n",
    "elif SEQ == \"PT3_hairpin\":\n",
    "    filter_threshold = 0.2\n",
    "elif SEQ == \"PT4_hairpin\":\n",
    "    filter_threshold = 0.5\n",
    "\n",
    "filter_idx = np.where(P_tot>=filter_threshold)[0]\n",
    "filter_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do PCA for combined data\n",
    "\n",
    "# comb_pca_coords = pca_coords[:,:2]\n",
    "# comb_pca_coords = PCA(n_components=2).fit_transform(combined_data)\n",
    "comb_pca_coords = phate_coords\n",
    "\n",
    "# #######################\n",
    "# import deeptime\n",
    "# from deeptime.decomposition import TICA\n",
    "# tica = TICA(lagtime=1,dim=2)\n",
    "# comb_pca_coords = tica.fit_transform(combined_data)\n",
    "# #######################\n",
    "\n",
    "## filtered data\n",
    "filter_comb_pca_coords = comb_pca_coords[filter_idx]\n",
    "filter_comb_pca_coords.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elbow method to find eps for DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "n_neighbors = 4  # Number of neighbors to find\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(filter_comb_pca_coords)\n",
    "distances, indices = nbrs.kneighbors(filter_comb_pca_coords)\n",
    "four_dist = np.sum(distances,axis=1)\n",
    "sorted_four_dist = np.sort(four_dist)[::-1]\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "# Add a line trace\n",
    "fig.add_trace(go.Scatter(x=indices[:,0], y=sorted_four_dist, \n",
    "                         mode='lines', name='Line Plot'))\n",
    "# Set labels and title\n",
    "fig.update_layout(xaxis_title='points', yaxis_title='4-dist', title='Elbow')\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "## eps = 0.07, min_samples = 4 for PT0 with filter=0.13\n",
    "## eps = 0.14, min_samples = 4 for PT3 with filter=0.15\n",
    "## eps = 0.18, min_samples = 4 for PT4 with filter=0.2\n",
    "## eps = 0.08, min_samples = 4 for PT3_hairpin with filter=0.2\n",
    "## eps = 0.15, min_samples = 4 for PT4_hairpin with filter=0.5\n",
    "\n",
    "if SEQ == 'PT0':\n",
    "    eps = 0.07\n",
    "elif SEQ == 'PT3':\n",
    "    eps = 0.14\n",
    "elif SEQ == 'PT4':\n",
    "    eps = 0.005\n",
    "    # eps = 0.003\n",
    "    \n",
    "elif SEQ == 'PT3_hairpin':\n",
    "    eps = 0.08\n",
    "elif SEQ == 'PT4_hairpin':\n",
    "    eps = 0.15\n",
    "\n",
    "\n",
    "X = filter_comb_pca_coords\n",
    "clusters = DBSCAN(eps = eps, min_samples = 4).fit(X)\n",
    "# get cluster labels\n",
    "labels = clusters.labels_\n",
    "\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "# # check unique clusters  \n",
    "set(clusters.labels_)\n",
    "# # -1 value represents noisy points could not assigned to any cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove no trap clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels = labels.copy()\n",
    "for k_clust in np.unique(labels):\n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(labels==k_clust)[0]])\n",
    "    # print(\"For cluster {}:\".format(k_clust))\n",
    "    plausible_trap = SIMS_dict_uniq[filter_idx][np.where(labels==k_clust)[0]][min_index][0]\n",
    "    if \"(\"*15 in plausible_trap:\n",
    "        real_labels = [-1 if x==k_clust else x for x in real_labels]\n",
    "        print(\"Cluster {} is NOT a trap\".format(k_clust))\n",
    "    else:\n",
    "        print(\"Cluster {} is a trap\".format(k_clust))\n",
    "        \n",
    "real_labels = np.array(real_labels)\n",
    "\n",
    "print(\"\\nClusters with trap are: {}\".format(np.unique(real_labels)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the kinetic trap in each cluster\n",
    "trap_list = []\n",
    "for k_clust in np.unique(real_labels):\n",
    "    if k_clust == -1:\n",
    "        continue\n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(labels==k_clust)[0]])\n",
    "    trap = SIMS_dict_uniq[filter_idx][np.where(labels==k_clust)[0]][min_index]\n",
    "    trap_list.append(trap)\n",
    "    print(\"Kinetic trap in cluster {} is:\".format(k_clust))\n",
    "    print(trap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exact each trajectory\n",
    "split_id = trj_id + 1 # index for split to each trajectory\n",
    "# real_labels = real_labels[real_labels!=-1]\n",
    "traj_in_clust = np.zeros(len(np.unique(real_labels))-1, dtype=int)\n",
    "avg_time_in_clust = np.zeros(len(np.unique(real_labels))-1, dtype=float)\n",
    "trap_0, trap_1, trap_2 = trap_list[0][0], trap_list[1][0], trap_list[2][0]\n",
    "trap_list_new = [trap_0, trap_1, trap_2]\n",
    "total_avg_time = 0\n",
    "\n",
    "for i in range(len(split_id)):\n",
    "    if i == 0:\n",
    "        trj_dp = SIMS_dict[0:split_id[i],0]\n",
    "    else:\n",
    "        trj_dp = SIMS_dict[split_id[i-1]:split_id[i],0]\n",
    "        \n",
    "    total_avg_time += SIMS_T[trj_id[i]]\n",
    "    \n",
    "    for j, trap in enumerate(trap_list_new):\n",
    "        if trap == trap_0:\n",
    "            # if trap in trj_dp:  \n",
    "            if trap in trj_dp and trap_1 not in trj_dp:                \n",
    "                traj_in_clust[j] += 1\n",
    "                avg_time_in_clust[j] += SIMS_T[trj_id[i]]  \n",
    "                \n",
    "        elif trap == trap_1:\n",
    "            # if trap in trj_dp and trap_0 not in trj_dp: \n",
    "            if trap in trj_dp:\n",
    "                traj_in_clust[j] += 1\n",
    "                avg_time_in_clust[j] += SIMS_T[trj_id[i]]\n",
    "                \n",
    "        elif trap == trap_2:\n",
    "            if trap in trj_dp:  \n",
    "                traj_in_clust[j] += 1\n",
    "                avg_time_in_clust[j] += SIMS_T[trj_id[i]]\n",
    "\n",
    "print(f\"{SEQ}:\")\n",
    "for i in range(len(traj_in_clust)):\n",
    "    print(\"{} trajs in cluster {}. Average time: {:.3e}.\".format(traj_in_clust[i], np.setdiff1d(np.unique(real_labels),-1)[i], avg_time_in_clust[i]/traj_in_clust[i]))\n",
    "print(\"Total average time: {:.3e}.\".format(total_avg_time/len(trj_id)))\n",
    "print(\"No hairpin (PT0) time: {:.3e}\".format(4.872*1e-6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual traps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = filter_comb_pca_coords[:,0]\n",
    "Y = filter_comb_pca_coords[:,1]\n",
    "clusters = real_labels  # Cluster real labels\n",
    "\n",
    "# Get unique cluster labels\n",
    "unique_clusters = np.unique(clusters)\n",
    "\n",
    "# Define colors for each cluster\n",
    "colors = ['grey', 'cyan', 'red', 'blue', 'green', 'orange', 'purple', 'yellow',  'magenta', 'lime', 'teal']  # Add more colors as needed\n",
    "\n",
    "# Create a scatter trace for each cluster\n",
    "traces = []\n",
    "\n",
    "# background\n",
    "trace = go.Scattergl(\n",
    "        x=comb_pca_coords[:,0],\n",
    "        y=comb_pca_coords[:,1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            sizemode='area',\n",
    "            size=3,\n",
    "            color='lightgrey',\n",
    "            line=dict(width=0),\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        text=SIMS_dict_uniq[:,0],\n",
    "        hovertemplate=\"DP notation: <br> <b>%{text}</b><br>\",\n",
    "    )\n",
    "traces.append(trace)\n",
    "\n",
    "# clusters\n",
    "i = 0\n",
    "for cluster_label in unique_clusters:\n",
    "    mask = clusters == cluster_label\n",
    "    \n",
    "    if cluster_label == -1:\n",
    "        continue\n",
    "    # elif cluster_label == 0:\n",
    "    #     continue\n",
    "    trace = go.Scattergl(\n",
    "        x=X[mask],\n",
    "        y=Y[mask],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=colors[i],\n",
    "            size = 4,\n",
    "            sizemode='area',\n",
    "            ),\n",
    "        showlegend=False,\n",
    "    )\n",
    "    i += 1\n",
    "    traces.append(trace)\n",
    "\n",
    "# label initial and final states\n",
    "trace = go.Scattergl(\n",
    "        x = comb_pca_coords[:,0][[initial_idx, final_idx]],\n",
    "        y = comb_pca_coords[:,1][[initial_idx, final_idx]],\n",
    "        mode='markers+text',\n",
    "        marker_color=\"lime\", \n",
    "        marker_size=20,\n",
    "        text=[\"I\", \"F\"],\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(\n",
    "        family=\"sans serif\",\n",
    "        size=15,\n",
    "        color=\"black\"\n",
    "    ),\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False,\n",
    "                    )\n",
    "traces.append(trace)\n",
    "\n",
    "\n",
    "# label kinetic traps\n",
    "i = 0\n",
    "for k_clust in unique_clusters:\n",
    "    if k_clust == -1:\n",
    "        continue\n",
    "    # if k_clust == 0:\n",
    "    #     continue\n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(clusters==k_clust)[0]])\n",
    "    trace = go.Scattergl(\n",
    "        x = np.array(X[np.where(clusters==k_clust)[0]][min_index]),\n",
    "        y = np.array(Y[np.where(clusters==k_clust)[0]][min_index]),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=\"black\",\n",
    "            symbol='star',\n",
    "            size=15,\n",
    "        ),\n",
    "        showlegend=False,\n",
    "    )\n",
    "    i += 1\n",
    "    traces.append(trace)\n",
    "\n",
    "# legend setting\n",
    "layout = go.Layout(\n",
    "    title=f\"DBSCAN finding Kinetic Traps for sample {SEQ}\",\n",
    ")\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "pio.write_html(fig, file=f\"../output_files/saved_ViDa_plots/plot_dna29/PT4_PHATE_traps_dna29.html\", auto_open=True)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptime\n",
    "from deeptime.decomposition import TICA\n",
    "\n",
    "tica = TICA(lagtime=1,dim=2)\n",
    "data = combined_data\n",
    "tica_coor = tica.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combined_data\n",
    "tica_coor = tica.fit_transform(data)\n",
    "tica_coor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt plot for tica\n",
    "plt.scatter(tica_coor[:,0],tica_coor[:,1],c=SIMS_G_uniq,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vida')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e442af4fad2330d8f4febe7e8e7250535e161341429a4f0b93cbf21b824330cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
