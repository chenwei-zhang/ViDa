{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation,PillowWriter\n",
    "from IPython import display\n",
    "import h5py\n",
    "import pickle\n",
    "# from copy import deepcopy\n",
    "\n",
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# sys.path.append('/Users/chenwei/Desktop/Github/RPE/GSAE_source/GSAE/gsae/data_processing/')\n",
    "from gsae.models.gsae_model import GSAE\n",
    "from gsae.data_processing.utils import dot2adj\n",
    "from gsae.data_processing.create_splits import split_data\n",
    "\n",
    "from gsae.scattering.scattering import transform_dataset, get_normalized_moments\n",
    "from gsae.utils import eval_metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import phate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset generated by Multistrand\n",
    "def loadtrj(f,FINAL_STRUCTURE,type):\n",
    "    \"\"\"load text data and split it into individual trajectory \n",
    "    with seperated structure, time, and energy\n",
    "\n",
    "    Args:\n",
    "        f: text file with trajectory dp notation, time, energy\n",
    "            eg. '..((((....)))).', 't=0.000000103', 'seconds, dG=  0.26 kcal/mol\\n'\n",
    "        FINAL_STRUCTURE: final state structure, eg. \"..((((....)))).\"\n",
    "        type: 'Single' or 'Multiple' mode\n",
    "    Returns:\n",
    "        [list]: dot-parenthesis notation, time floats, energy floats\n",
    "            eg. ['...............', 0.0, 0.0]\n",
    "    \"\"\"\n",
    "    TRAJ=[];i=0;SIM=[]\n",
    "    for s in f:\n",
    "        i+=1\n",
    "        if i>1: # remove headers\n",
    "            ss = s.split(\" \",3)\n",
    "            s_dotparan=ss[0] # dp notation\n",
    "            s_time = float(ss[1].split(\"=\",1)[1]) # simulation time\n",
    "            s_energy = float(ss[3].split(\"=\")[1].split(\"kcal\")[0]) # energy\n",
    "            TRAJ.append([s_dotparan,s_time,s_energy])\n",
    "\n",
    "            if type == \"Single\":\n",
    "                if s_dotparan == FINAL_STRUCTURE: # split to individual trajectory\n",
    "                    SIM.append(TRAJ)\n",
    "                    TRAJ = []\n",
    "    if type == \"Multiple\":\n",
    "        SIM = TRAJ\n",
    "    return SIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dot-parenthesis notation to adjacency matrix in a single trajectory\n",
    "def sim_adj(sim):\n",
    "    \"\"\"convert dot-parenthesis notation to adjacency matrix in a single trajectory\n",
    "    Args:\n",
    "        sim: [list of sims] dot-parenthesis notation, time floats, energy floats\n",
    "            eg. [['...............', 0.0, 0.0], ...]\n",
    "    Returns:\n",
    "        (tuple): NxN adjacency np matrix, Nx1 energy np array, Nx1 time np array, Nx1 HT np array\n",
    "    \"\"\"\n",
    "    \n",
    "    adj_mtr = []\n",
    "    sim_G = np.array([])\n",
    "    sim_T = np.array([])\n",
    "    \n",
    "    for s in sim:\n",
    "        sim_T = np.append(sim_T,s[1]) # get time array\n",
    "        sim_G = np.append(sim_G,s[2]) # get energy array\n",
    "        \n",
    "        adj = dot2adj(s[0])\n",
    "        adj_mtr.append(adj)\n",
    "    adj_mtr = np.array(adj_mtr) # get adjacency matrix\n",
    "    \n",
    "    sim_HT = np.concatenate([np.diff(sim_T),[0]])\n",
    "    \n",
    "    return adj_mtr,sim_G,sim_T,sim_HT\n",
    "\n",
    "\n",
    "# convert all simulations\n",
    "def get_whole_data(SIM):\n",
    "    SIMS_adj=[]; SIMS_G=[]; SIMS_T=[]; SIMS_HT=[]\n",
    "    for i in range(len(SIM)):\n",
    "        data = SIM[i]\n",
    "        SIM_adj,SIM_G,SIM_T,SIM_HT = sim_adj(data)\n",
    "        assert min(SIM_G) == SIM_G[-1], \"Final state is not the minimum energy state.\"\n",
    "        SIMS_adj.append(SIM_adj); SIMS_T.append(SIM_T); SIMS_G.append(SIM_G); SIMS_HT.append(SIM_HT)\n",
    "\n",
    "    SIMS_adj = np.concatenate((SIMS_adj),axis=0)\n",
    "    SIMS_G = np.concatenate((SIMS_G),axis=0)\n",
    "    SIMS_T = np.concatenate((SIMS_T),axis=0)\n",
    "    SIMS_HT = np.concatenate((SIMS_HT),axis=0)\n",
    "    \n",
    "    return SIMS_adj,SIMS_G,SIMS_T,SIMS_HT\n",
    "\n",
    "\n",
    "# get unique structures\n",
    "def get_unique(SIM_adj,SIM_G,SIM_T,SIM_HT):\n",
    "    \"\"\"\n",
    "    # get unique states adjacency matrix with their occupancy density\n",
    "    # get unique energy, and time\n",
    "    \"\"\"\n",
    "    indices, occ_density = adj_uniq_occp(SIM_adj)\n",
    "\n",
    "    SIM_adj_uniq = SIM_adj[indices]\n",
    "    SIM_G_uniq = SIM_G[indices]\n",
    "    SIM_T_uniq = SIM_T[indices]\n",
    "    SIM_HT_uniq = SIM_HT[indices]\n",
    "    \n",
    "    return indices,occ_density,SIM_adj_uniq,SIM_G_uniq,SIM_T_uniq,SIM_HT_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and test data\n",
    "def load_trte(train_data,test_data,\n",
    "              batch_size=32,gnn=False,subsize=None,lognorm=False):\n",
    "\n",
    "    train_adjs = train_data[0]\n",
    "    train_coeffs = train_data[1]\n",
    "    train_energies = train_data[2]\n",
    "    \n",
    "    test_adjs = test_data[0]\n",
    "    test_coeffs = test_data[1]\n",
    "    test_energies = test_data[2]\n",
    "\n",
    "    if lognorm:\n",
    "        # shift\n",
    "        train_coeffs +=  np.abs(train_coeffs.min()) + 1\n",
    "        test_coeffs += np.abs(train_coeffs.min()) + 1\n",
    "        \n",
    "        # log\n",
    "        train_coeffs = np.log(train_coeffs)\n",
    "        test_coeffs = np.log(test_coeffs)\n",
    "\n",
    "\n",
    "    if gnn:\n",
    "        train_diracs = torch.eye(train_adjs.shape[-1]).unsqueeze(0).repeat(train_adjs.shape[0],1,1)\n",
    "        train_tup = (torch.Tensor(train_diracs),\n",
    "                    torch.Tensor(train_adjs),\n",
    "                    torch.Tensor(train_energies))\n",
    "    else:\n",
    "        train_tup = (torch.Tensor(train_coeffs),\n",
    "                    torch.Tensor(train_energies))\n",
    "\n",
    "\n",
    "\n",
    "    if gnn:\n",
    "        test_diracs = torch.eye(test_adjs.shape[-1]).unsqueeze(0).repeat(test_adjs.shape[0],1,1)\n",
    "        test_tup = (torch.Tensor(test_diracs),\n",
    "                    torch.Tensor(test_adjs),\n",
    "                    torch.Tensor(test_energies))\n",
    "\n",
    "    else:\n",
    "        test_tup = (torch.Tensor(test_coeffs), \n",
    "                    torch.Tensor(test_adjs), \n",
    "                    torch.Tensor(test_energies))\n",
    "        \n",
    "    #################\n",
    "    # SUBSET DATA \n",
    "    #################tre\n",
    "    if subsize != None:\n",
    "        train_tup, _ = eval_metrics.compute_subsample(train_tup, subsize)\n",
    "        test_tup, _ = eval_metrics.compute_subsample(test_tup, subsize)\n",
    "\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(*train_tup)\n",
    "    test_dataset = torch.utils.data.TensorDataset(*test_tup)\n",
    "    \n",
    "    # get valid set\n",
    "    train_full_size = len(train_dataset)\n",
    "    train_split_size = int(train_full_size * .80)\n",
    "    valid_split_size = train_full_size - train_split_size \n",
    "    train_set, val_set = torch.utils.data.random_split(train_dataset, [train_split_size, valid_split_size])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    # valid loader \n",
    "    valid_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                        shuffle=False)\n",
    "    \n",
    " \n",
    "    # early stopping \n",
    "    early_stop_callback = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.00,\n",
    "            patience=3,\n",
    "            verbose=True,\n",
    "            mode='min'\n",
    "            )\n",
    "    \n",
    "    return train_loader, train_tup, test_tup, valid_loader,early_stop_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate the occupancy density of each state\n",
    "def adj_uniq_occp(states):\n",
    "    \"\"\"load adjacency matrix and calculate the occupancy density of each state\n",
    "    Args:\n",
    "        states: adjacency matrix\n",
    "    Returns:\n",
    "        indices,density: indices of unique states, occupancy density of each state\n",
    "    \"\"\"\n",
    "    _, indices, counts = np.unique(states,axis=0,return_index=True,return_counts=True)\n",
    "    counts = counts[np.argsort(indices)]\n",
    "    indices = np.sort(indices)\n",
    "    return indices, counts/counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate the time fraction of each state\n",
    "def time_frac(SIM_adj,SIM_adj_uniq,SIM_HT):\n",
    "    \"\"\"load time array and calculate the time fraction of each state\n",
    "    Args:\n",
    "        SIM_adj,SIM_adj_uniq,SIM_HT\n",
    "    Returns:\n",
    "        time fractions: time fraction of each unique state\n",
    "    \"\"\"\n",
    "    time_count = np.zeros(len(SIM_adj_uniq))\n",
    "    \n",
    "    for i in range(len(SIM_adj_uniq)):\n",
    "        for j in range(len(SIM_adj)):\n",
    "            if np.array_equal(SIM_adj_uniq[i],SIM_adj[j]):\n",
    "                time_count[i] += SIM_HT[j]  \n",
    "                \n",
    "    time_fract = time_count/time_count.sum()\n",
    "    \n",
    "    # assert time_count.sum() == SIM_HT.sum(), \"Time counts are not equal to total time.\"\n",
    "    # assert time_fract.sum() == 1, \"Total time fraction is not equal to 1.\"\n",
    "\n",
    "    assert_almost_equal(time_count.sum(),SIM_HT.sum(),err_msg='Time counts are not equal to total time.')\n",
    "    assert_almost_equal(time_fract.sum(),1,err_msg='Total time fraction is not equal to 1.')\n",
    "\n",
    "    return time_count, time_fract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import/generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load simulated data from Mulistrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define absorbing (final) state structure\n",
    "FINAL_STRUCTURE = \"..((((....)))).\"\n",
    "\n",
    "# load text file\n",
    "f = open('./data/I1_10000sim.txt', 'r')\n",
    "SIM_retrieve = loadtrj(f,FINAL_STRUCTURE,type=\"Multiple\")\n",
    "SIM_retrieve = np.array(SIM_retrieve)\n",
    "print(\"SIM_retrieve: \", SIM_retrieve.shape)\n",
    "\n",
    "# load text file\n",
    "f = open('./data/I1_10000sim.txt', 'r')\n",
    "\n",
    "\"\"\" Dimenstions of SIM list \n",
    "SIM: [[sim1], [sim2], ...]\n",
    "sim: [[state1], [state2], ...]\n",
    "state: [structure, time, energy]\n",
    "\"\"\"\n",
    "SIM = loadtrj(f,FINAL_STRUCTURE,type=\"Single\")\n",
    "print(\"SIM: \", len(SIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert dot-paren to adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dot-parenthesis to adjecency matrix\n",
    "\"\"\" Dimenstions of SIM_adj list \n",
    "SIM_adj: N*m*m\n",
    "    N: number of states in the trajectory\n",
    "    m: number of nucleotides in the state (strand)\n",
    "\"\"\"\n",
    "\n",
    "# convert all simulations\n",
    "SIMS_adj,SIMS_G,SIMS_T,SIMS_HT = get_whole_data(SIM)\n",
    "\n",
    "SIMS_adj.shape, SIMS_G.shape, SIMS_T.shape, SIMS_HT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique states adjacency matrix with their occupancy density\n",
    "# get unique energy, and time\n",
    "\n",
    "indices,occ_density,SIMS_adj_uniq,SIMS_G_uniq,SIMS_T_uniq,SIMS_HT_uniq \\\n",
    "    = get_unique(SIMS_adj,SIMS_G,SIMS_T,SIMS_HT)\n",
    "    \n",
    "SIMS_adj_uniq.shape, SIMS_G_uniq.shape,SIMS_T_uniq.shape,SIMS_HT_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the structure having the largest occupancy density\n",
    "SIM_retrieve[indices[occ_density.argmax()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique structures with dot-parenthesis notation\n",
    "SIM_retrieve[indices].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert adjacency matrix scattering coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all states\n",
    "scat_coeff_array = transform_dataset(SIMS_adj)\n",
    "norm_scat_coeffs = get_normalized_moments(scat_coeff_array).squeeze()\n",
    "SIMS_scar = norm_scat_coeffs\n",
    "SIMS_scar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert only unique states to get unique scattering\n",
    "scat_coeff_array = transform_dataset(SIMS_adj_uniq)\n",
    "norm_scat_coeffs = get_normalized_moments(scat_coeff_array).squeeze()\n",
    "SIMS_scar_uniq = norm_scat_coeffs\n",
    "SIMS_scar_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save some obtained arrays to npy file\n",
    "\"\"\"\n",
    "# save GSAE embeded data\n",
    "with open('data/I1_sims_trte_uniq.npy', 'wb') as f:\n",
    "    np.savez(f,SIMS_adj_uniq=SIMS_adj_uniq,SIMS_scar_uniq=SIMS_scar_uniq,\n",
    "             SIMS_G_uniq=SIMS_G_uniq)\n",
    "\n",
    "npyfileT = np.load(\"data/I1_sims_trte_uniq.npy\")\n",
    "\n",
    "SIMS_adj_uniq = npyfileT[\"SIMS_adj_uniq\"]\n",
    "SIMS_scar_uniq = npyfileT[\"SIMS_scar_uniq\"]\n",
    "SIMS_G_uniq = npyfileT[\"SIMS_G_uniq\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Split data into tranning and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shape of split data\n",
    "    train_data: [tr_adjs, tr_coeffs, tr_energies]\n",
    "    test_data: [te_adjs, te_coeffs, te_energies]\n",
    "\"\"\"\n",
    "train_data,test_data = split_data(SIMS_adj_uniq,SIMS_scar_uniq,SIMS_G_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(train_data[1],axis=0)).shape, train_data[1].shape,test_data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Structure of train_tup when gnn=False\n",
    "    train_tup: [train_coeffs,train_energy] \n",
    "\"\"\"\n",
    "train_loader, train_tup, test_tup, valid_loader,early_stop_callback = load_trte(train_data,test_data,\n",
    "                                              batch_size=16)\n",
    "train_tup[0].shape, test_tup[0].shape, train_loader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters\n",
    "\n",
    "input_dim = train_tup[0].shape[-1]\n",
    "len_epoch = len(train_loader)\n",
    "\n",
    "hparams = {\n",
    "    'input_dim':  input_dim,\n",
    "    'bottle_dim': 25,\n",
    "    'hidden_dim': 400,\n",
    "    \n",
    "    'len_epoch': len_epoch,\n",
    "    'learning_rate': 0.0001,\n",
    "    'max_epochs': 400,\n",
    "    'n_gpus': 0,\n",
    "    # 'batch_size': 32,\n",
    "    \n",
    "    'alpha':1.0,\n",
    "    'beta':0.0001,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = argparse.Namespace(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.len_epoch,hparams.max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GSAE(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer.from_argparse_args(hparams,\n",
    "                                        max_epochs=hparams.max_epochs,\n",
    "                                        gpus=hparams.n_gpus,\n",
    "                                        # callbacks=[early_stop_callback],\n",
    "                                        )\n",
    "trainer.fit(model=model,\n",
    "            train_dataloader=train_loader,\n",
    "            val_dataloaders=valid_loader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "# from data I1_10000sim.txt SIM[1]\n",
    "filename = \"models/I1_10000_SIMS_model.pickle\"\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "print('Trained model saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"/Users/chenwei/Desktop/Github/GSAE/saved_models/GSAE_trained_model_seq3\"\n",
    "filename = \"/Users/chenwei/Desktop/Github/RPE/code/models/I1_10000_SIMS_model.pickle\"\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without duplicates\n",
    "with torch.no_grad():\n",
    "        data_embed = model.embed(torch.Tensor(SIMS_scar_uniq))[0]\n",
    "\n",
    "# # with duplicates\n",
    "# with torch.no_grad():\n",
    "#         data_tensor =  (torch.Tensor(SIM_scar),\n",
    "#                         torch.Tensor(SIM_G))\n",
    "#         data_embed = model.embed(data_tensor[0])[0]\n",
    "\n",
    "# data_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save embeded data to npy file\n",
    "\"\"\"\n",
    "# save GSAE embeded data\n",
    "with open('data/I1_sims_embed_uniq_t1.npy', 'wb') as f:\n",
    "    np.savez(f,data_embed=data_embed)\n",
    "\n",
    "npyfile0 = np.load(\"data/I1_sims_embed_uniq_t1.npy\")\n",
    "npyfile0.files\n",
    "data_embed = npyfile0[\"data_embed\"]\n",
    "data_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"save embeded data to h5 file\n",
    "# \"\"\"\n",
    "# def main():\n",
    "#     hf = h5py.File(\"data/I1_0_embed.h5\", \"w\")\n",
    "#     hf.create_dataset(\"data_embed\", data=data_embed)\n",
    "#     hf.close\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_coords = PCA(n_components=3).fit_transform(data_embed)\n",
    "pca_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save plot data to npy file\n",
    "\"\"\"  \n",
    "# save pca coordinates, energy, time data to npy file for Julia plot\n",
    "with open('data/I1_sims_pca_uniq_t1.npy', 'wb') as f:\n",
    "    np.savez(f,pca=pca_coords,energy=SIMS_G_uniq,time_HT=SIMS_HT_uniq,\n",
    "                  occp = occ_density)\n",
    "\n",
    "npyfile = np.load(\"data/I1_sims_pca_uniq_t1.npy\")\n",
    "npyfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = npyfile[\"pca\"][:,0]\n",
    "Y = npyfile[\"pca\"][:,1]\n",
    "Z = npyfile[\"pca\"][:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=npyfile[\"energy\"], \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = npyfile[\"pca\"][:,0]\n",
    "Y = npyfile[\"pca\"][:,1]\n",
    "Z = npyfile[\"pca\"][:,2]\n",
    "\n",
    "# PCA: 3 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "im = ax.scatter3D(X,Y,Z,\n",
    "          c=npyfile[\"energy\"],\n",
    "          cmap='plasma')\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "z = [Z[0], Z[-1]]\n",
    "ax.scatter(x,y,z,s=100,c=\"green\",alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try use PCA directly without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_coords1 = PCA(n_components=3).fit_transform(SIMS_scar_uniq)\n",
    "pca_coords1.shape\n",
    "\n",
    "X = pca_coords1[:,0]\n",
    "Y = pca_coords1[:,1]\n",
    "Z = pca_coords1[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=npyfile[\"energy\"], \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PHATE Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npyfile0 = np.load(\"data/I1_sims_embed_uniq_t1.npy\")\n",
    "data_embed = npyfile0[\"data_embed\"]\n",
    "data_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "Y_phate = phate_operator.fit_transform(data_embed)\n",
    "Y_phate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save plot data to npy file\n",
    "\"\"\"  \n",
    "# save pca coordinates, energy, time data to npy file for Julia plot\n",
    "with open('data/I1_sims_phate_uniq.npy', 'wb') as f:\n",
    "    np.savez(f,phate=Y_phate,energy=SIMS_G_uniq,time_HT=SIMS_HT_uniq,\n",
    "             occp=occ_density)\n",
    "\n",
    "npyphate = np.load(\"data/I1_sims_phate_uniq.npy\")\n",
    "Y_phate = npyphate[\"phate\"]\n",
    "Y_phate.shape, npyphate.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(Y_phate[:,0],\n",
    "          Y_phate[:,1],\n",
    "          c=npyphate[\"energy\"], \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [Y_phate[:,0][0],Y_phate[:,0][-1]]\n",
    "y = [Y_phate[:,1][0],Y_phate[:,1][-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PHATE without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "Y_phate = phate_operator.fit_transform(SIMS_scar_uniq)\n",
    "Y_phate.shape\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(Y_phate[:,0],\n",
    "          Y_phate[:,1],\n",
    "          c=npyphate[\"energy\"], \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [Y_phate[:,0][0],Y_phate[:,0][-1]]\n",
    "y = [Y_phate[:,1][0],Y_phate[:,1][-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('GSAE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "960be4cf653645892f8c5040dafd36ede801108a57618a05d934dc04f8033a0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
