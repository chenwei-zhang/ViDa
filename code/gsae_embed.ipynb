{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation,PillowWriter\n",
    "from IPython import display\n",
    "import h5py\n",
    "import pickle\n",
    "# from copy import deepcopy\n",
    "\n",
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# sys.path.append('/Users/chenwei/Desktop/Github/RPE/GSAE_source/GSAE/gsae/data_processing/')\n",
    "from gsae.models.gsae_model import GSAE\n",
    "from gsae.data_processing.utils import dot2adj\n",
    "from gsae.data_processing.create_splits import split_data\n",
    "\n",
    "from gsae.scattering.scattering import transform_dataset, get_normalized_moments\n",
    "from gsae.utils import eval_metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import phate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset generated by Multistrand\n",
    "def loadtrj(f,FINAL_STRUCTURE,type):\n",
    "    \"\"\"load text data and split it into individual trajectory \n",
    "    with seperated structure, time, and energy\n",
    "\n",
    "    Args:\n",
    "        f: text file with trajectory dp notation, time, energy\n",
    "            eg. '..((((....)))).', 't=0.000000103', 'seconds, dG=  0.26 kcal/mol\\n'\n",
    "        FINAL_STRUCTURE: final state structure, eg. \"..((((....)))).\"\n",
    "        type: 'Single' or 'Multiple' mode\n",
    "    Returns:\n",
    "        [list]: dot-parenthesis notation, time floats, energy floats\n",
    "            eg. ['...............', 0.0, 0.0]\n",
    "    \"\"\"\n",
    "    TRAJ=[];i=0;SIM=[]\n",
    "    for s in f:\n",
    "        i+=1\n",
    "        if i>4: # remove headers\n",
    "            ss = s.split(\" \",3)\n",
    "            s_dotparan=ss[0] # dp notation\n",
    "            s_time = float(ss[1].split(\"=\",1)[1]) # simulation time\n",
    "            s_energy = float(ss[3].split(\"=\")[1].split(\"kcal\")[0]) # energy\n",
    "            TRAJ.append([s_dotparan,s_time,s_energy])\n",
    "\n",
    "            if type == \"Single\":\n",
    "                if s_dotparan == FINAL_STRUCTURE: # split to individual trajectory\n",
    "                    SIM.append(TRAJ)\n",
    "                    TRAJ = []\n",
    "    if type == \"Multiple\":\n",
    "        SIM = TRAJ\n",
    "    return SIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dot-parenthesis notation to adjacency matrix in a single trajectory\n",
    "def sim_adj(sim):\n",
    "\n",
    "    adj_mtr = []\n",
    "    sim_G = np.array([])\n",
    "    sim_T = np.array([])\n",
    "    \n",
    "    for s in sim:\n",
    "        sim_T = np.append(sim_T,s[1]) # get time array\n",
    "        sim_G = np.append(sim_G,s[2]) # get energy array\n",
    "        \n",
    "        adj = dot2adj(s[0])\n",
    "        adj_mtr.append(adj)\n",
    "    adj_mtr = np.array(adj_mtr) # get adjacency matrix\n",
    "    \n",
    "    sim_HT = np.concatenate([np.diff(sim_T),[0]])\n",
    "    \n",
    "    return adj_mtr,sim_T,sim_G,sim_HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and test data\n",
    "def load_trte(train_data,test_data,\n",
    "              batch_size=32,gnn=False,subsize=None,lognorm=False):\n",
    "\n",
    "    train_adjs = train_data[0]\n",
    "    train_coeffs = train_data[1]\n",
    "    train_energies = train_data[2]\n",
    "    \n",
    "    test_adjs = test_data[0]\n",
    "    test_coeffs = test_data[1]\n",
    "    test_energies = test_data[2]\n",
    "\n",
    "    if lognorm:\n",
    "        # shift\n",
    "        train_coeffs +=  np.abs(train_coeffs.min()) + 1\n",
    "        test_coeffs += np.abs(train_coeffs.min()) + 1\n",
    "        \n",
    "        # log\n",
    "        train_coeffs = np.log(train_coeffs)\n",
    "        test_coeffs = np.log(test_coeffs)\n",
    "\n",
    "\n",
    "    if gnn:\n",
    "        train_diracs = torch.eye(train_adjs.shape[-1]).unsqueeze(0).repeat(train_adjs.shape[0],1,1)\n",
    "        train_tup = (torch.Tensor(train_diracs),\n",
    "                    torch.Tensor(train_adjs),\n",
    "                    torch.Tensor(train_energies))\n",
    "    else:\n",
    "        train_tup = (torch.Tensor(train_coeffs),\n",
    "                    torch.Tensor(train_energies))\n",
    "\n",
    "\n",
    "\n",
    "    if gnn:\n",
    "        test_diracs = torch.eye(test_adjs.shape[-1]).unsqueeze(0).repeat(test_adjs.shape[0],1,1)\n",
    "        test_tup = (torch.Tensor(test_diracs),\n",
    "                    torch.Tensor(test_adjs),\n",
    "                    torch.Tensor(test_energies))\n",
    "\n",
    "    else:\n",
    "        test_tup = (torch.Tensor(test_coeffs), \n",
    "                    torch.Tensor(test_adjs), \n",
    "                    torch.Tensor(test_energies))\n",
    "        \n",
    "    #################\n",
    "    # SUBSET DATA \n",
    "    #################tre\n",
    "    if subsize != None:\n",
    "        train_tup, _ = eval_metrics.compute_subsample(train_tup, subsize)\n",
    "        test_tup, _ = eval_metrics.compute_subsample(test_tup, subsize)\n",
    "\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(*train_tup)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    return train_loader, train_tup, test_tup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate the occupancy density of each state\n",
    "def adj_uniq_occp(states):\n",
    "    \"\"\"load adjacency matrix and calculate the occupancy density of each state\n",
    "    Args:\n",
    "        states: adjacency matrix\n",
    "    Returns:\n",
    "        indices,density: indices of unique states, occupancy density of each state\n",
    "    \"\"\"\n",
    "    _, indices, counts = np.unique(states,axis=0,return_index=True,return_counts=True)\n",
    "    counts = counts[np.argsort(indices)]\n",
    "    indices = np.sort(indices)\n",
    "    return indices, counts/counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate the time fraction of each state\n",
    "def time_frac(SIM_adj,SIM_adj_uniq,SIM_HT):\n",
    "    \"\"\"load time array and calculate the time fraction of each state\n",
    "    Args:\n",
    "        SIM_adj,SIM_adj_uniq,SIM_HT\n",
    "    Returns:\n",
    "        time fractions: time fraction of each unique state\n",
    "    \"\"\"\n",
    "    time_count = np.zeros(len(SIM_adj_uniq))\n",
    "    \n",
    "    for i in range(len(SIM_adj_uniq)):\n",
    "        for j in range(len(SIM_adj)):\n",
    "            if np.array_equal(SIM_adj_uniq[i],SIM_adj[j]):\n",
    "                time_count[i] += SIM_HT[j]  \n",
    "                \n",
    "    time_fract = time_count/time_count.sum()\n",
    "    \n",
    "    # assert time_count.sum() == SIM_HT.sum(), \"Time counts are not equal to total time.\"\n",
    "    # assert time_fract.sum() == 1, \"Total time fraction is not equal to 1.\"\n",
    "\n",
    "    assert_almost_equal(time_count.sum(),SIM_HT.sum(),err_msg='Time counts are not equal to total time.')\n",
    "    assert_almost_equal(time_fract.sum(),1,err_msg='Total time fraction is not equal to 1.')\n",
    "\n",
    "    return time_count, time_fract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import/generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load simulated data from Mulistrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file\n",
    "f = open('./data/I1_10000sim.txt', 'r')\n",
    "# define absorbing (final) state structure\n",
    "sim_num = 10000\n",
    "FINAL_STRUCTURE = \"..((((....)))).\"\n",
    "\n",
    "\"\"\" Dimenstions of SIM list \n",
    "SIM: [[sim1], [sim2], ...]\n",
    "sim: [[state1], [state2], ...]\n",
    "state: [structure, time, energy]\n",
    "\"\"\"\n",
    "SIM = loadtrj(f,sim_num,FINAL_STRUCTURE)\n",
    "len(SIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert dot-paren to adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dot-parenthesis to adjecency matrix\n",
    "\"\"\" Dimenstions of SIM_adj list \n",
    "SIM_adj: N*m*m\n",
    "    N: number of states in the trajectory\n",
    "    m: number of nucleotides in the state (strand)\n",
    "\"\"\"\n",
    "# convert single simulations\n",
    "data = SIM[1]\n",
    "SIM_adj,SIM_T,SIM_G,SIM_HT = sim_adj(data)\n",
    "assert min(SIM_G) == SIM_G[-1], \"Final state is not the minimum energy state.\"\n",
    "SIM_adj.shape,SIM_HT.shape,SIM_T.shape,SIM_G.shape\n",
    "\n",
    "# convert dot-parenthesis to adjecency matrix\n",
    "\"\"\" Dimenstions of SIM_adj list \n",
    "SIM_adj: N*m*m\n",
    "    N: number of states in the trajectory\n",
    "    m: number of nucleotides in the state (strand)\n",
    "\"\"\"\n",
    "\n",
    "# # convert all simulations\n",
    "# SIMS_adj=[]; SIMS_T=[]; SIMS_G=[]; SIMS_HT=[]\n",
    "# for i in range(len(SIM)):\n",
    "#     data = SIM[i]\n",
    "#     SIM_adj,SIM_T,SIM_G,SIM_HT = sim_adj(data)\n",
    "#     assert min(SIM_G) == SIM_G[-1], \"Final state is not the minimum energy state.\"\n",
    "#     SIMS_adj.append(SIM_adj); SIMS_T.append(SIM_T); SIMS_G.append(SIM_G); SIMS_HT.append(SIM_HT)\n",
    "\n",
    "# SIMS_adj = np.concatenate((SIMS_adj),axis=0)\n",
    "# SIMS_G = np.concatenate((SIMS_G),axis=0)\n",
    "# SIMS_T = np.concatenate((SIMS_T),axis=0)\n",
    "# SIMS_HT = np.concatenate((SIMS_HT),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique states adjacency matrix with their occupancy density\n",
    "# get unique energy, and time\n",
    "indices, occ_density = adj_uniq_occp(SIM_adj)\n",
    "\n",
    "SIM_adj_uniq = SIM_adj[indices]\n",
    "SIM_G_uniq = SIM_G[indices]\n",
    "SIM_T_uniq = SIM_T[indices]\n",
    "SIM_HT_uniq = SIM_HT[indices]\n",
    "SIM_adj_uniq.shape, SIM_G_uniq.shape,SIM_T_uniq.shape,SIM_HT_uniq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get time counts and its fraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_count,time_fract = time_frac(SIM_adj,SIM_adj_uniq,SIM_HT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the structure consuming the most of time\n",
    "data[indices[time_fract.argmax()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert adjacency matrix scattering coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all states\n",
    "scat_coeff_array = transform_dataset(SIM_adj)\n",
    "norm_scat_coeffs = get_normalized_moments(scat_coeff_array).squeeze()\n",
    "SIM_scar = norm_scat_coeffs\n",
    "SIM_scar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert only unique states to get unique scattering\n",
    "scat_coeff_array = transform_dataset(SIM_adj_uniq)\n",
    "norm_scat_coeffs = get_normalized_moments(scat_coeff_array).squeeze()\n",
    "SIM_scar_uniq = norm_scat_coeffs\n",
    "SIM_scar_uniq.shape\n",
    "\n",
    "# # get unique scattering structure for all states\n",
    "# SIM_scar_uniq = SIM_scar[indices]\n",
    "# SIM_scar_uniq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Split data into tranning and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shape of split data\n",
    "    train_data: [tr_adjs, tr_coeffs, tr_energies]\n",
    "    test_data: [te_adjs, te_coeffs, te_energies]\n",
    "\"\"\"\n",
    "train_data,test_data = split_data(SIM_adj,SIM_scar,SIM_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(train_data[1],axis=0)).shape, train_data[1].shape,test_data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Structure of train_tup when gnn=False\n",
    "    train_tup: [train_coeffs,train_energy] \n",
    "\"\"\"\n",
    "train_loader, train_tup, test_tup = load_trte(train_data,test_data,\n",
    "                                              batch_size=16)\n",
    "train_tup[0].shape, test_tup[0].shape, train_loader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters\n",
    "\n",
    "input_dim = train_tup[0].shape[-1]\n",
    "len_epoch = len(train_loader)\n",
    "\n",
    "hparams = {\n",
    "    'input_dim':  input_dim,\n",
    "    'bottle_dim': 25,\n",
    "    'hidden_dim': 400,\n",
    "    \n",
    "    'len_epoch': len_epoch,\n",
    "    'learning_rate': 0.0001,\n",
    "    'max_epochs': 80,\n",
    "    'n_gpus': 0,\n",
    "    'batch_size': 16,\n",
    "    \n",
    "    'alpha':1.0,\n",
    "    'beta':0.0001,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = argparse.Namespace(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.len_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GSAE(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer.from_argparse_args(hparams,\n",
    "                                        max_epochs=hparams.max_epochs,\n",
    "                                        gpus=hparams.n_gpus,\n",
    "                                        )\n",
    "trainer.fit(model=model,\n",
    "            train_dataloader=train_loader,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "# from data I1_10000sim.txt SIM[1]\n",
    "filename = \"models/I1_10000_SIM1_model.pickle\"\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "print('Trained model saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"/Users/chenwei/Desktop/Github/GSAE/saved_models/GSAE_trained_model_seq3\"\n",
    "filename = \"/Users/chenwei/Desktop/Github/RPE/code/models/I1_10000_SIM1_model.pickle\"\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without duplicates\n",
    "with torch.no_grad():\n",
    "        data_embed = model.embed(torch.Tensor(SIM_scar_uniq))[0]\n",
    "\n",
    "# # with duplicates\n",
    "# with torch.no_grad():\n",
    "#         data_tensor =  (torch.Tensor(SIM_scar),\n",
    "#                         torch.Tensor(SIM_G))\n",
    "#         data_embed = model.embed(data_tensor[0])[0]\n",
    "\n",
    "# data_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save embeded data to npy file\n",
    "\"\"\"\n",
    "# save GSAE embeded data\n",
    "with open('data/I1_sim1_embed_uniq.npy', 'wb') as f:\n",
    "    np.savez(f,data_embed=data_embed)\n",
    "\n",
    "npyfile0 = np.load(\"data/I1_sim1_embed_uniq.npy\")\n",
    "npyfile0.files\n",
    "data_embed = npyfile0[\"data_embed\"]\n",
    "data_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"save embeded data to h5 file\n",
    "# \"\"\"\n",
    "# def main():\n",
    "#     hf = h5py.File(\"data/I1_0_embed.h5\", \"w\")\n",
    "#     hf.create_dataset(\"data_embed\", data=data_embed)\n",
    "#     hf.close\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_coords = PCA(n_components=3).fit_transform(data_embed)\n",
    "pca_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save plot data to npy file\n",
    "\"\"\"  \n",
    "# save pca coordinates, energy, time data to npy file for Julia plot\n",
    "with open('data/I1_sim1_pca_uniq.npy', 'wb') as f:\n",
    "    np.savez(f,pca=pca_coords,energy=SIM_G_uniq,time_HT=SIM_HT_uniq,\n",
    "                  occp = occ_density,time_count=time_count,\n",
    "                  time_fract=time_fract)\n",
    "\n",
    "npyfile = np.load(\"data/I1_sim1_pca_uniq.npy\")\n",
    "npyfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = npyfile[\"pca\"][:,0]\n",
    "Y = npyfile[\"pca\"][:,1]\n",
    "Z = npyfile[\"pca\"][:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=npyfile[\"energy\"], \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: 2 components\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "im = ax.scatter3D(npyfile[\"pca1\"],\n",
    "          npyfile[\"pca2\"],\n",
    "          npyfile[\"energy\"],\n",
    "          c=npyfile[\"energy\"], \n",
    "          cmap='plasma')\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Energy\")\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [npyfile[\"pca1\"][0],npyfile[\"pca1\"][-1]]\n",
    "y = [npyfile[\"pca2\"][0],npyfile[\"pca2\"][-1]]\n",
    "z = [npyfile[\"energy\"][0], npyfile[\"energy\"][-1]]\n",
    "ax.scatter(x,y,z,s=150,c=\"green\",alpha=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: 3 components\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "im = ax.scatter3D(npyfile[\"pca1\"],\n",
    "          npyfile[\"pca2\"],\n",
    "          npyfile[\"pca3\"],\n",
    "          c=npyfile[\"energy\"],\n",
    "          cmap='plasma')\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [npyfile[\"pca1\"][0],npyfile[\"pca1\"][-1]]\n",
    "y = [npyfile[\"pca2\"][0],npyfile[\"pca2\"][-1]]\n",
    "z = [npyfile[\"pca3\"][0], npyfile[\"pca3\"][-1]]\n",
    "ax.scatter(x,y,z,s=200,c=\"green\",alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PHATE Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npyfile0 = np.load(\"data/I1_sim1_embed_uniq.npy\")\n",
    "data_gsae = npyfile0[\"data_embed\"]\n",
    "data_gsae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gsae = data_embed\n",
    "data_gsae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "Y_phate = phate_operator.fit_transform(data_gsae)\n",
    "Y_phate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save plot data to npy file\n",
    "\"\"\"  \n",
    "# save pca coordinates, energy, time data to npy file for Julia plot\n",
    "with open('data/I1_sim1_phate_uniq.npy', 'wb') as f:\n",
    "    np.savez(f,phate=Y_phate,energy=SIM_G_uniq,time_HT=SIM_HT_uniq,\n",
    "             occp=occ_density,time_count=time_count,\n",
    "             time_fract=time_fract)\n",
    "\n",
    "npyphate = np.load(\"data/I1_sim1_phate_uniq.npy\")\n",
    "Y_phate = npyphate[\"phate\"]\n",
    "Y_phate.shape, npyphate.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(Y_phate[:,0],\n",
    "          Y_phate[:,1],\n",
    "          c=npyphate[\"energy\"], \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [Y_phate[:,0][0],Y_phate[:,0][-1]]\n",
    "y = [Y_phate[:,1][0],Y_phate[:,1][-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = \"..(((..)))..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadj1 = dot2adj(aa)\n",
    "tadj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = \"..(((..)))..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadj2 = dot2adj(bb)\n",
    "tadj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(tadj1,tadj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_1 = transform_dataset(tadj1)\n",
    "scat_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_2 = transform_dataset(tadj2)\n",
    "scat_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(scat_1,scat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_coeff_array1 = transform_dataset(tadj1)\n",
    "norm_scat_coeffs = get_normalized_moments(scat_coeff_array1).squeeze()\n",
    "scat_coeff_array1 = norm_scat_coeffs\n",
    "scat_coeff_array1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_coeff_array2 = transform_dataset(tadj2)\n",
    "norm_scat_coeffs = get_normalized_moments(scat_coeff_array2).squeeze()\n",
    "scat_coeff_array2 = norm_scat_coeffs\n",
    "scat_coeff_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(scat_coeff_array1,scat_coeff_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fname_data = \"/Users/chenwei/Desktop/Github/RPE/code/data/helix_assos/assos_PT0_1sim_20C_51_1985epoch.npz\"\n",
    "npyfile = np.load(fname_data)\n",
    "npyfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npyfile[\"SIM_dict\"][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('GSAE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "960be4cf653645892f8c5040dafd36ede801108a57618a05d934dc04f8033a0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
