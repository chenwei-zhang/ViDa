{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from argparse import Namespace\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import yaml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import phate\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(621984,) (621984,) (46606,)\n",
      "(621984, 50, 50) (621984, 4000) (621984,) (621984,) (621984,)\n",
      "(46606, 50, 50) (46606, 4000) (46606,) (46606,)\n",
      "(621984, 5) (46606, 4)\n",
      "(621984,) (46606,) (100,) (46606,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load saved trajectories data for npz file\n",
    "\"\"\"\n",
    "SEQ = \"PT4\"\n",
    "# SEQ = \"PT4_hairpin\"\n",
    "\n",
    "# laod pre-training data\n",
    "fnpz_data = \"./data/pretraining/pretraining_{}.npz\".format(SEQ)\n",
    "data_npz = np.load(fnpz_data)\n",
    "\n",
    "# asssign data to variables\n",
    "for var in data_npz.files:\n",
    "     locals()[var] = data_npz[var]\n",
    "\n",
    "# recover full data based on coord_id, indices, and unique data\n",
    "SIMS_adj = SIMS_adj_uniq[coord_id_S]\n",
    "SIMS_scar = SIMS_scar_uniq[coord_id_S]\n",
    "SIMS_G = SIMS_G_uniq[coord_id_S]\n",
    "SIMS_pair = SIMS_pair_uniq[coord_id_S]\n",
    "\n",
    "print(SIMS_T.shape,SIMS_HT.shape,SIMS_HT_uniq.shape)\n",
    "print(SIMS_adj.shape,SIMS_scar.shape,SIMS_G.shape,SIMS_HT.shape,SIMS_pair.shape)\n",
    "print(SIMS_adj_uniq.shape,SIMS_scar_uniq.shape,SIMS_G_uniq.shape,SIMS_pair_uniq.shape) \n",
    "print(SIMS_dict.shape,SIMS_dict_uniq.shape)\n",
    "print(coord_id_S.shape,indices_S.shape,trj_id.shape,occ_density_S.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct weights (expected holding time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.602668547999989e-08 5.3379999818823536e-14 1.648221154642124e-09 (46606,)\n"
     ]
    }
   ],
   "source": [
    "print(SIMS_HT_uniq.max(), SIMS_HT_uniq[SIMS_HT_uniq!=0].min(), SIMS_HT_uniq.std(), SIMS_HT_uniq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible kinetic trap: \n",
      " ['.((((..........))))......+...((((.....))).......)..' '2.51719523185e-06'\n",
      " '-4.28761684582' '0']\n"
     ]
    }
   ],
   "source": [
    "print(\"Possible kinetic trap: \\n\", \n",
    "      SIMS_dict_uniq[SIMS_HT_uniq.argmax()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(621984,) [  0   1   2 ... 288 289 291] 46605\n",
      "Initial node: 0   Final node: 291\n"
     ]
    }
   ],
   "source": [
    "all_nodes = np.array(SIMS_dict[:,-1], dtype=int)\n",
    "print(all_nodes.shape, all_nodes, all_nodes.max())\n",
    "print(\"Initial node:\", all_nodes[0], \"  Final node:\", all_nodes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(coord_id_S == np.array(SIMS_dict[:,-1], dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise nodes\n",
    "# note: this step connect final->initial node,\n",
    "# which will be remove later\n",
    "all_edges_temp = []\n",
    "for previous, current in zip(all_nodes, all_nodes[1:]):\n",
    "    all_edges_temp.append((previous, current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove all edges that connect final->initial node\n",
    "# all_edges = list(filter((all_nodes[-1],all_nodes[0]).__ne__, all_edges_temp))\n",
    "import copy\n",
    "\n",
    "indices_to_delete = trj_id[:-1]\n",
    "# # Sort the indices in reverse order so that deleting elements won't affect subsequent indices\n",
    "indices_to_delete = sorted(indices_to_delete, reverse=True)\n",
    "\n",
    "all_edges = copy.deepcopy(all_edges_temp)\n",
    "for index in indices_to_delete:\n",
    "    del all_edges[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no edges from final node 291 to initial node 0\n",
      "before prune:  621884\n",
      "after prune:  621983\n",
      "difference:  99\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "if (all_nodes[-1],all_nodes[0]) in all_edges:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(f\"There are no edges from final node {all_nodes[-1]} to initial node {all_nodes[0]}\")\n",
    "    \n",
    "print(\"before prune: \", len(all_edges))\n",
    "print(\"after prune: \",len(all_edges_temp))\n",
    "print(\"difference: \", len(all_edges_temp) - len(all_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct directed weight graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DG = nx.DiGraph()\n",
    "for i in range(len(all_edges)):\n",
    "    weight = SIMS_HT_uniq[all_edges[i][0]]  \n",
    "    DG.add_edge(int(all_edges[i][0]), int(all_edges[i][1]), weight = float(weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.14611817e-10, 9.86611864e-10, 3.16825110e-09, ...,\n",
       "       1.41202200e-10, 2.80204730e-09, 9.62795700e-10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIMS_HT_uniq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct modified undirected weight graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MUG = nx.Graph()\n",
    "\n",
    "# for  i in range(len(all_edges)):\n",
    "#     idx0 = all_edges[i][0]\n",
    "#     idx1 = all_edges[i][1]\n",
    "    \n",
    "#     if SIMS_HT_uniq[idx0] < SIMS_HT_uniq[idx1]:\n",
    "#         weight = SIMS_HT_uniq[idx0]\n",
    "#     else:\n",
    "#         weight = SIMS_HT_uniq[idx1]\n",
    "        \n",
    "#     if SIMS_HT_uniq[idx0] == 0 or SIMS_HT_uniq[idx1] == 0:\n",
    "#         weight = SIMS_HT_uniq[idx0] + SIMS_HT_uniq[idx1]\n",
    "        \n",
    "#     MUG.add_edge(int(all_edges[i][0]), int(all_edges[i][1]), weight = float(weight))\n",
    "    \n",
    "    \n",
    "# MUG.get_edge_data(98,99), MUG.get_edge_data(97,98), MUG.get_edge_data(98,97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "start_time = time.time()\n",
    "# collect the shortest path for each node\n",
    "knn = 100\n",
    "X_j = []\n",
    "D_ij = []\n",
    "\n",
    "# for i in range(len(SIMS_HT_uniq)):\n",
    "for i in range(200):\n",
    "\n",
    "    # length = nx.single_source_dijkstra_path_length(DG, i)\n",
    "    length = nx.single_source_shortest_path_length(DG, i, cutoff=100)\n",
    "    \n",
    "    length_arr = np.array(list(length.items()), dtype=object)\n",
    "\n",
    "    X_j.append(length_arr[:,0])\n",
    "    D_ij.append(length_arr[:,1])\n",
    "    \n",
    "    if i % 5000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "X_j = np.array(X_j)\n",
    "D_ij = np.array(D_ij)      \n",
    "\n",
    "end_time = time.time()  \n",
    "print(f\"--- {end_time - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j = np.array(np.load(f'./data/graph/{SEQ}/shortestpath_knn=100.npz',allow_pickle=True)[\"X_j\"], dtype=int)\n",
    "D_ij = np.array(np.load(f'./data/graph/{SEQ}/shortestpath_knn=100.npz',allow_pickle=True)[\"D_ij\"], dtype=float)\n",
    "\n",
    "print(X_j.shape, D_ij.shape)\n",
    "print(D_ij.max(), D_ij.min(), D_ij.mean(), D_ij.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect and save KNN=n neighbor's shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import heapq\n",
    "\n",
    "def dijkstra_n_shortest_paths(graph, source, n_neigh=100):\n",
    "    # Initialize data structures\n",
    "    visited = set()\n",
    "    distances = {node: float('infinity') for node in graph}\n",
    "    distances[source] = 0\n",
    "    priority_queue = [(0, source)]\n",
    "    paths = []\n",
    "\n",
    "    # Main loop\n",
    "    while priority_queue and len(paths) < n_neigh:\n",
    "                \n",
    "        _, current_node = heapq.heappop(priority_queue)\n",
    "        \n",
    "        # print(current_node)\n",
    "        \n",
    "        if current_node in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(current_node)\n",
    "        \n",
    "        print(current_node)\n",
    "\n",
    "        for neighbor, weight in graph[current_node].items():\n",
    "            if neighbor not in visited:\n",
    "                print('neighbor', neighbor)\n",
    "                tentative_distance = distances[current_node] + weight['weight']\n",
    "\n",
    "                if tentative_distance < distances[neighbor]:\n",
    "                    distances[neighbor] = tentative_distance\n",
    "                    heapq.heappush(priority_queue, (tentative_distance, neighbor))\n",
    "                    print('priority_queue', priority_queue)\n",
    "                            \n",
    "        paths.append((source, current_node, distances[current_node]))\n",
    "        \n",
    "    return np.array(paths)\n",
    "\n",
    "\n",
    "x_j, d_ij = [], []\n",
    "n_neigh = 100\n",
    "\n",
    "# for i in range(len(DG.nodes)):\n",
    "#     shortest_path_i = dijkstra_n_shortest_paths(DG, i)\n",
    "#     x_j.append(shortest_path_i[:,1].astype(int))\n",
    "#     d_ij.append(shortest_path_i[:,2].astype(float))\n",
    "    \n",
    "#     if len(x_j[i]) < n_neigh:\n",
    "#         x_j[i] = np.pad(x_j[i], (0, n_neigh-len(x_j[i])), 'constant', constant_values=i)\n",
    "#         d_ij[i] = np.pad(d_ij[i], (0, n_neigh-len(d_ij[i])), 'constant', constant_values=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weighted directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "nodes = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add weighted edges\n",
    "edges = [('A', 'B', 1), ('A', 'C', 4), ('B', 'C', 2), ('B', 'D', 5),\n",
    "         ('C', 'E', 3), ('D', 'E', 1), ('D', 'F', 7), ('E', 'F', 2), \n",
    "         ('E', 'G', 5), ('F', 'G', 1)]\n",
    "G.add_weighted_edges_from(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dijkstra_n_shortest_paths(G, \"A\", n_neigh=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weighted directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "nodes = ['A', 'B', 'C', 'D', 'E']\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add weighted edges\n",
    "edges = [('A', 'B', 1), ('B', 'C', 3), ('A', 'D', 3), ('C', 'E', 1)]\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "dijkstra_n_shortest_paths(G, \"A\", n_neigh=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the distance\n",
    "min_val = np.min(D_ij)\n",
    "max_val = np.max(D_ij)\n",
    "norm_Dij = (D_ij - min_val) / (max_val - min_val)\n",
    "norm_Dij.min(), norm_Dij.max(), norm_Dij.mean(), norm_Dij.std()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the importance weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability of being visited during a simulated trajectory \n",
    "# from the initial state\n",
    "split_id = trj_id + 1 # index for split to each trajectory\n",
    "P_tot = np.zeros(len(SIMS_dict_uniq))\n",
    "\n",
    "for i in range(len(split_id)):\n",
    "    if i == 0:\n",
    "        trj = set(SIMS_dict[0:split_id[i],4].astype(int))\n",
    "    else:\n",
    "        trj = set(SIMS_dict[split_id[i-1]:split_id[i],4].astype(int))\n",
    "\n",
    "    P_tot[list(trj)] += 1\n",
    "\n",
    "P_tot = P_tot / 100\n",
    "\n",
    "P_tot.shape, P_tot.max(), P_tot.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "def edit_distance(adj1, adj2):\n",
    "    # Calculate edit distance based on its adjacency matrix\n",
    "    edit_dist = np.sum(np.abs(adj1-adj2),dtype=int)\n",
    "    \n",
    "    return edit_dist\n",
    "\n",
    "\n",
    "# Calculate the edit distance between X_i and X_j'\n",
    "ED_ij = []\n",
    "for i in range(X_j.shape[0]):\n",
    "    ed_ij = []\n",
    "    for j in range(X_j.shape[1]):\n",
    "        ed_ij.append(edit_distance(SIMS_adj_uniq[i], SIMS_adj_uniq[X_j[i,j]]))\n",
    "    ED_ij.append(ed_ij)\n",
    "ED_ij = np.array(ED_ij, dtype=int)\n",
    "    \n",
    "# save npz file for shortest path\n",
    "with open(f'./data/graph/{SEQ}_edit_distance.npz', 'wb') as f:\n",
    "    np.savez(f,\n",
    "             ED_ij = ED_ij,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ED_ij = np.array(np.load(f'./data/graph/{SEQ}/{SEQ}_edit_distance.npz',allow_pickle=True)[\"ED_ij\"], dtype=float)\n",
    "X_j = np.array(np.load(f'./data/graph/{SEQ}/shortestpath_knn=100.npz',allow_pickle=True)[\"X_j\"], dtype=int)\n",
    "print(ED_ij.max(), ED_ij.min(), ED_ij.mean(), ED_ij.std(), ED_ij.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46606, 50, 50)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIMS_adj_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(adj1, adj2):\n",
    "    # Calculate edit distance based on its adjacency matrix\n",
    "    edit_dist = np.sum(np.abs(adj1-adj2),dtype=int)\n",
    "    \n",
    "    return edit_dist\n",
    "\n",
    "\n",
    "# Calculate the edit distance between X_i and X_j'\n",
    "ED_ij = []\n",
    "for i in range(SIMS_adj_uniq.shape[0]):\n",
    "    ed_ij = []\n",
    "    for j in range(SIMS_adj_uniq.shape[0]):\n",
    "        ed_ij.append(edit_distance(SIMS_adj_uniq[i], SIMS_adj_uniq[j]))\n",
    "    ED_ij.append(ed_ij)\n",
    "ED_ij = np.array(ED_ij, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 52, 54, ..., 42, 16, 10])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ED_ij[291]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['(((((((((((((((((((((((((+)))))))))))))))))))))))))',\n",
       "       '1.35942471353e-06', '-38.9130860896', '1'], dtype='<U51')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIMS_dict_uniq[291]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViDa Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tup = (torch.Tensor(SIMS_scar_uniq),\n",
    "            torch.Tensor(SIMS_G_uniq),\n",
    "            torch.arange(len(SIMS_scar_uniq)))\n",
    "data_dataset = torch.utils.data.TensorDataset(*data_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and validation\n",
    "data_size = len(data_dataset)\n",
    "train_size = int(0.7 * data_size)\n",
    "\n",
    "val_size = data_size - train_size\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(data_dataset, [train_size, val_size], \n",
    "                                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(data_size, len(train_data), len(val_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters\n",
    "input_dim = data_tup[0].shape[-1]\n",
    "\n",
    "config = Namespace(\n",
    "    seq = SEQ,\n",
    "    type = 'vida',\n",
    "    knn = 100,\n",
    "    edit_distance = \"100_neighbors\",\n",
    "    device = 'mps', # change to cuda if using Nvida GPU\n",
    "    log_dir = f'{time.strftime(\"%m%d-%H%M\")}', # log directory\n",
    "    batch_size = 256,\n",
    "    input_dim = input_dim,\n",
    "    output_dim = input_dim,\n",
    "    latent_dim = 25, # bottleneck dimension\n",
    "    hidden_dim = 400,\n",
    "    n_epochs = 150, #(try 60 for pt4-hairpin)\n",
    "    \n",
    "    learning_rate = 1e-4, # learning rate #try 6e-5 for pt4-hairpin\n",
    "    \n",
    "    log_interval = 10, # how many batches to wait before logging training status    \n",
    "    patience = 20, # how many epochs to wait before early stopping    \n",
    "      \n",
    "    # hyperparameters for loss function\n",
    "    alpha = 1.0, # reconstruction loss\n",
    "    \n",
    "    beta = 1e-4, # kl divergence\n",
    "    # beta = 1e-5, # kl divergence\n",
    "    \n",
    "    gamma = 0.3, # energy loss   # 0.3 for PT4; \n",
    "    # gamma = 1, # energy loss\n",
    "\n",
    "    delta = 1e-4, # distance loss # 0.04 for PT4;\n",
    "    # delta = 4e-4, # distance loss # 0.04 for PT4;\n",
    "  \n",
    "    epsilon = 1e-4, # edit distance loss\n",
    "    # epsilon = 5e-5, # edit distance loss\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make datalaoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(data_dataset, batch_size=config.batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=config.batch_size, \n",
    "                                           shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=config.batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_loader: ', len(data_loader.dataset), len(data_loader), data_loader.batch_size)\n",
    "print('train_loader: ', len(train_loader.dataset), len(train_loader), train_loader.batch_size)\n",
    "print('val_loader: ', len(val_loader.dataset), len(val_loader), val_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.dataset.tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.dataset.tensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        '''\n",
    "        Args:\n",
    "        ----\n",
    "            - input_dim: the dimension of the input node feature\n",
    "            - hiddent_dim: the dimension of the hidden layer\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        \n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution, note how we only output\n",
    "        # diagonal values of covariance matrix. Here we assume\n",
    "        # they are conditionally independent\n",
    "        self.hid2mu = nn.Linear(400, self.latent_dim)\n",
    "        self.hid2logvar = nn.Linear(400, self.latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.fc1(x)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        mu = self.hid2mu(x)\n",
    "        logvar = self.hid2logvar(x)\n",
    "        return mu, logvar\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        '''\n",
    "        Args:\n",
    "        ----\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "            - hiddent_dim: the dimension of the hidden layer\n",
    "            - output_dim: the dimension of the output node feature\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.latent_dim, self.hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        self.fc3 = nn.Linear(400, self.output_dim)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.bn1(F.relu(self.fc1(z)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        '''\n",
    "        The regressor is used to predict the energy of the node\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "        '''\n",
    "        super(Regressor, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.regfc1 = nn.Linear(self.latent_dim, 15)\n",
    "        self.regfc2 = nn.Linear(15, 1)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        y = F.relu(self.regfc1(z))\n",
    "        y = self.regfc2(y)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIDA(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, regressor):\n",
    "        '''\n",
    "        Args:\n",
    "        ----\n",
    "            - input_dim: the dimension of the input node feature\n",
    "            - hiddent_dim: the dimension of the hidden layer\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "            - output_dim: the dimension of the output node feature (same as input_dim)\n",
    "        '''\n",
    "        super(VIDA, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.regressor = regressor\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps*std\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        y_pred = self.regressor(z)\n",
    "        return x_recon, y_pred, z, mu, logvar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_recon, x, mu, logvar):\n",
    "    '''\n",
    "    Compute the VAE loss\n",
    "    \n",
    "    Args:\n",
    "        - x_recon: the reconstructed node feature\n",
    "        - x: the original node feature\n",
    "        - mu: the mean of the latent space\n",
    "        - logvar: the log variance of the latent space\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing (scalar) the loss for the VAE\n",
    "    '''\n",
    "    BCE = F.mse_loss(x_recon.flatten(), x.flatten()) # L2 loss\n",
    "    # BCE = F.l1_loss(x_recon.flatten(), x.flatten()) # L1 loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE, KLD\n",
    "\n",
    "\n",
    "def pred_loss(y_pred, y):\n",
    "    '''\n",
    "    Compute the energy prediction loss\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        - y_pred: the predicted energy of the node\n",
    "        - y: the true energy of the node\n",
    "    \n",
    "    Returns:\n",
    "        - loss: PyTorch Tensor containing (scalar) the loss for the prediction\n",
    "    '''\n",
    "    return F.mse_loss(y_pred.flatten(), y.flatten())\n",
    "\n",
    "\n",
    "def distance_knn_loss_torch(config, zi, zj, Dij, P_tot, idx, batchXj_id):\n",
    "    '''\n",
    "    Compute the distance loss between embeddings \n",
    "    and the minimum expected holding time\n",
    "    \n",
    "    Args:\n",
    "        - zi: the embedding of the node i\n",
    "        - zj: the embedding of the nodes j's\n",
    "        - Dij: the post-processing distance between nodes i and j's\n",
    "        - P_tot: the total probability of the nodes i and j's\n",
    "        - idx: the index of the node i\n",
    "        - batchXj_id: the index of the nodes j's\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing (scalar) the loss for the embedding distance\n",
    "    '''\n",
    "    zi = zi.reshape(-1,1,zi.shape[-1])\n",
    "    l2_zizj = torch.sqrt(torch.sum((zi-zj)**2, dim=-1))\n",
    "    dist_diff = (l2_zizj - (Dij[idx]).to(config.device))**2\n",
    "\n",
    "    wij = (P_tot[idx].reshape(-1,1) * P_tot[batchXj_id]).to(config.device) # importance weight of nodes i and j       \n",
    "    dist_loss = torch.sum(dist_diff * wij)\n",
    "    return dist_loss\n",
    "\n",
    "def edit_distance_loss(config, zi, zj, ED_ij, idx):\n",
    "    '''\n",
    "    Compute the edit distance loss between embeddings \n",
    "    \n",
    "    Args:\n",
    "        - zi: the embedding of the node i\n",
    "        - zj: the embedding of the nodes j's\n",
    "        - ED_ij: the post-processing distance between nodes i and j's\n",
    "        - idx: the index of the node i\n",
    "        - batchXj_id: the index of the nodes j's\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing (scalar) the loss for the embedding distance\n",
    "    '''\n",
    "    zi = zi.reshape(-1,1,zi.shape[-1])\n",
    "    l2_zizj = torch.sqrt(torch.sum((zi-zj)**2, dim=-1))\n",
    "    dist_diff = (l2_zizj - (ED_ij[idx]).to(config.device))**2\n",
    "    # wij = (P_tot[idx].reshape(-1,1) * P_tot[batchXj_id]).to(config.device) # importance weight of nodes i and j       \n",
    "    # dist_loss = torch.sum(dist_diff * wij)\n",
    "    editdist_loss = torch.sum(dist_diff)\n",
    "    return editdist_loss\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early_stopping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop(val_loss, epoch, patience):\n",
    "    \"\"\"\n",
    "    Check if validation loss has not improved for a certain number of epochs\n",
    "    \n",
    "    Args:\n",
    "        val_loss (float): current validation loss\n",
    "        epoch (int): current epoch number\n",
    "        patience (int): number of epochs to wait before stopping if validation loss does not improve\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if validation loss has not improved for the last `patience` epochs, False otherwise\n",
    "    \"\"\"\n",
    "    if epoch == 0:\n",
    "        # First epoch, don't stop yet\n",
    "        return False\n",
    "    else:\n",
    "        # Check if validation loss has not improved for `patience` epochs\n",
    "        if val_loss >= early_stop.best_loss:\n",
    "            early_stop.num_epochs_without_improvement += 1\n",
    "            if early_stop.num_epochs_without_improvement >= patience:\n",
    "                print(\"Stopping early\")\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            # Validation loss improved, reset counter\n",
    "            early_stop.best_loss = val_loss\n",
    "            early_stop.num_epochs_without_improvement = 0\n",
    "            return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function\n",
    "def validate(config, model, data_loader, val_loader, P_tot, Dij, ED_ij, X_j,\n",
    "             vae_loss, pred_loss, distance_knn_loss, edit_distance_loss):\n",
    "    model.to(config.device)\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0; val_bce = 0; val_kld = 0; val_pred = 0; val_dist = 0; val_edit = 0\n",
    "    \n",
    "    # Disable gradient calculation to speed up inference\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_loader:\n",
    "            \n",
    "            # Configure input\n",
    "            x = x.to(config.device)\n",
    "            y = y.to(config.device)\n",
    "            \n",
    "            # forward X_j\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                batchXj_id = X_j[idx]\n",
    "                neighbor_input = data_loader.dataset.tensors[0][batchXj_id].reshape(-1, config.input_dim).to(config.device)\n",
    "                _, _, neighbor_embed, _, _ = model(neighbor_input)\n",
    "                neighbor_embed = neighbor_embed.reshape(-1, config.knn, neighbor_embed.shape[-1])\n",
    "                \n",
    "            # embedding\n",
    "            x_recon, y_pred, z, mu, logvar = model(x)\n",
    "            \n",
    "            # # compute the total loss\n",
    "            # vae loss\n",
    "            recon_loss, kl_loss = vae_loss(x_recon, x, mu, logvar)\n",
    "            recon_loss = recon_loss.item()\n",
    "            kl_loss = kl_loss.item()\n",
    "            \n",
    "            # energy prediction loss\n",
    "            p_loss = pred_loss(y_pred, y).item()\n",
    "                \n",
    "            # distance loss\n",
    "            dist_loss = distance_knn_loss(config, z, neighbor_embed, Dij, P_tot, idx, batchXj_id).item()\n",
    "            \n",
    "            # edit distance loss\n",
    "            edit_loss = edit_distance_loss(config, z, neighbor_embed, ED_ij, idx).item()\n",
    "            \n",
    "            # scaling the loss\n",
    "            recon_loss = config.alpha * recon_loss\n",
    "            kl_loss = config.beta * kl_loss\n",
    "            p_loss = config.gamma * p_loss\n",
    "            dist_loss = config.delta * dist_loss\n",
    "            edit_loss = config.epsilon * edit_loss\n",
    "            \n",
    "            # total loss\n",
    "            loss = recon_loss + kl_loss + p_loss + dist_loss + edit_loss\n",
    "            \n",
    "            val_loss += loss\n",
    "            val_bce += recon_loss\n",
    "            val_kld += kl_loss\n",
    "            val_pred += p_loss\n",
    "            val_dist += dist_loss\n",
    "            val_edit += edit_loss\n",
    "                        \n",
    "    print('Validation Loss: {:.4f}'.format(val_loss/len(val_loader.dataset)))\n",
    "    \n",
    "    # Clear the cache\n",
    "    torch.cuda.empty_cache()\n",
    "    # torch.mps.empty_cache()\n",
    "    \n",
    "    return val_loss/len(val_loader.dataset), val_bce/len(val_loader.dataset), val_kld/len(val_loader.dataset), val_pred/len(val_loader.dataset), val_dist/len(val_loader.dataset), val_edit/len(val_loader.dataset)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, data_loader, train_loader, val_loader, P_tot, Dij, ED_ij, X_j,\n",
    "          optimizer, scheduler, vae_loss, pred_loss, distance_knn_loss, edit_distance_loss, early_stop,):\n",
    "    '''\n",
    "    Train VIDA!\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        - config: Experiment configurations\n",
    "        - model: Pytorch VIDA model\n",
    "        - data_loader: Pytorch DataLoader for all data\n",
    "        - train_loader: Pytorch DataLoader for training set\n",
    "        - val_loader: Pytorch DataLoader for validation set\n",
    "        - P_tot: the total probability of each node\n",
    "        - Dij: the shortest path distance between k nearest pairs of nodes\n",
    "        - X_j: the index of k nearest neighbours of each node\n",
    "        - optimizer: Pytorch optimizer\n",
    "        - scheduler: Pytorch learning rate scheduler\n",
    "        - vae_loss: the VAE loss function\n",
    "        - pred_loss: the energy prediction loss function\n",
    "        - distant_knn_loss: the k nearest neighbour distance loss function\n",
    "    '''\n",
    "    \n",
    "    model.to(config.device)\n",
    "    \n",
    "    log_dir = f'./model_config/{config.log_dir}'\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    # save the config file\n",
    "    with open(f'{log_dir}/hparams.yaml','w') as f:\n",
    "        yaml.dump(config,f)\n",
    "    \n",
    "    # Initialize early stop object\n",
    "    early_stop.best_loss = np.Inf\n",
    "    early_stop.num_epochs_without_improvement = 0\n",
    "\n",
    "    # convert the numpy array to tensor\n",
    "    P_tot = torch.from_numpy(P_tot.astype(np.float32))\n",
    "    Dij = torch.from_numpy(Dij.astype(np.float32))\n",
    "    ED_ij = torch.from_numpy(ED_ij.astype(int))\n",
    "    X_j = torch.from_numpy(X_j)\n",
    "    \n",
    "    print('\\n ------- Start Training -------')\n",
    "    for epoch in range(config.n_epochs):\n",
    "        start_time = time.time()\n",
    "        training_loss = 0\n",
    "        \n",
    "        for batch_idx, (x, y, idx) in enumerate(train_loader):  # mini batch\n",
    "            \n",
    "            # Configure input\n",
    "            x = x.to(config.device)\n",
    "            y = y.to(config.device)\n",
    "            \n",
    "            # forward X_j\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                batchXj_id = X_j[idx]\n",
    "                neighbor_input = data_loader.dataset.tensors[0][batchXj_id].reshape(-1, config.input_dim).to(config.device)\n",
    "                _, _, neighbor_embed, _, _ = model(neighbor_input)\n",
    "                neighbor_embed = neighbor_embed.reshape(-1, config.knn, neighbor_embed.shape[-1])\n",
    "                \n",
    "            # ------------------------------------------\n",
    "            #  Train VIDA\n",
    "            # ------------------------------------------\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get the reconstructed nodes, predicted energy, and the embeddings\n",
    "            x_recon, y_pred, z, mu, logvar = model(x)\n",
    "        \n",
    "            ## compute the total loss\n",
    "            # vae loss\n",
    "            recon_loss, kl_loss = vae_loss(x_recon, x, mu, logvar)\n",
    "            \n",
    "            # energy prediction loss\n",
    "            p_loss = pred_loss(y_pred, y)\n",
    "\n",
    "            # distance loss\n",
    "            dist_loss = distance_knn_loss(config, z, neighbor_embed, Dij, P_tot, idx, batchXj_id)\n",
    "\n",
    "            # edit distance loss\n",
    "            edit_loss = edit_distance_loss(config, z, neighbor_embed, ED_ij, idx)\n",
    "            \n",
    "            # scaling the loss\n",
    "            recon_loss = config.alpha * recon_loss\n",
    "            kl_loss = config.beta * kl_loss\n",
    "            p_loss = config.gamma * p_loss\n",
    "            dist_loss = config.delta * dist_loss\n",
    "            edit_loss = config.epsilon * edit_loss\n",
    "            \n",
    "            # total loss\n",
    "            loss = recon_loss + kl_loss + p_loss + dist_loss + edit_loss\n",
    "            \n",
    "            # backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item()\n",
    "            \n",
    "            # ------------------------------------------\n",
    "            # Log Progress\n",
    "            # ------------------------------------------\n",
    "            if batch_idx % config.log_interval == 0:\n",
    "                print('Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, config.n_epochs, batch_idx * len(x), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item()))\n",
    "                \n",
    "                writer.add_scalar('training loss',\n",
    "                                  loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('recon loss',\n",
    "                                  recon_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('kl loss',\n",
    "                                  kl_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('pred loss',\n",
    "                                  p_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('dist loss',\n",
    "                                  dist_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('edit loss',\n",
    "                                  edit_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "    \n",
    "        print ('====> Epoch: {} Average loss: {:.4f}'.format(epoch, training_loss/len(train_loader.dataset)))\n",
    "        writer.add_scalar('epoch training loss', training_loss/len(train_loader.dataset), epoch)\n",
    "        \n",
    "        # validation\n",
    "        val_loss, val_bce, val_kld, val_pred, val_dist, val_edit = validate(config, model, data_loader, val_loader, P_tot, Dij, ED_ij, X_j, \n",
    "                                                                  vae_loss, pred_loss, distance_knn_loss, edit_distance_loss)\n",
    "        writer.add_scalar('validation loss', val_loss, epoch)\n",
    "        writer.add_scalar('val_recon loss', val_bce, epoch)\n",
    "        writer.add_scalar('val_kl loss', val_kld, epoch)\n",
    "        writer.add_scalar('val_pred loss', val_pred, epoch)\n",
    "        writer.add_scalar('val_dist loss', val_dist, epoch)\n",
    "        writer.add_scalar('val_edit loss', val_edit, epoch)\n",
    "\n",
    "        # log the learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar('learning rate', current_lr, epoch)\n",
    "        \n",
    "        # update the learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # timing\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        print (f'Epoch {epoch} train+val time: {epoch_time:.2f} seconds \\n')\n",
    "        \n",
    "        # Check if validation loss has not improved for `patience` epochs\n",
    "        if early_stop(val_loss, epoch, config.patience):\n",
    "            break\n",
    "    \n",
    "        # Clear the cache\n",
    "        # torch.mps.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "    writer.close()\n",
    "    print('\\n ------- Finished Training -------')\n",
    "    \n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), f'{log_dir}/model.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "encoder = Encoder(input_dim=config.input_dim, hidden_dim=config.hidden_dim, latent_dim=config.latent_dim)\n",
    "decoder = Decoder(latent_dim=config.latent_dim, hidden_dim=config.hidden_dim, output_dim=config.output_dim)\n",
    "regressor = Regressor(latent_dim=config.latent_dim)\n",
    "\n",
    "# Initialize ViDa \n",
    "vida = VIDA(encoder, decoder, regressor)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(vida.parameters(), lr=config.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train VIDA\n",
    "train(config, vida, data_loader, train_loader, val_loader, P_tot, norm_Dij, ED_ij, X_j,\n",
    "      optimizer, scheduler, vae_loss, pred_loss, distance_knn_loss_torch, edit_distance_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "model = VIDA(encoder, decoder, regressor)\n",
    "model.load_state_dict(torch.load('./model_config/0827-1520/model.pt',map_location=torch.device('cpu')))\n",
    "# model.load_state_dict(torch.load('./model_config/0303-0320/model.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VIDA(encoder, decoder, regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do inference\n",
    "model.to(config.device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "        _, _, z, _, _ = model(data_loader.dataset.tensors[0].to(config.device))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed = z.to('cpu').numpy()\n",
    "data_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_embed.max(), data_embed.min(), data_embed.mean(), data_embed.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do PCA for GSAE embeded data\n",
    "pca_coords = PCA(n_components=3).fit_transform(data_embed)\n",
    "\n",
    "# # get all pca embedded states coordinates\n",
    "pca_all_coords = pca_coords[coord_id_S]  # multiple trj\n",
    "\n",
    "pca_coords.shape, pca_all_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(pca_coords,axis=0)).shape, (np.unique(pca_all_coords,axis=0)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_embed = scaler.fit_transform(data_embed)\n",
    "data_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do PHATE for GSAE embeded data\n",
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "phate_coords = phate_operator.fit_transform(data_embed)\n",
    "\n",
    "# # get all phate embedded states coordinates\n",
    "phate_all_coords = phate_coords[coord_id_S]\n",
    "\n",
    "phate_coords.shape, phate_all_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(phate_coords,axis=0)).shape, (np.unique(phate_all_coords,axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct PCA/PHATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_scar_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do PCA for GSAE embeded data\n",
    "pca_coords = PCA(n_components=2).fit_transform(SIMS_scar_uniq)\n",
    "\n",
    "# # get all pca embedded states coordinates\n",
    "pca_all_coords = pca_coords[coord_id_S]  # multiple trj\n",
    "\n",
    "pca_coords.shape, pca_all_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_embed = scaler.fit_transform(SIMS_scar_uniq)\n",
    "# # do PHATE for GSAE embeded data\n",
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "phate_coords = phate_operator.fit_transform(data_embed)\n",
    "\n",
    "# # get all phate embedded states coordinates\n",
    "phate_all_coords = phate_coords[coord_id_S]\n",
    "\n",
    "phate_coords.shape, phate_all_coords.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "\"\"\" Save all DRs\n",
    "\"\"\"\n",
    "# save for python \n",
    "# fnpz_data = f\"data/vida_data/{SEQ}_{config.log_dir}.npz\"\n",
    "fnpz_data = f\"../output_files/saved_ViDa_plots/plot_dna29/dir_PCA_PHATE.npz\"\n",
    "\n",
    "\n",
    "with open(fnpz_data, 'wb') as f:\n",
    "    np.savez(f,\n",
    "            # embed data\n",
    "            data_embed=data_embed,\n",
    "            # plotting data\n",
    "            pca_coords=pca_coords, pca_all_coords=pca_all_coords,\n",
    "            phate_coords=phate_coords, phate_all_coords=phate_all_coords,\n",
    "            # umap_coord_2d=umap_coord_2d, umap_all_coord_2d=umap_all_coord_2d,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDS for embeded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "\"\"\"Both ways cause kernel crashed\n",
    "This MDS is designed for after-VAE embedding or direct for scattering transformed data\n",
    "\"\"\"\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# method 1\n",
    "X_eucl = pairwise_distances(data_embed, metric='euclidean')\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed')\n",
    "mds_coords = mds.fit_transform(X_eucl)\n",
    "\n",
    "# method 2\n",
    "mds = MDS(n_components=2)\n",
    "mds_coords = mds.fit_transform(data_embed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDS for distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make precomputed distance matrix for MDS\n",
    "MDS_dist = np.ones((D_ij.shape[0],D_ij.shape[0]))\n",
    "for i in range(len(D_ij)):\n",
    "    MDS_dist[i,X_j[i]] = D_ij[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSymmetric(mat):\n",
    "    # Loop to traverse lower triangular\n",
    "    # elements of the given matrix\n",
    "    for i in range(0, len(mat)):\n",
    "        for j in range(0, len(mat)):\n",
    "            if (j < i):\n",
    "                mat[i][j] = mat[j][i] = min(mat[i][j], mat[j][i])\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDS_dist_symm = makeSymmetric(MDS_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed')\n",
    "mds_coords = mds.fit_transform(MDS_dist_symm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fnpz_data_embed = f\"./data/vida_data/{SEQ}_0305-2258.npz\"\n",
    "# # fnpz_data_embed = f\"./data/vida_data/{SEQ}_usePT4_03040216.npz\"\n",
    "# data_npz_embed = np.load(fnpz_data_embed,allow_pickle=True)\n",
    "# # asssign data to variables\n",
    "# for var in data_npz_embed.files:\n",
    "#     globals()[var] = data_npz_embed[var]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_dist(X_j, D_ij, P_tot, z):\n",
    "    \"\"\"\n",
    "    Metric to calculate the distance \n",
    "    \"\"\"\n",
    "    z_re = z.reshape(-1,1,z.shape[-1])\n",
    "    zj = z[X_j]\n",
    "    l2_zizj = np.sqrt(np.sum((z_re-zj)**2, axis=-1))\n",
    "    \n",
    "    # normalize the distance\n",
    "    scaler = MinMaxScaler(feature_range=(1,3)) \n",
    "    l2_zizj = scaler.fit_transform(l2_zizj)\n",
    "    D_ij = scaler.fit_transform(D_ij)\n",
    "    \n",
    "    dist_diff = (l2_zizj - D_ij)**2\n",
    "    wij = (P_tot.reshape(-1,1) * P_tot[X_j])\n",
    "    dist_loss = np.sum(wij * dist_diff)/len(dist_diff)\n",
    "    return dist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dist = metric_dist(X_j, D_ij, P_tot, pca_coords[:,:2])\n",
    "phate_dist = metric_dist(X_j, D_ij, P_tot, phate_coords)\n",
    "# umap_dist = metric_dist(X_j, D_ij, P_tot, umap_coord_2d)\n",
    "print (f'PCA distance loss: {pca_dist:.4f}')\n",
    "print (f'PHATE distance loss: {phate_dist:.4f}')\n",
    "# print (f'UMAP distance loss: {umap_dist:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighboring preservation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def neighboring_preservation_rate(X, X_j, k):\n",
    "    \"\"\"\n",
    "    Metric to calculate the neighboring preservation rate \n",
    "    \"\"\"\n",
    "    # Compute the k-nearest neighbors for both X and Y.\n",
    "    nn_X = NearestNeighbors(n_neighbors=k+1).fit(X) # k+1 because we don't want to include the point itself\n",
    "    indices_X = nn_X.kneighbors(X,return_distance=False)[:,1:] # exclude the point itself\n",
    "    \n",
    "    # compute the rate of each point\n",
    "    rate_list = []\n",
    "    for i in range(len(indices_X)):\n",
    "        count = len(np.intersect1d(indices_X[i], X_j[i,:k]))\n",
    "        rate_i = count/k\n",
    "        rate_list.append(rate_i)\n",
    "        \n",
    "    # Compute the overall neighboring preservation rate\n",
    "    return np.mean(rate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnn = 100\n",
    "print(\"PCA rate: \", neighboring_preservation_rate(pca_coords[:,:2], X_j, k=knnn))\n",
    "print(\"PHATE rate: \", neighboring_preservation_rate(phate_coords, X_j, k=knnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = PCA(n_components=25)\n",
    "cm.fit(data_embed)\n",
    "\n",
    "PC_values = np.arange(cm.n_components_) + 1\n",
    "plt.plot(PC_values, np.cumsum(cm.explained_variance_ratio_), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot: PCA')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "# plt.xticks(np.arange(0, data_embed.shape[-1]+1, 1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(np.cumsum(cm.explained_variance_ratio_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "%matplotlib inline\n",
    "X = pca_all_coords[:,0]\n",
    "Y = pca_all_coords[:,1]\n",
    "Z = pca_all_coords[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G,\n",
    "          cmap='plasma',\n",
    "          s=20\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]*0.95),fontsize=15,c=\"yellow\", horizontalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "X = pca_coords[:,0]\n",
    "Y = pca_coords[:,1]\n",
    "Z = pca_coords[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[all_nodes[-1]]]\n",
    "\n",
    "y = [Y[0],Y[all_nodes[-1]]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "X = pca_coords[:,0]\n",
    "Y = pca_coords[:,1]\n",
    "Z = pca_coords[:,2]\n",
    "\n",
    "# PCA: 3 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "im = ax.scatter3D(X,Y,Z,\n",
    "          c=SIMS_G_uniq,      \n",
    "          cmap='plasma')\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "z = [Z[0], Z[-1]]\n",
    "ax.scatter(x,y,z,s=100,c=\"green\",alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "X = pca_coords[:,0]\n",
    "Y = pca_coords[:,1]\n",
    "Z = pca_coords[:,2]\n",
    "\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y,\n",
    "          c=SIMS_pair_uniq,\n",
    "          cmap='plasma',\n",
    "          s=15\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try use PCA directly without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "pca_coords_direct = PCA(n_components=3).fit_transform(SIMS_scar_uniq)   # multiple trj\n",
    "\n",
    "X = pca_coords_direct[:,0]\n",
    "Y = pca_coords_direct[:,1]\n",
    "Z = pca_coords_direct[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = PCA(n_components=25)\n",
    "cm.fit(SIMS_scar_uniq)\n",
    "\n",
    "PC_values = np.arange(cm.n_components_) + 1\n",
    "plt.plot(PC_values, np.cumsum(cm.explained_variance_ratio_), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot: PCA')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "# plt.xticks(np.arange(0, data_embed.shape[-1]+1, 1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(np.cumsum(cm.explained_variance_ratio_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHATE Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "X_phate = phate_all_coords[:,0]\n",
    "Y_phate = phate_all_coords[:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X_phate,Y_phate,\n",
    "                c=SIMS_G,   # multiple trj               \n",
    "                cmap='plasma',\n",
    "               )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X_phate[0],X_phate[-1]]\n",
    "y = [Y_phate[0],Y_phate[-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=30,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phate = phate_coords[:,0]\n",
    "Y_phate = phate_coords[:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X_phate,Y_phate,\n",
    "                c=SIMS_G_uniq,            \n",
    "                cmap='plasma',\n",
    "               )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X_phate[0],X_phate[all_nodes[-1]]]\n",
    "y = [Y_phate[0],Y_phate[all_nodes[-1]]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=30,c=\"black\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PHATE without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "phate1 = phate_operator.fit_transform(SIMS_scar_uniq)   # multiple trj\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(phate1[:,0],\n",
    "          phate1[:,1],\n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [phate1[:,0][0],phate1[:,0][-1]]\n",
    "y = [phate1[:,1][0],phate1[:,1][-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = umap_coord_2d[:,0]\n",
    "Y = umap_coord_2d[:,1]\n",
    "cmap = plt.cm.plasma\n",
    "cmap_r = plt.cm.get_cmap('plasma_r')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c = SIMS_G_uniq,\n",
    "          cmap=cmap,\n",
    "          s=10\n",
    "        )\n",
    " \n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[all_nodes[-1]]]\n",
    "y = [Y[0],Y[all_nodes[-1]]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# directly UMAP 2D\n",
    "umap_coord_2dscar = umap_2d.fit_transform(SIMS_scar_uniq)\n",
    "\n",
    "fig_2d = px.scatter(\n",
    "    umap_coord_2dscar, x=0, y=1,color=SIMS_G_uniq\n",
    ")\n",
    "fig_2d.update_traces(marker_size=3)\n",
    "fig_2d.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "fig_2d = px.scatter(\n",
    "    umap_coord_2d, x=0, y=1,color=SIMS_G_uniq\n",
    ")\n",
    "fig_2d.update_traces(marker_size=3)\n",
    "\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    umap_coord_3d, x=0, y=1, z=2,color=SIMS_G_uniq\n",
    ")\n",
    "\n",
    "fig_3d.update_traces(marker_size=2)\n",
    "\n",
    "fig_2d.show()\n",
    "fig_3d.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mds_coords[:,0]\n",
    "Y = mds_coords[:,1]\n",
    "cmap = plt.cm.plasma\n",
    "cmap_r = plt.cm.get_cmap('plasma_r')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c = SIMS_G_uniq,\n",
    "          cmap=cmap,\n",
    "          s=10\n",
    "        )\n",
    " \n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[all_nodes[-1]]]\n",
    "y = [Y[0],Y[all_nodes[-1]]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vida')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e442af4fad2330d8f4febe7e8e7250535e161341429a4f0b93cbf21b824330cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
