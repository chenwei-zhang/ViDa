{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pc\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import phate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepared embedded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load saved trajectories data for npz file\n",
    "\"\"\"\n",
    "SEQ = \"PT4\"\n",
    "# SEQ = \"PT4_hairpin\"\n",
    "\n",
    "# laod pre-training data\n",
    "fnpz_data = \"./data/pretraining/pretraining_{}.npz\".format(SEQ)\n",
    "data_npz = np.load(fnpz_data)\n",
    "\n",
    "# asssign data to variables\n",
    "for var in data_npz.files:\n",
    "     locals()[var] = data_npz[var]\n",
    "     print(var, locals()[var].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j = np.array(np.load(f'./data/graph/{SEQ}/shortestpath_knn=100.npz',allow_pickle=True)[\"X_j\"], dtype=int)\n",
    "D_ij = np.array(np.load(f'./data/graph/{SEQ}/shortestpath_knn=100.npz',allow_pickle=True)[\"D_ij\"], dtype=float)\n",
    "print(\"X_j\", X_j.shape)\n",
    "print(\"D_ij\", D_ij.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability of being visited during a simulated trajectory \n",
    "# from the initial state\n",
    "split_id = trj_id + 1 # index for split to each trajectory\n",
    "P_tot = np.zeros(len(SIMS_dict_uniq))\n",
    "\n",
    "for i in range(len(split_id)):\n",
    "    if i == 0:\n",
    "        trj = set(SIMS_dict[0:split_id[i],4].astype(int))\n",
    "    else:\n",
    "        trj = set(SIMS_dict[split_id[i-1]:split_id[i],4].astype(int))\n",
    "\n",
    "    P_tot[list(trj)] += 1\n",
    "\n",
    "P_tot = P_tot / 100\n",
    "\n",
    "print(\"P_tot\", P_tot.shape, P_tot.max(), P_tot.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# load embedding WITHOUT vida plot data\n",
    "# SEQ = \"PT0\"\n",
    "fnpz_noViDa = f\"./data/vida_data/noViDa-noEnergy/{SEQ}_noViDa.npz\"\n",
    "data_noViDa = np.load(fnpz_noViDa,allow_pickle=True)\n",
    "for var in data_noViDa.files:\n",
    "    globals()[var] = data_noViDa[var]\n",
    "    print(var, globals()[var].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ViDa embedding plot data\n",
    "\n",
    "fnpz_data_embed = f\"./data/vida_data/{SEQ}.npz\"\n",
    "# fnpz_data_embed = f\"./data/vida_data/{SEQ}_usePT4_03040216.npz\"\n",
    "data_npz_embed = np.load(fnpz_data_embed,allow_pickle=True)\n",
    "\n",
    "# asssign data to variables\n",
    "for var in data_npz_embed.files:\n",
    "    globals()[var] = data_npz_embed[var]\n",
    "    print(var, globals()[var].shape)\n",
    "print(\"data_embed:\", data_embed.max(), data_embed.min(), data_embed.mean(), data_embed.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# load GSAE embedding plot data\n",
    "\n",
    "fnpz_data_embed = f\"./data/gsae_data/helix_assoc_{SEQ}.npz\"\n",
    "\n",
    "data_npz_embed = np.load(fnpz_data_embed,allow_pickle=True)\n",
    "# asssign data to variables\n",
    "for var in data_npz_embed.files:\n",
    "    globals()[var] = data_npz_embed[var]\n",
    "    print(var, globals()[var].shape)\n",
    "print(\"data_embed\", data_embed.max(), data_embed.min(), data_embed.mean(), data_embed.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajec_embed(trj_id, embedding):\n",
    "    trajec_embed = []\n",
    "    TRJ_ID = trj_id+1\n",
    "    \n",
    "    for i in range(len(TRJ_ID)):\n",
    "        if i == 0:\n",
    "            s = 0\n",
    "            s_prime = TRJ_ID[i]\n",
    "        elif i == len(trj_id):\n",
    "            s = TRJ_ID[i-1]\n",
    "            s_prime = len(embedding)\n",
    "        else:\n",
    "            s = TRJ_ID[i-1]\n",
    "            s_prime = TRJ_ID[i]\n",
    "        \n",
    "        trajec_embed.append(embedding[s:s_prime][:,:2])\n",
    "    \n",
    "    return trajec_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_1trj_distance(arr):\n",
    "    total_distance = 0\n",
    "    \n",
    "    for i in range(1, len(arr)):\n",
    "        total_distance += np.sqrt(np.sum((arr[i] - arr[i-1])**2))\n",
    "        \n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the expected total distance over all trajectories\n",
    "def calculate_avg_total_distance(embedding, trj_id):\n",
    "    scaler = MinMaxScaler()\n",
    "    std_embedding = scaler.fit_transform(embedding) \n",
    "    \n",
    "    trajec_embed = get_trajec_embed(trj_id, std_embedding)\n",
    "    total_distance = 0\n",
    "    \n",
    "    for i in range(len(trajec_embed)):\n",
    "        total_distance += calculate_1trj_distance(trajec_embed[i])        \n",
    "        \n",
    "    return total_distance / (trj_id[-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ViDa PCA: {calculate_avg_total_distance(pca_all_coords, trj_id):.3f}\")\n",
    "print(f\"ViDa PHATE: {calculate_avg_total_distance(phate_all_coords, trj_id):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GSAE PCA: {calculate_avg_total_distance(pca_all_coords, trj_id):.3f}\")\n",
    "print(f\"GSAE PHATE: {calculate_avg_total_distance(phate_all_coords, trj_id):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Direct PCA: {calculate_avg_total_distance(pca_all_coords_direct, trj_id):.3f}\")\n",
    "print(f\"Direct PHATE: {calculate_avg_total_distance(phate_all_coords_direct, trj_id):.3f}\")\n",
    "# print(f\"Direct MDS: {calculate_avg_total_distance(mds_all_coords, trj_id):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_step_length(trj_id, embedding):\n",
    "    trajec_embed = get_trajec_embed(trj_id, embedding)\n",
    "    step_length = []\n",
    "    \n",
    "    for  i in range(len(trajec_embed)):\n",
    "        arr = trajec_embed[i]\n",
    "        for j in range(1, len(arr)):\n",
    "            step_length.append(np.sqrt(np.sum((arr[j] - arr[j-1])**2)))\n",
    "    return np.array(step_length)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_pdf(steps, num_bins):\n",
    "#     # Sort the data\n",
    "#     data = np.sort(steps)\n",
    "    \n",
    "#     # Group the data into the bins   \n",
    "#     counts, bin_edges = np.histogram(data, bins=num_bins)\n",
    "    \n",
    "#     # Calculate the density of data points in each bin\n",
    "#     density = counts / sum(counts)\n",
    "\n",
    "#     # Plot the PDF\n",
    "#     bin_widths = np.diff(bin_edges)\n",
    "#     bin_centers = bin_edges[:-1] + bin_widths / 2\n",
    "#     bin_centers = bin_centers / bin_centers.max() * 100\n",
    "\n",
    "#     plt.plot(bin_centers, density)\n",
    "#     plt.xticks(np.arange(0, 101, 10))\n",
    "#     plt.xlabel('Step length percentage (%)')\n",
    "#     plt.ylabel('Density')\n",
    "#     plt.title('PDF of step length for embedding')\n",
    "#     return plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pdf(steps, num_bins):\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    norm_steps = scaler.fit_transform(steps.reshape(-1,1))\n",
    "    \n",
    "    # Sort the data\n",
    "    data = np.sort(norm_steps)\n",
    "    \n",
    "    # Group the data into the bins   \n",
    "    make_pdf.counts, make_pdf.bin_edges = np.histogram(data, bins=num_bins)\n",
    "    \n",
    "    # Calculate the density of data points in each bin\n",
    "    make_pdf.density = make_pdf.counts / sum(make_pdf.counts)\n",
    "\n",
    "    # Plot the PDF\n",
    "    bin_widths = np.diff(make_pdf.bin_edges)\n",
    "    make_pdf.bin_centers = make_pdf.bin_edges[:-1] + bin_widths / 2\n",
    "    make_pdf.bin_centers = make_pdf.bin_centers / make_pdf.bin_centers.max() * 100\n",
    "\n",
    "    plt.plot(make_pdf.bin_centers, make_pdf.density)\n",
    "    plt.xticks(np.arange(0, 101, 10))\n",
    "    plt.xlabel('Step length percentage (%)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('PDF of step length for embedding')\n",
    "    return plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = calculate_step_length(trj_id, phate_all_coords)\n",
    "# steps = calculate_step_length(trj_id, phate_all_coords_direct)\n",
    "\n",
    "make_pdf(steps, num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pdf.bin_edges, make_pdf.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsae_phate = make_pdf.counts\n",
    "vida_pca, vida_phate, direct_pca, direct_phate, gsae_pca, gsae_phate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vida_pca, vida_phate, direct_pca, direct_phate, gsae_pca, gsae_phate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_widths = np.diff(make_pdf.bin_edges)\n",
    "bin_centers = make_pdf.bin_edges[:-1] + bin_widths / 2\n",
    "bin_centers = make_pdf.bin_centers / make_pdf.bin_centers.max() * 100\n",
    "bin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for plotting\n",
    "fnpz_data = f\"data/vida_data/pdf_plot_{SEQ}.npz\"\n",
    "with open(fnpz_data, 'wb') as f:\n",
    "    np.savez(f,\n",
    "            bin_centers=bin_centers,\n",
    "            vida_pca=vida_pca,\n",
    "            vida_phate=vida_phate,\n",
    "            gsae_pca=gsae_pca,\n",
    "            gsae_phate=gsae_phate,\n",
    "            direct_pca=direct_pca,\n",
    "            direct_phate=direct_phate,\n",
    "            # direct_mds=direct_mds,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a whole plot\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize =(15, 8))\n",
    "\n",
    "# Plot the lines\n",
    "ax.plot(bin_centers, vida_pca/sum(vida_pca), label='ViDa PCA')\n",
    "ax.plot(bin_centers, vida_phate/sum(vida_phate), label='ViDa PHATE')\n",
    "ax.plot(bin_centers, gsae_pca/sum(gsae_pca), label='GSAE PCA')\n",
    "ax.plot(bin_centers, gsae_phate/sum(gsae_phate), label='GSAE PHATE')\n",
    "ax.plot(bin_centers, direct_pca/sum(direct_pca), label='Direct PCA')\n",
    "ax.plot(bin_centers, direct_phate/sum(direct_phate), label='Direct PHATE')\n",
    "# ax.plot(bin_centers, direct_mds/sum(direct_mds), label='Direct MDS')\n",
    "\n",
    "# Set the legend\n",
    "ax.legend()\n",
    "\n",
    "plt.xticks(np.arange(0, 101, 10))\n",
    "plt.xlabel('Step length percentage (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f'PDF of step length for {SEQ}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = calculate_step_length(trj_id, pca_all_coords_direct)\n",
    "steps = calculate_step_length(trj_id, pca_all_coords)\n",
    "\n",
    "make_pdf(steps, num_bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsd(X_j, D_ij, P_tot, z):\n",
    "    \"\"\"\n",
    "    Metric to calculate the distance \n",
    "    \"\"\"\n",
    "    z_re = z.reshape(-1,1,z.shape[-1])\n",
    "    zj = z[X_j]\n",
    "    global l2_zizj\n",
    "    l2_zizj = np.sqrt(np.sum((z_re-zj)**2, axis=-1))\n",
    "    \n",
    "    # # normalize the distance\n",
    "    # scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "    # l2_zizj = scaler.fit_transform(l2_zizj)\n",
    "    # D_ij = scaler.fit_transform(D_ij)\n",
    "    \n",
    "    dist_diff = (l2_zizj - D_ij)**2\n",
    "    root = \n",
    "    wij = (P_tot.reshape(-1,1) * P_tot[X_j])\n",
    "    # dist_loss = np.sum(wij * dist_diff)\n",
    "    dist_loss = ((wij * dist_diff) * P_tot.reshape(-1,1) / P_tot.sum()).sum()\n",
    "    \n",
    "    \n",
    "    return dist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_dist(X_j, D_ij, P_tot, z):\n",
    "    \"\"\"\n",
    "    Metric to calculate the distance \n",
    "    \"\"\"\n",
    "    z_re = z.reshape(-1,1,z.shape[-1])\n",
    "    zj = z[X_j]\n",
    "    global l2_zizj\n",
    "    l2_zizj = np.sqrt(np.sum((z_re-zj)**2, axis=-1))\n",
    "    \n",
    "    # # normalize the distance\n",
    "    # scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "    # l2_zizj = scaler.fit_transform(l2_zizj)\n",
    "    # D_ij = scaler.fit_transform(D_ij)\n",
    "    \n",
    "    dist_diff = (l2_zizj - D_ij)**2\n",
    "    wij = (P_tot.reshape(-1,1) * P_tot[X_j])\n",
    "    # dist_loss = np.sum(wij * dist_diff)\n",
    "    dist_loss = ((wij * dist_diff) * P_tot.reshape(-1,1) / P_tot.sum()).sum()\n",
    "    \n",
    "    \n",
    "    return dist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Dij = np.log(D_ij)+np.abs(np.log(D_ij).min())\n",
    "print(\"log_Dij\", log_Dij.shape, log_Dij.max(), log_Dij.min())\n",
    "\n",
    "pca_dist = metric_dist(X_j, log_Dij, P_tot, pca_coords[:,:2])\n",
    "phate_dist = metric_dist(X_j, log_Dij, P_tot, phate_coords)\n",
    "print (f'ViDa PCA distance loss: {pca_dist:.4f}')\n",
    "print (f'ViDa PHATE distance loss: {phate_dist:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Dij = np.log(D_ij)+np.abs(np.log(D_ij).min())\n",
    "\n",
    "pca_dist = metric_dist(X_j, log_Dij, P_tot, pca_coords_direct[:,:2])\n",
    "phate_dist = metric_dist(X_j, log_Dij, P_tot, phate_coords_direct)\n",
    "print (f'Direct PCA distance loss: {pca_dist:.4f}')\n",
    "print (f'Direct PHATE distance loss: {phate_dist:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Dij = np.log(D_ij)+np.abs(np.log(D_ij).min())\n",
    "\n",
    "pca_dist = metric_dist(X_j, log_Dij, P_tot, pca_coords[:,:2])\n",
    "phate_dist = metric_dist(X_j, log_Dij, P_tot, pca_coords)\n",
    "print (f'GSAE PCA distance loss: {pca_dist:.4f}')\n",
    "print (f'GSAE Direct PHATE distance loss: {phate_dist:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighboring preservation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def neighboring_preservation_rate(X, X_j, P_tot, k):\n",
    "    \"\"\"\n",
    "    Metric to calculate the neighboring preservation rate \n",
    "    \"\"\"\n",
    "    # Compute the k-nearest neighbors for both X and Y.\n",
    "    nn_X = NearestNeighbors(n_neighbors=k+1).fit(X) # k+1 because we don't want to include the point itself\n",
    "    indices_X = nn_X.kneighbors(X,return_distance=False)[:,1:] # exclude the point itself\n",
    "    \n",
    "    # compute the rate of each point\n",
    "    rate_list = []\n",
    "    for i in range(len(indices_X)):\n",
    "        count = len(np.intersect1d(indices_X[i], X_j[i,:k]))\n",
    "        rate_i = count/k\n",
    "        rate_list.append(rate_i)\n",
    "    \n",
    "    # Compute the overall neighbsoring preservation rate\n",
    "    \n",
    "    # rate = np.mean(rate_list) # average\n",
    "    rate = (rate_list * P_tot / P_tot.sum()).sum() # weighted average\n",
    "    \n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViDa embedding\n",
    "knn = 10000\n",
    "print(\"ViDa PCA rate: {:.4f}\".format(neighboring_preservation_rate(pca_coords[:,:2], X_j_all, P_tot, k=knn)))\n",
    "print(\"ViDa PHATE rate: {:.4f}\".format(neighboring_preservation_rate(phate_coords, X_j_all, P_tot, k=knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained ViDa embedding\n",
    "knn = 10000\n",
    "print(\"Pretrained ViDa PCA rate: {:.4f}\".format(neighboring_preservation_rate(pca_coords[:,:2], X_j_all, P_tot, k=knn)))\n",
    "print(\"Pretrained ViDa PHATE rate: {:.4f}\".format(neighboring_preservation_rate(phate_coords, X_j_all, P_tot, k=knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSAE embedding\n",
    "knn = 10000\n",
    "print(\"GSAE PCA rate: {:.4f}\".format(neighboring_preservation_rate(pca_coords[:,:2], X_j_all, P_tot, k=knn)))\n",
    "print(\"GSAE PHATE rate: {:.4f}\".format(neighboring_preservation_rate(phate_coords, X_j_all, P_tot, k=knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct embedding\n",
    "knn = 100\n",
    "print(\"Direct PCA rate: {:.4f}\".format(neighboring_preservation_rate(pca_coords_direct[:,:2], X_j, P_tot, k=knn)))\n",
    "print(\"Direct PHATE rate: {:.4f}\".format(neighboring_preservation_rate(phate_coords_direct, X_j, P_tot, k=knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_ij_all = []\n",
    "# X_j_all = []\n",
    "\n",
    "# for i in range(len(SIMS_HT_uniq)):\n",
    "#     dij = np.array(list(np.load(f'./data/graph/{SEQ}/allpath_{SEQ}/path_{i}.npy',allow_pickle=True)[0].values()), dtype=float)\n",
    "#     xj = np.array(list(np.load(f'./data/graph/{SEQ}/allpath_{SEQ}/path_{i}.npy',allow_pickle=True)[0].keys()), dtype=int)\n",
    "    \n",
    "#     D_ij_all.append(dij)\n",
    "#     X_j_all.append(xj)\n",
    "\n",
    "# D_ij_all = np.stack(D_ij_all)\n",
    "# X_j_all = np.stack(X_j_all)\n",
    "\n",
    "# # save npz file for shortest path\n",
    "# with open(f'./data/graph/{SEQ}_allpath.npz', 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#              X_j_all = np.stack(X_j_all),\n",
    "#              D_ij_all = np.stack(D_ij_all),\n",
    "#          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = PCA(n_components=25)\n",
    "cm.fit(data_embed)\n",
    "\n",
    "PC_values = np.arange(cm.n_components_) + 1\n",
    "plt.plot(PC_values, np.cumsum(cm.explained_variance_ratio_), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot: PCA')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "# plt.xticks(np.arange(0, data_embed.shape[-1]+1, 1))\n",
    "plt.show()\n",
    "\n",
    "print(np.cumsum(cm.explained_variance_ratio_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|### MDS for distance matri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PT4_hairpin all path\n",
    "X_j_all = np.array(np.load(f'./data/graph/{SEQ}_allpath.npz',allow_pickle=True)[\"X_j_all\"])\n",
    "D_ij_all = np.array(np.load(f'./data/graph/{SEQ}_allpath.npz',allow_pickle=True)[\"D_ij_all\"])\n",
    "print(\"X_j_all\", X_j_all.shape)\n",
    "print(\"D_ij_all\", D_ij_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make precomputed distance matrix for MDS with \n",
    "MDS_dist = np.ones((D_ij_all.shape[0],D_ij_all.shape[0]))\n",
    "for i in range(len(D_ij_all)):\n",
    "    MDS_dist[i,X_j_all[i]] = D_ij_all[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDS_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary if the distance matrix includes all the neighbors\n",
    "def makeSymmetric(mat):\n",
    "    # Loop to traverse lower triangular\n",
    "    # elements of the given matrix\n",
    "    for i in range(0, len(mat)):\n",
    "        for j in range(0, len(mat)):\n",
    "            if (j < i):\n",
    "                mat[i][j] = mat[j][i] = min(mat[i][j], mat[j][i])\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDS_dist_symm = makeSymmetric(MDS_dist)\n",
    "MDS_dist_symm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed')\n",
    "mds_coords = mds.fit_transform(MDS_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save for python\n",
    "# pca_all_coords_direct = pca_coords_direct[coord_id_S]  \n",
    "# phate_all_coords_direct = phate_coords_direct[coord_id_S]\n",
    "# mds_all_coords = mds_coords[coord_id_S]\n",
    "\n",
    "# fnpz_data = f\"data/vida_data/noViDa-noEnergy/{SEQ}_noViDa.npz\"\n",
    "# with open(fnpz_data, 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#             pca_coords_direct=pca_coords_direct, pca_all_coords_direct=pca_all_coords_direct,\n",
    "#             phate_coords_direct=phate_coords_direct, phate_all_coords_direct=phate_all_coords_direct,\n",
    "#             mds_coords=mds_coords, mds_all_coords=mds_all_coords,\n",
    "#             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "X = pca_coords[:,0]\n",
    "Y = pca_coords[:,1]\n",
    "Z = pca_coords[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[int(SIMS_dict[-1,-1])]]\n",
    "y = [Y[0],Y[int(SIMS_dict[-1,-1])]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try use PCA directly without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "pca_coords_direct = PCA(n_components=3).fit_transform(SIMS_scar_uniq)   # multiple trj\n",
    "\n",
    "X = pca_coords_direct[:,0]\n",
    "Y = pca_coords_direct[:,1]\n",
    "Z = pca_coords_direct[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=20,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = PCA(n_components=25)\n",
    "cm.fit(SIMS_scar_uniq)\n",
    "\n",
    "PC_values = np.arange(cm.n_components_) + 1\n",
    "plt.plot(PC_values, np.cumsum(cm.explained_variance_ratio_), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot: PCA')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "# plt.xticks(np.arange(0, data_embed.shape[-1]+1, 1))\n",
    "plt.show()\n",
    "\n",
    "print(np.cumsum(cm.explained_variance_ratio_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHATE Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phate = phate_coords[:,0]\n",
    "Y_phate = phate_coords[:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X_phate,Y_phate,\n",
    "                c=SIMS_G_uniq,            \n",
    "                cmap='plasma',\n",
    "               )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X_phate[0],X_phate[int(SIMS_dict[-1,-1])]]\n",
    "y = [Y_phate[0],Y_phate[int(SIMS_dict[-1,-1])]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PHATE without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "phate_coords_direct = phate_operator.fit_transform(SIMS_scar_uniq)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(phate_coords_direct[:,0],\n",
    "          phate_coords_direct[:,1],\n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [phate_coords_direct[:,0][0],phate_coords_direct[:,0][-1]]\n",
    "y = [phate_coords_direct[:,1][0],phate_coords_direct[:,1][-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS with distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mds_coords[:,0]\n",
    "Y = mds_coords[:,1]\n",
    "cmap = plt.cm.plasma\n",
    "cmap_r = plt.cm.get_cmap('plasma_r')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c = SIMS_G_uniq,\n",
    "          cmap=cmap,\n",
    "          s=10\n",
    "        )\n",
    " \n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[int(SIMS_dict[-1,-1])]]\n",
    "y = [Y[0],Y[int(SIMS_dict[-1,-1])]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering to Find Kinetic Traps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_idx = 0\n",
    "# initial_idx = np.where(SIMS_dict_uniq[:,0] == '.........................+.........................')[0][0]\n",
    "final_idx = np.where(SIMS_dict_uniq[:,0] == '(((((((((((((((((((((((((+)))))))))))))))))))))))))')[0][0]\n",
    "print(initial_idx, final_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine all features into one array\n",
    "pos_data = data_embed\n",
    "energy_data = SIMS_G_uniq\n",
    "time_data = SIMS_HT_uniq\n",
    "frq_data = P_tot\n",
    "\n",
    "# # normalize different features to the same scale\n",
    "scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "\n",
    "norm_pos_data = scaler.fit_transform(pos_data)\n",
    "norm_energy_data = scaler.fit_transform(energy_data.reshape(-1,1))\n",
    "norm_time_data = scaler.fit_transform(time_data.reshape(-1,1))\n",
    "\n",
    "combined_data = np.concatenate((norm_pos_data, norm_energy_data, norm_time_data, frq_data.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out data points with low frequency\n",
    "\n",
    "## eps = 0.07, min_samples = 4 for PT0 with filter=0.13\n",
    "## eps = 0.14, min_samples = 4 for PT3 with filter=0.15\n",
    "## eps = 0.18, min_samples = 4 for PT4 with filter=0.2\n",
    "## eps = 0.08, min_samples = 4 for PT3_hairpin with filter=0.2\n",
    "## eps = 0.15, min_samples = 4 for PT4_hairpin with filter=0.5\n",
    "\n",
    "if SEQ == \"PT0\": \n",
    "    filter_threshold = 0.13\n",
    "elif SEQ == \"PT3\":\n",
    "    filter_threshold = 0.15\n",
    "elif SEQ == \"PT4\":\n",
    "    filter_threshold = 0.2\n",
    "elif SEQ == \"PT3_hairpin\":\n",
    "    filter_threshold = 0.2\n",
    "elif SEQ == \"PT4_hairpin\":\n",
    "    filter_threshold = 0.5\n",
    "\n",
    "filter_idx = np.where(P_tot>=filter_threshold)[0]\n",
    "filter_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do PCA for combined data\n",
    "\n",
    "# comb_pca_coords = pca_coords[:,:2]\n",
    "comb_pca_coords = PCA(n_components=2).fit_transform(combined_data)\n",
    "\n",
    "# #######################\n",
    "# import deeptime\n",
    "# from deeptime.decomposition import TICA\n",
    "# tica = TICA(lagtime=1,dim=2)\n",
    "# comb_pca_coords = tica.fit_transform(combined_data)\n",
    "# #######################\n",
    "\n",
    "## filtered data\n",
    "filter_comb_pca_coords = comb_pca_coords[filter_idx]\n",
    "filter_comb_pca_coords.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elbow method to find eps for DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "n_neighbors = 4  # Number of neighbors to find\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(filter_comb_pca_coords)\n",
    "distances, indices = nbrs.kneighbors(filter_comb_pca_coords)\n",
    "four_dist = np.sum(distances,axis=1)\n",
    "sorted_four_dist = np.sort(four_dist)[::-1]\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "# Add a line trace\n",
    "fig.add_trace(go.Scatter(x=indices[:,0], y=sorted_four_dist, \n",
    "                         mode='lines', name='Line Plot'))\n",
    "# Set labels and title\n",
    "fig.update_layout(xaxis_title='points', yaxis_title='4-dist', title='Elbow')\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "## eps = 0.07, min_samples = 4 for PT0 with filter=0.13\n",
    "## eps = 0.14, min_samples = 4 for PT3 with filter=0.15\n",
    "## eps = 0.18, min_samples = 4 for PT4 with filter=0.2\n",
    "## eps = 0.08, min_samples = 4 for PT3_hairpin with filter=0.2\n",
    "## eps = 0.15, min_samples = 4 for PT4_hairpin with filter=0.5\n",
    "\n",
    "if SEQ == 'PT0':\n",
    "    eps = 0.07\n",
    "elif SEQ == 'PT3':\n",
    "    eps = 0.14\n",
    "elif SEQ == 'PT4':\n",
    "    eps = 0.18\n",
    "elif SEQ == 'PT3_hairpin':\n",
    "    eps = 0.08\n",
    "elif SEQ == 'PT4_hairpin':\n",
    "    eps = 0.15\n",
    "\n",
    "\n",
    "X = filter_comb_pca_coords\n",
    "clusters = DBSCAN(eps = eps, min_samples = 4).fit(X)\n",
    "# get cluster labels\n",
    "labels = clusters.labels_\n",
    "\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "# # check unique clusters  \n",
    "set(clusters.labels_)\n",
    "# # -1 value represents noisy points could not assigned to any cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove no trap clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels = labels.copy()\n",
    "for k_clust in np.unique(labels):\n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(labels==k_clust)[0]])\n",
    "    # print(\"For cluster {}:\".format(k_clust))\n",
    "    plausible_trap = SIMS_dict_uniq[filter_idx][np.where(labels==k_clust)[0]][min_index][0]\n",
    "    if \"(\"*10 in plausible_trap:\n",
    "        real_labels = [-1 if x==k_clust else x for x in real_labels]\n",
    "        print(\"Cluster {} is NOT a trap\".format(k_clust))\n",
    "    else:\n",
    "        print(\"Cluster {} is a trap\".format(k_clust))\n",
    "        \n",
    "real_labels = np.array(real_labels)\n",
    "\n",
    "print(\"\\nClusters with trap are: {}\".format(np.unique(real_labels)))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual to find traps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "X = filter_comb_pca_coords[:,0]\n",
    "Y = filter_comb_pca_coords[:,1]\n",
    "\n",
    "# color = SIMS_G_uniq[filter_idx]\n",
    "color = labels\n",
    "size = SIMS_HT_uniq[filter_idx]\n",
    "text = SIMS_dict_uniq[:,0][filter_idx]\n",
    "prob = P_tot[filter_idx]\n",
    "\n",
    "color_palette = pc.qualitative.Plotly[:n_clusters_]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x = X,\n",
    "    y = Y,\n",
    "    mode = 'markers',\n",
    "    marker=dict(\n",
    "        color = color,\n",
    "        # size = P_tot,\n",
    "        size = size,\n",
    "        # colorscale=\"Plasma\",\n",
    "        colorscale=color_palette,\n",
    "        sizeref=3e-10,\n",
    "        showscale=True,\n",
    "    ),\n",
    "    \n",
    "    customdata = np.stack((SIMS_G_uniq[filter_idx],\n",
    "                           size,\n",
    "                           prob,\n",
    "                           ),axis=-1),\n",
    "\n",
    "    text = text,\n",
    "    hovertemplate=\n",
    "        \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "        \"DP notation: <br> <b>%{text}</b><br>\" +  \n",
    "        \"Energy:  %{customdata[0]:.3f} kcal/mol<br>\"+\n",
    "        \"Average holding time:  %{customdata[1]:.5g} s<br>\"+\n",
    "        \"Probability:  %{customdata[2]:.2g} <br>\",\n",
    "    name=\"states\",\n",
    "    \n",
    "))\n",
    "\n",
    "# label initial and final states\n",
    "fig.add_trace(\n",
    "    go.Scattergl(\n",
    "        x = comb_pca_coords[:,0][[initial_idx, final_idx]],\n",
    "        y = comb_pca_coords[:,1][[initial_idx, final_idx]],\n",
    "        mode='markers+text',\n",
    "        marker_color=\"lime\", \n",
    "        marker_size=15,\n",
    "        text=[\"I\", \"F\"],\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(\n",
    "        family=\"sans serif\",\n",
    "        size=10,\n",
    "        color=\"black\"\n",
    "    ),\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False,\n",
    "        \n",
    "                    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = filter_comb_pca_coords[:,0]\n",
    "Y = filter_comb_pca_coords[:,1]\n",
    "clusters = labels  # Cluster labels\n",
    "\n",
    "# Get unique cluster labels\n",
    "unique_clusters = np.unique(clusters)\n",
    "\n",
    "# Define colors for each cluster\n",
    "noise  = 'grey'\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'cyan', 'magenta', 'lime', 'teal']  # Add more colors as needed\n",
    "\n",
    "# Create a scatter trace for each cluster\n",
    "traces = []\n",
    "for cluster_label in unique_clusters:\n",
    "    mask = clusters == cluster_label\n",
    "    if cluster_label == -1:\n",
    "        # Assign a color for cluster label -1\n",
    "        color = noise\n",
    "        name = 'Cluster -1'\n",
    "        \n",
    "    else:\n",
    "        # Assign a color for other cluster labels\n",
    "        color = colors[cluster_label]\n",
    "        name = f'Cluster {cluster_label}'\n",
    "    \n",
    "    trace = go.Scattergl(\n",
    "        x=X[mask],\n",
    "        y=Y[mask],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "            size = SIMS_HT_uniq[filter_idx][mask],\n",
    "            sizeref=3e-10,\n",
    "            # sizeref=1e-10,\n",
    "            \n",
    "            sizemode='diameter',\n",
    "            ),\n",
    "        name=name,\n",
    "        showlegend=True,\n",
    "        \n",
    "        customdata = np.stack((SIMS_G_uniq[filter_idx][mask],\n",
    "                           SIMS_HT_uniq[filter_idx][mask],\n",
    "                           P_tot[filter_idx][mask],\n",
    "                           ),axis=-1),\n",
    "        text = SIMS_dict_uniq[:,0][filter_idx][mask],\n",
    "        hovertemplate=\n",
    "            \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "            \"DP notation: <br> <b>%{text}</b><br>\" +  \n",
    "            \"Energy:  %{customdata[0]:.3f} kcal/mol<br>\"+\n",
    "            \"Average holding time:  %{customdata[1]:.5g} s<br>\"+\n",
    "            \"Probability:  %{customdata[2]:.2g} <br>\",\n",
    "    )\n",
    "\n",
    "    traces.append(trace)\n",
    "\n",
    "# label initial and final states\n",
    "trace = go.Scattergl(\n",
    "        x = comb_pca_coords[:,0][[initial_idx, final_idx]],\n",
    "        y = comb_pca_coords[:,1][[initial_idx, final_idx]],\n",
    "        mode='markers+text',\n",
    "        \n",
    "        marker_color=\"lime\", \n",
    "        marker_size=10,\n",
    "        text=[\"I\", \"F\"],\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(\n",
    "        family=\"sans serif\",\n",
    "        size=10,\n",
    "        color=\"black\"\n",
    "    ),\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False,\n",
    "                    )\n",
    "\n",
    "traces.append(trace)\n",
    "\n",
    "# label kinetic traps\n",
    "for k_clust in np.unique(clusters):\n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(clusters==k_clust)[0]])\n",
    "    trace = go.Scattergl(\n",
    "        x = np.array(X[np.where(clusters==k_clust)[0]][min_index]),\n",
    "        y = np.array(Y[np.where(clusters==k_clust)[0]][min_index]),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=\"black\",\n",
    "            symbol='star',\n",
    "            size=50,\n",
    "        ),\n",
    "        showlegend=False,\n",
    "    )\n",
    "    traces.append(trace)\n",
    "    \n",
    "# legend setting\n",
    "layout = go.Layout(\n",
    "    legend=dict(\n",
    "        # x=0.5,  # Adjust the x position of the legend\n",
    "        # y=0.5,  # Adjust the y position of the legend\n",
    "        # font=dict(\n",
    "        #     size=10  # Adjust the font size of the legend\n",
    "        # ),\n",
    "        itemsizing='constant',\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        range = [min(X)*1.1,max(X)*1.1],\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range = [min(Y)*1.1,max(Y)*1.1],\n",
    "    ),\n",
    "    title=f\"DBSCAN finding Kinetic Traps for sample {SEQ}\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = filter_comb_pca_coords[:,0]\n",
    "Y = filter_comb_pca_coords[:,1]\n",
    "clusters = real_labels  # Cluster real labels\n",
    "\n",
    "# Get unique cluster labels\n",
    "unique_clusters = np.unique(clusters)\n",
    "\n",
    "# Define colors for each cluster\n",
    "noise  = 'grey'\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'cyan', 'magenta', 'lime', 'teal']  # Add more colors as needed\n",
    "name_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "trap_shape = ['star', 'x', 'triangle-up', 'cross', 'pentagon', 'diamond', 'square', 'triangle-down', 'triangle-left', 'triangle-right']\n",
    "\n",
    "# Create a scatter trace for each cluster\n",
    "traces = []\n",
    "i = 0\n",
    "for cluster_label in unique_clusters:\n",
    "    mask = clusters == cluster_label\n",
    "    if cluster_label == -1:\n",
    "        # Assign a color for cluster label -1\n",
    "        color = noise\n",
    "        # name = 'Cluster -1'\n",
    "        name = 'Noise'\n",
    "        \n",
    "    else:\n",
    "        # Assign a color for other cluster labels\n",
    "        color = colors[cluster_label]\n",
    "        name = f'Cluster {name_list[i]}'\n",
    "        i += 1\n",
    "    \n",
    "    trace = go.Scattergl(\n",
    "        x=X[mask],\n",
    "        y=Y[mask],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "            size = SIMS_HT_uniq[filter_idx][mask],\n",
    "            sizeref=3e-10,\n",
    "            # sizeref=1e-10,\n",
    "            sizemode='diameter',\n",
    "            ),\n",
    "        name=name,\n",
    "        showlegend=True,\n",
    "        \n",
    "        customdata = np.stack((SIMS_G_uniq[filter_idx][mask],\n",
    "                           SIMS_HT_uniq[filter_idx][mask],\n",
    "                           P_tot[filter_idx][mask],\n",
    "                           ),axis=-1),\n",
    "        text = SIMS_dict_uniq[:,0][filter_idx][mask],\n",
    "        hovertemplate=\n",
    "            \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "            \"DP notation: <br> <b>%{text}</b><br>\" +  \n",
    "            \"Energy:  %{customdata[0]:.3f} kcal/mol<br>\"+\n",
    "            \"Average holding time:  %{customdata[1]:.5g} s<br>\"+\n",
    "            \"Probability:  %{customdata[2]:.2g} <br>\",\n",
    "    )\n",
    "\n",
    "    traces.append(trace)\n",
    "\n",
    "# label initial and final states\n",
    "trace = go.Scattergl(\n",
    "        x = comb_pca_coords[:,0][[initial_idx, final_idx]],\n",
    "        y = comb_pca_coords[:,1][[initial_idx, final_idx]],\n",
    "        mode='markers+text',\n",
    "        \n",
    "        marker_color=\"lime\", \n",
    "        marker_size=10,\n",
    "        text=[\"I\", \"F\"],\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(\n",
    "        family=\"sans serif\",\n",
    "        size=10,\n",
    "        color=\"black\"\n",
    "    ),\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False,\n",
    "                    )\n",
    "\n",
    "traces.append(trace)\n",
    "\n",
    "# label kinetic traps\n",
    "i = 0\n",
    "for k_clust in np.unique(clusters):\n",
    "    if k_clust == -1:\n",
    "        continue\n",
    "    \n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(clusters==k_clust)[0]])\n",
    "    trace = go.Scattergl(\n",
    "        x = np.array(X[np.where(clusters==k_clust)[0]][min_index]),\n",
    "        y = np.array(Y[np.where(clusters==k_clust)[0]][min_index]),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=\"black\",\n",
    "            symbol=trap_shape[i],\n",
    "            size=10,\n",
    "        ),\n",
    "        name = f\"Trap {name_list[i]}\",\n",
    "        showlegend=True,\n",
    "    )\n",
    "    i += 1\n",
    "    traces.append(trace)\n",
    "\n",
    "# legend setting\n",
    "layout = go.Layout(\n",
    "    legend=dict(\n",
    "        # x=0.5,  # Adjust the x position of the legend\n",
    "        # y=0.5,  # Adjust the y position of the legend\n",
    "        # font=dict(\n",
    "        #     size=10  # Adjust the font size of the legend\n",
    "        # ),\n",
    "        itemsizing='constant',\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        range = [min(X)*1.1,max(X)*1.1],\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range = [min(Y)*1.1,max(Y)*1.1],\n",
    "    ),\n",
    "    title=f\"DBSCAN finding Kinetic Traps for sample {SEQ}\",\n",
    ")\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the kinetic trap in each cluster\n",
    "for k_clust in np.unique(real_labels):\n",
    "    if k_clust == -1:\n",
    "        continue\n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(labels==k_clust)[0]])\n",
    "    print(\"Kinetic trap in cluster {} is:\".format(k_clust))\n",
    "    print(SIMS_dict_uniq[filter_idx][np.where(labels==k_clust)[0]][min_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the plausible kinetic trap in each cluster\n",
    "for k_clust in np.unique(labels):\n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(labels==k_clust)[0]])\n",
    "    print(\"Kinetic trap in cluster {} is:\".format(k_clust))\n",
    "    print(SIMS_dict_uniq[filter_idx][np.where(labels==k_clust)[0]][min_index])\n",
    "    # print(\"Position in the cluster is:\", X[np.where(labels==k_clust)[0]][min_index], Y[np.where(labels==k_clust)[0]][min_index], \"\\n\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exact each trajectory\n",
    "split_id = trj_id + 1 # index for split to each trajectory\n",
    "traj_in_clust = np.zeros(len(np.unique(labels)), dtype=int)\n",
    "avg_time_in_clust = np.zeros(len(np.unique(labels)), dtype=float)\n",
    "\n",
    "for i in range(len(split_id)):\n",
    "    if i == 0:\n",
    "        trj_dp = SIMS_dict[0:split_id[i],0]\n",
    "    else:\n",
    "        trj_dp = SIMS_dict[split_id[i-1]:split_id[i],0]\n",
    "\n",
    "    for j, k_clust in enumerate(np.unique(labels)):\n",
    "        mask = labels == k_clust\n",
    "        if np.size(np.intersect1d(trj_dp, SIMS_dict_uniq[:,0][filter_idx][mask])) != 0:\n",
    "            traj_in_clust[j] += 1\n",
    "            avg_time_in_clust[j] += SIMS_T[trj_id[i]]\n",
    "\n",
    "print(f\"{SEQ}:\")\n",
    "for i in range(len(traj_in_clust)):\n",
    "    print(\"{} trajs in cluster {}. Average time: {:.3e}.\".format(traj_in_clust[i], np.unique(labels)[i], avg_time_in_clust[i]/traj_in_clust[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptime\n",
    "from deeptime.decomposition import TICA\n",
    "\n",
    "tica = TICA(lagtime=1,dim=2)\n",
    "data = combined_data\n",
    "tica_coor = tica.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combined_data\n",
    "tica_coor = tica.fit_transform(data)\n",
    "tica_coor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt plot for tica\n",
    "plt.scatter(tica_coor[:,0],tica_coor[:,1],c=SIMS_G_uniq,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vida')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e442af4fad2330d8f4febe7e8e7250535e161341429a4f0b93cbf21b824330cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
