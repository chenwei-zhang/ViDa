{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from argparse import Namespace\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import phate\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load saved trajectories data for npz file\n",
    "\"\"\"\n",
    "SEQ = \"PT4\"\n",
    "# SEQ = \"PT4_hairpin\"\n",
    "\n",
    "# multiple trajectories\n",
    "if SEQ in [\"PT3\", \"PT4\", \"PT3_hairpin\"]:\n",
    "     fnpz_data = \"data/vida_data/helix_assoc/helix_assoc_{}_multrj_100epoch_py.npz\".format(SEQ)\n",
    "elif SEQ in [\"PT0\", \"PT4_hairpin\"]:\n",
    "     fnpz_data = \"data/vida_data/helix_assoc/helix_assoc_{}_multrj_60epoch_py.npz\".format(SEQ)\n",
    "\n",
    "data_npz = np.load(fnpz_data)\n",
    "\n",
    "# asssign data to variables\n",
    "for var in data_npz.files:\n",
    "     locals()[var] = data_npz[var]\n",
    "\n",
    "# recover full data based on coord_id, indices, and unique data\n",
    "SIMS_adj = SIMS_adj_uniq[coord_id_S]\n",
    "SIMS_scar = SIMS_scar_uniq[coord_id_S]\n",
    "SIMS_G = SIMS_G_uniq[coord_id_S]\n",
    "SIMS_pair = SIMS_pair_uniq[coord_id_S]\n",
    "\n",
    "print(SIMS_T.shape,SIMS_HT.shape,SIMS_HT_uniq.shape)\n",
    "print(SIMS_adj.shape,SIMS_scar.shape,SIMS_G.shape,SIMS_HT.shape,SIMS_pair.shape)\n",
    "print(SIMS_adj_uniq.shape,SIMS_scar_uniq.shape,SIMS_G_uniq.shape,SIMS_pair_uniq.shape) \n",
    "print(SIMS_dict.shape,SIMS_dict_uniq.shape)\n",
    "print(coord_id_S.shape,indices_S.shape,trj_id.shape,data_embed.shape,occ_density_S.shape)\n",
    "print(pca_coords.shape,pca_all_coords.shape)\n",
    "print(phate_coords.shape,phate_all_coords.shape)\n",
    "print(umap_coord_2d.shape,umap_all_coord_2d.shape,umap_coord_3d.shape,umap_all_coord_3d.shape)\n",
    "print(tsne_coord_2d.shape,tsne_all_coord_2d.shape,tsne_coord_3d.shape,tsne_all_coord_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(SIMS_HT == 0, return_counts=True)\n",
    "print(unique, counts)\n",
    "\n",
    "print(SIMS_HT.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect graph information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# calculate the expected holding time for each node\n",
    "all_graph_info = dict()\n",
    "\n",
    "for i in range(SIMS_dict_uniq.shape[0]):\n",
    "    idx = np.where(SIMS_dict[:,-1] == str(i))\n",
    "    expected_time = SIMS_HT[idx].mean()\n",
    "    all_graph_info[i] = expected_time, SIMS_dict_uniq[i][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('all_graph_info.json', 'w') as f:\n",
    "#     json.dump(all_graph_info, f)\n",
    "\n",
    "\n",
    "# # load json file\n",
    "all_graph_info = json.load(open('all_graph_info.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_graph_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct weights (expected holding time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the cutoff based on the min or max value of the SIMS_HT\n",
    "avg_time = np.array(list(all_graph_info.values()),dtype=object)[:,0]\n",
    "print(avg_time.max(), avg_time[avg_time!=0].min(), avg_time.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = np.array(SIMS_dict[:,-1], dtype=int)\n",
    "print(all_nodes.shape, all_nodes, all_nodes.max())\n",
    "print(\"Initial node:\", all_nodes[0], \"  Final node:\", all_nodes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise nodes\n",
    "# note: this step connect final->initial node (291->0),\n",
    "# which will be remove later\n",
    "all_edges_temp = []\n",
    "for previous, current in zip(all_nodes, all_nodes[1:]):\n",
    "    all_edges_temp.append((previous, current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_edges_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all edges that connect final->initial node (291->0)\n",
    "all_edges = list(filter((291,0).__ne__, all_edges_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "if (291,0) in all_edges:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"There are no edges from node 291 to node 0\")\n",
    "    \n",
    "len(all_edges), len(all_edges_temp), len(all_edges_temp) - len(all_edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct modified undirected weight graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUG = nx.Graph()\n",
    "\n",
    "for  i in range(len(all_edges)):\n",
    "    idx0 = all_edges[i][0]\n",
    "    idx1 = all_edges[i][1]\n",
    "    \n",
    "    if avg_time[idx0] < avg_time[idx1]:\n",
    "         weight = avg_time[idx0]\n",
    "    else:\n",
    "        weight = avg_time[idx1]\n",
    "        \n",
    "    if avg_time[idx0] == 0 or avg_time[idx1] == 0:\n",
    "        weight = avg_time[idx0] + avg_time[idx1]\n",
    "        \n",
    "    MUG.add_edge(int(all_edges[i][0]), int(all_edges[i][1]), weight = float(weight))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shortest path length which should not be 0\n",
    "UDD = []\n",
    "for i, w in enumerate(np.asarray((list(MUG.edges.data())))[:,2]):\n",
    "    UDD.append(w['weight'])\n",
    "    if w['weight'] == 0:\n",
    "        print(np.asarray(list(MUG.edges.data()))[i])\n",
    "        \n",
    "UDD = np.asarray(UDD)\n",
    "print(len(UDD), UDD.max(), UDD.min())\n",
    "\n",
    "m = 35\n",
    "n = 300\n",
    "# m = 289\n",
    "# n = 291\n",
    "print('\\n',nx.dijkstra_path_length(MUG, m, n),  nx.dijkstra_path_length(MUG, n, m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges[2][0], all_edges[2][1], avg_time[all_edges[2][0]], avg_time[all_edges[2][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUG.get_edge_data(0,1), MUG.get_edge_data(1,0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect and save all pair shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# collect and save all shortest paths for each node in individual npz file\n",
    "\n",
    "for i in range(len(MUG.nodes())):\n",
    "    path_list = []\n",
    "\n",
    "    path_list.append(nx.single_source_dijkstra_path_length(MUG, i))\n",
    "    \n",
    "    np.save(f'./data/shortest_path/path_{i}.npy', path_list)\n",
    "\n",
    "print(\"Finish saving all shortest paths for each node in individual npz file\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutoff (currently not use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the cutoff based on the min or max value of the SIMS_HT\n",
    "alpha = 0.1\n",
    "cutoff = alpha * avg_time.max()\n",
    "cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the shortest path for each node\n",
    "shortestpath_dict = dict()\n",
    "\n",
    "for i in range(len(avg_time)):\n",
    "    length = nx.single_source_dijkstra_path_length(DG, i)\n",
    "    length_arr = np.array(list(length.items()), dtype=object)\n",
    "    \n",
    "    # find the shortest path that is less than cutoff\n",
    "    # and get the index of the node\n",
    "    pos = np.where(length_arr[:,1] < cutoff)\n",
    "    Xj = length_arr[pos][:,0]\n",
    "    dij = length_arr[pos][:,1]\n",
    "    shortestpath_dict[i] = Xj, dij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shortestpath_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save npy file for shortestpath_dict\n",
    "# np.save('shortestpath_dict.npy', shortestpath_dict)\n",
    "\n",
    "## load npy file for shortestpath_dict\n",
    "shortestpath_dict = np.load('shortestpath_dict.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortestpath_dict[0][0], \n",
    "shortestpath_dict[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all shortest paths for each node\n",
    "allpath_dict = dict()\n",
    "\n",
    "for i in range(len(avg_time)):\n",
    "    length = dict(nx.single_source_dijkstra_path_length(DG, i))\n",
    "    length_arr = np.array(list(length.items()), dtype=object)\n",
    "    \n",
    "    # all shortest paths and index of the node\n",
    "    Xj = length_arr[:,0]\n",
    "    dij = length_arr[:,1]\n",
    "    allpath_dict[i] = Xj, dij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save npy file for allpath_dict\n",
    "# np.save('allpath_dict.npy', allpath_dict)\n",
    "\n",
    "## load npy file for shortestpath_dict\n",
    "allpath_dict = np.load('allpath_dict.npy', allow_pickle='TRUE').item() ## extremely large memory usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case analysis (currently not use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note: if directed graph: 35->36 is way larger than 36->35\n",
    "'''\n",
    "print(avg_time[35])\n",
    "print(avg_time[36])\n",
    "SIMS_dict_uniq[36], SIMS_dict_uniq[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time.shape, SIMS_dict_uniq.shape,len(all_graph_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "## no longer needed\n",
    "\n",
    "# define a function to find the x_j for a given x_i\n",
    "def findXjFromXi(xi_id, input, shortestpath_dict):\n",
    "    '''\n",
    "    xi: the index of the node\n",
    "    path_dict: the dictionary of the shortest path\n",
    "    '''\n",
    "    xj_id = shortestpath_dict[xi_id][0].astype(int)[1:] # avoid the node itself\n",
    "    xj = input[xj_id]\n",
    "    dij = shortestpath_dict[xi_id][1].astype(float)[1:] # avoid the node itself\n",
    "    \n",
    "    return xj_id, xj, dij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "## no longer needed\n",
    "\n",
    "def findpath2Xj(MDG, xi_id, xj_id):\n",
    "    '''\n",
    "    MDG: modified directed weighted graph\n",
    "    i: node index i\n",
    "    j: node index j\n",
    "    '''    \n",
    "    d = None; dij = None; dij = None\n",
    "    try:\n",
    "        dij = nx.dijkstra_path_length(MDG, xi_id, xj_id)\n",
    "    except:\n",
    "        dij = nx.dijkstra_path_length(MDG, xj_id, xi_id)\n",
    "\n",
    "    try:\n",
    "        dji = nx.dijkstra_path_length(MDG, xj_id, xi_id)\n",
    "    except:\n",
    "        dji = nx.dijkstra_path_length(MDG, xi_id, xj_id)\n",
    "\n",
    "    finally:\n",
    "        d = min(dij,dji) # return the shortest path\n",
    "        return d\n",
    "    \n",
    "m = 2494\n",
    "n = 9898\n",
    "findpath2Xj(MDG, m, n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the importance weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# alreay run and saved in npy file\n",
    "\n",
    "# calculate the probability of being visited during a simulated trajectory \n",
    "# from the initial state\n",
    "\n",
    "split_id = trj_id + 1 # index for split to each trajectory\n",
    "P_tot = np.zeros(len(SIMS_dict_uniq))\n",
    " \n",
    "for i in range(len(SIMS_dict_uniq)):\n",
    "    for j in range(len(split_id)):\n",
    "        if j == 0:\n",
    "            trj = SIMS_dict[0:split_id[j],4].astype(int)\n",
    "        else:\n",
    "            trj = SIMS_dict[split_id[j-1]:split_id[j],4].astype(int)\n",
    "            \n",
    "        if i in trj:\n",
    "            P_tot[i] += 1\n",
    "P_tot = P_tot / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"path_weight.npz\", \"wb\") as f:\n",
    "#     np.savez(f, \n",
    "#             shortestpath_dict=shortestpath_dict,\n",
    "#             P_tot=P_tot)\n",
    "\n",
    "P_tot = np.load(\"path_weight.npz\", allow_pickle='TRUE')[\"P_tot\"]\n",
    "P_tot.shape, P_tot.max(), P_tot.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# might not be needed\n",
    "\n",
    "# calculate the important weight for each edge (node i to j)\n",
    "def findWij(xi_id, xj_id, P_tot):\n",
    "    '''\n",
    "    xi_id: the index of the x_i, one number\n",
    "    xj_id: the index of the x_j, one number\n",
    "    P_tot: the probability of all nodes\n",
    "    '''\n",
    "    return P_tot[xi_id] * P_tot[xj_id]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViDa Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tup = (torch.Tensor(SIMS_scar_uniq),\n",
    "            torch.Tensor(SIMS_G_uniq),\n",
    "            torch.Tensor(np.arange(len(SIMS_scar_uniq))))\n",
    "train_dataset = torch.utils.data.TensorDataset(*train_tup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters\n",
    "input_dim = train_tup[0].shape[-1]\n",
    "\n",
    "config = Namespace(\n",
    "    device = 'mps', # change to cuda if using GPU\n",
    "    batch_size = 256,\n",
    "    input_dim = input_dim,\n",
    "    output_dim = input_dim,\n",
    "    latent_dim = 25, # bottleneck dimension\n",
    "    hidden_dim = 400,\n",
    "    n_epochs = 100, # 60 for PT4_hairpin, PT0, 100 for others\n",
    "    learning_rate = 0.0001, # learning rate\n",
    "    log_interval = 10, # how many batches to wait before logging training status    \n",
    "    \n",
    "    # hyperparameters for loss function\n",
    "    alpha = 0.5,\n",
    "    beta = 1e-2,\n",
    "    gamma = 0.5,\n",
    "    delta = 5e-16,\n",
    "    \n",
    "    # alpha = 1.0,\n",
    "    # beta = 1.0,\n",
    "    # gamma = 1.0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make datalaoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader.dataset), len(train_loader), train_loader.batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        '''\n",
    "        Args:\n",
    "        ----\n",
    "            - input_dim: the dimension of the input node feature\n",
    "            - hiddent_dim: the dimension of the hidden layer\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        \n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution, note how we only output\n",
    "        # diagonal values of covariance matrix. Here we assume\n",
    "        # they are conditionally independent\n",
    "        self.hid2mu = nn.Linear(400, self.latent_dim)\n",
    "        self.hid2logvar = nn.Linear(400, self.latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.fc1(x)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        mu = self.hid2mu(x)\n",
    "        logvar = self.hid2logvar(x)\n",
    "        return mu, logvar\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        '''\n",
    "        Args:\n",
    "        ----\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "            - hiddent_dim: the dimension of the hidden layer\n",
    "            - output_dim: the dimension of the output node feature\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.latent_dim, self.hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        self.fc3 = nn.Linear(400, self.output_dim)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.bn1(F.relu(self.fc1(z)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        # x = torch.sigmoid(self.fc3(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        '''\n",
    "        The regressor is used to predict the energy of the node\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "        '''\n",
    "        super(Regressor, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.regfc1 = nn.Linear(self.latent_dim, 15)\n",
    "        self.regfc2 = nn.Linear(15, 1)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        y = F.relu(self.regfc1(z))\n",
    "        y = self.regfc2(y)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIDA(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, regressor):\n",
    "        '''\n",
    "        Args:\n",
    "        ----\n",
    "            - input_dim: the dimension of the input node feature\n",
    "            - hiddent_dim: the dimension of the hidden layer\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "            - output_dim: the dimension of the output node feature (same as input_dim)\n",
    "        '''\n",
    "        super(VIDA, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.regressor = regressor\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps*std\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        y_pred = self.regressor(z)\n",
    "        return x_recon, y_pred, z, mu, logvar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_recon, x, mu, logvar):\n",
    "    '''\n",
    "    Compute the VAE loss\n",
    "    \n",
    "    Args:\n",
    "        - x_recon: the reconstructed node feature\n",
    "        - x: the original node feature\n",
    "        - mu: the mean of the latent space\n",
    "        - logvar: the log variance of the latent space\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing (scalar) the loss for the VAE\n",
    "    '''\n",
    "    BCE = F.mse_loss(x_recon.flatten(), x.flatten())\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    # return BCE + KLD\n",
    "    return BCE, KLD\n",
    "\n",
    "\n",
    "def pred_loss(y_pred, y):\n",
    "    '''\n",
    "    Compute the energy prediction loss\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        - y_pred: the predicted energy of the node\n",
    "        - y: the true energy of the node\n",
    "    \n",
    "    Returns:\n",
    "        - loss: PyTorch Tensor containing (scalar) the loss for the prediction\n",
    "    '''\n",
    "    return F.mse_loss(y_pred.flatten(), y.flatten())\n",
    "\n",
    "\n",
    "def distance_loss(zi, zj, dij, wij):\n",
    "    '''\n",
    "    Compute the distance loss between embeddings \n",
    "    and the minimum expected holding time\n",
    "    \n",
    "    Args:\n",
    "        - zi: the embedding of the node i\n",
    "        - zj: the embedding of the node j\n",
    "        - dij: the minimum expected holding time (distance) between i and j\n",
    "        - wij: the important weight of the edge (i,j) --> float\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing (scalar) the loss for the embedding distance\n",
    "    '''\n",
    "    pdist = torch.nn.PairwiseDistance(p=2)\n",
    "    embed_dis = pdist(zi, zj)\n",
    "    L = wij * (embed_dis - dij)**2 * 1/(dij**2)\n",
    "    return L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, train_loader,\n",
    "          optimizer, vae_loss, pred_loss, distance_loss,\n",
    "          ):\n",
    "    '''\n",
    "    Train VIDA!\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        - config: Experiment configurations\n",
    "        - model: Pytorch VIDA model\n",
    "        - train_loader: Pytorch DataLoader for training set\n",
    "        - optimizer: Pytorch optimizer\n",
    "        - vae_loss: the VAE loss function\n",
    "        - pred_loss: the energy prediction loss function\n",
    "        - distance_loss: the distance loss function\n",
    "    '''\n",
    "    \n",
    "    model.to(config.device)\n",
    "    model.train()\n",
    "    \n",
    "    log_dir = f'./model_config/{time.strftime(\"%m%d-%H%M\")}'\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    print('\\n ------- Start Training -------')\n",
    "    # for epoch in range(config.n_epochs):\n",
    "    for epoch in range(1):\n",
    "        \n",
    "        training_loss = []\n",
    "        \n",
    "        for batch_idx, (x, y, idx) in enumerate(train_loader):  # mini batch\n",
    "            \n",
    "            # Configure input\n",
    "            x = x.to(config.device)\n",
    "            y = y.to(config.device)\n",
    "            idx = idx.to(config.device)\n",
    "            \n",
    "            # ------------------------------------------\n",
    "            #  Train VIDA\n",
    "            # ------------------------------------------\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get the reconstructed nodes, predicted energy, and the embeddings\n",
    "            x_recon, y_pred, z, mu, logvar = model(x)\n",
    "            \n",
    "            # compute the distance loss (kinetic loss)\n",
    "            dist_loss = 0.0\n",
    "                        \n",
    "            for i in range(0,len(x)-2):\n",
    "                xi_id = int(idx[i].item())       # node i idex\n",
    "                xj_id = int(idx[i+1].item())     # node j idex\n",
    "                zi = z[i]                   # node i embedding in the batch\n",
    "                zj = z[i+1]                 # node j embedding in the batch\n",
    "                \n",
    "                # dij = nx.dijkstra_path_length(MUG, xi_id, xj_id) # embedding distance between i and j\n",
    "                dij = np.load(f'./data/shortest_path/path_{xi_id}.npy',allow_pickle=True)[0][xj_id]\n",
    "                \n",
    "                wij = P_tot[xi_id] * P_tot[xj_id] # importance weight of nodes i and j\n",
    "                \n",
    "                dist_loss += distance_loss(zi, zj, dij, wij) # loss between i and j\n",
    "\n",
    "            # compute the total loss\n",
    "            recon_loss, kl_loss = vae_loss(x_recon, x, mu, logvar)\n",
    "            p_loss = pred_loss(y_pred, y)\n",
    "                        \n",
    "            # scaling the loss\n",
    "            recon_loss = config.alpha * recon_loss\n",
    "            kl_loss = config.beta * kl_loss\n",
    "            p_loss = config.gamma * p_loss\n",
    "            dist_loss = config.delta * dist_loss\n",
    "            \n",
    "            loss = recon_loss + kl_loss + p_loss + dist_loss # total loss\n",
    "                        \n",
    "            training_loss.append(loss.item())\n",
    "            \n",
    "            # backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # ------------------------------------------\n",
    "            # Log Progress\n",
    "            # ------------------------------------------\n",
    "            if batch_idx % config.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item()))\n",
    "                \n",
    "                writer.add_scalar('training loss',\n",
    "                                  loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('recon loss',\n",
    "                                  recon_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('kl loss',\n",
    "                                  kl_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('pred loss',\n",
    "                                  p_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('dist loss',\n",
    "                                  dist_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                    \n",
    "        print ('====> Epoch: {} Average loss: {:.4f}'.format(epoch, np.mean(training_loss)))\n",
    "        writer.add_scalar('epoch training loss', np.mean(training_loss), epoch)\n",
    "    \n",
    "    writer.close()  \n",
    "    print('\\n ------- Finished Training -------')\n",
    "      \n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), f'{log_dir}/model.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "encoder = Encoder(input_dim=config.input_dim, hidden_dim=config.hidden_dim, latent_dim=config.latent_dim)\n",
    "decoder = Decoder(latent_dim=config.latent_dim, hidden_dim=config.hidden_dim, output_dim=config.output_dim)\n",
    "regressor = Regressor(latent_dim=config.latent_dim)\n",
    "\n",
    "vida = VIDA(encoder, decoder, regressor)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(vida.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train VIDA\n",
    "train(config, vida, train_loader, optimizer, vae_loss, pred_loss, distance_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir model_config/ --host localhost --port 8000\n",
    "#  http://localhost:8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VIDA(encoder, decoder, regressor)\n",
    "model.load_state_dict(torch.load('./model_config/0218-1818/model.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do inference\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "        _, _, z, _, _ = model(torch.tensor(SIMS_scar_uniq).to(config.device))\n",
    "        # _, _, z, _, _ = model(torch.tensor(train_loader.dataset.tensors[0]).to(config.device))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed = z.to('cpu').numpy()\n",
    "data_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do PCA for GSAE embeded data\n",
    "pca_coords = PCA(n_components=3).fit_transform(data_embed)\n",
    "\n",
    "# # get all pca embedded states coordinates\n",
    "pca_all_coords = pca_coords[coord_id_S]  # multiple trj\n",
    "\n",
    "pca_coords.shape, pca_all_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(pca_coords,axis=0)).shape, (np.unique(pca_all_coords,axis=0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PHATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_embed = scaler.fit_transform(data_embed)\n",
    "data_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do PHATE for GSAE embeded data\n",
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "phate_coords = phate_operator.fit_transform(data_embed)\n",
    "\n",
    "# # get all phate embedded states coordinates\n",
    "phate_all_coords = phate_coords[coord_id_S]\n",
    "\n",
    "phate_coords.shape, phate_all_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(phate_coords,axis=0)).shape, (np.unique(phate_all_coords,axis=0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP set\n",
    "umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "# umap_3d = UMAP(n_components=3, init='random', random_state=0)\n",
    "\n",
    "# UMAP 2D fit tranform\n",
    "umap_coord_2d = umap_2d.fit_transform(data_embed)\n",
    "umap_all_coord_2d = umap_coord_2d[coord_id_S]  \n",
    "\n",
    "# UMAP 3D fit tranform\n",
    "# umap_coord_3d = umap_3d.fit_transform(data_embed)\n",
    "# umap_all_coord_3d = umap_coord_3d[coord_id_S]\n",
    "\n",
    "print((np.unique(umap_coord_2d,axis=0)).shape, (np.unique(umap_coord_3d,axis=0)).shape)\n",
    "print(umap_all_coord_2d.shape, (np.unique(umap_all_coord_2d,axis=0)).shape)\n",
    "# print(umap_all_coord_3d.shape, (np.unique(umap_all_coord_3d,axis=0)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne set\n",
    "tsne_2d = TSNE(n_components=2, perplexity=1000.0, random_state=0)\n",
    "# tsne_3d = TSNE(n_components=3, random_state=0)\n",
    "\n",
    "# tsne 2D fit tranform\n",
    "tsne_coord_2d = tsne_2d.fit_transform(data_embed)\n",
    "tsne_all_coord_2d = tsne_coord_2d[coord_id_S] \n",
    "\n",
    "# # tsne 3D fit tranform\n",
    "# tsne_coord_3d = tsne_3d.fit_transform(data_embed)\n",
    "# tsne_all_coord_3d = tsne_coord_3d[coord_id_S] \n",
    "\n",
    " \n",
    "print((np.unique(tsne_coord_2d,axis=0)).shape, (np.unique(tsne_coord_3d,axis=0)).shape)\n",
    "print(tsne_all_coord_2d.shape, (np.unique(tsne_all_coord_2d,axis=0)).shape)\n",
    "# print(tsne_all_coord_3d.shape, (np.unique(tsne_all_coord_3d,axis=0)).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "X = pca_all_coords[:,0]\n",
    "Y = pca_all_coords[:,1]\n",
    "Z = pca_all_coords[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G,\n",
    "          cmap='plasma',\n",
    "          s=20\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]*0.95),fontsize=15,c=\"yellow\", horizontalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "X = pca_coords[:,0]\n",
    "Y = pca_coords[:,1]\n",
    "Z = pca_coords[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pca_coords[:,0]\n",
    "Y = pca_coords[:,1]\n",
    "Z = pca_coords[:,2]\n",
    "\n",
    "# PCA: 3 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "im = ax.scatter3D(X,Y,Z,\n",
    "          c=SIMS_G_uniq,      \n",
    "          cmap='plasma')\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "z = [Z[0], Z[-1]]\n",
    "ax.scatter(x,y,z,s=100,c=\"green\",alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pca_coords[:,0]\n",
    "Y = pca_coords[:,1]\n",
    "Z = pca_coords[:,2]\n",
    "\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y,\n",
    "          c=SIMS_pair_uniq,\n",
    "          cmap='plasma',\n",
    "          s=15\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try use PCA directly without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_coords1 = PCA(n_components=3).fit_transform(SIMS_scar_uniq)   # multiple trj\n",
    "\n",
    "X = pca_coords1[:,0]\n",
    "Y = pca_coords1[:,1]\n",
    "Z = pca_coords1[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = PCA(n_components=25)\n",
    "cm.fit(data_embed)\n",
    "\n",
    "PC_values = np.arange(cm.n_components_) + 1\n",
    "plt.plot(PC_values, np.cumsum(cm.explained_variance_ratio_), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot: PCA')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "# plt.xticks(np.arange(0, data_embed.shape[-1]+1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(cm.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PHATE Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phate = phate_all_coords[:,0]\n",
    "Y_phate = phate_all_coords[:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X_phate,Y_phate,\n",
    "                c=SIMS_G,   # multiple trj               \n",
    "                cmap='plasma',\n",
    "               )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X_phate[0],X_phate[-1]]\n",
    "y = [Y_phate[0],Y_phate[-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=30,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phate = phate_coords[:,0]\n",
    "Y_phate = phate_coords[:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X_phate,Y_phate,\n",
    "                c=SIMS_G_uniq,            \n",
    "                cmap='plasma',\n",
    "               )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X_phate[0],X_phate[-1]]\n",
    "y = [Y_phate[0],Y_phate[-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=30,c=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PHATE without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "phate1 = phate_operator.fit_transform(SIMS_scar_uniq)   # multiple trj\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(phate1[:,0],\n",
    "          phate1[:,1],\n",
    "          c=SIMS_G_uniq, \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [phate1[:,0][0],phate1[:,0][-1]]\n",
    "y = [phate1[:,1][0],phate1[:,1][-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. UMAP Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = umap_coord_2d[:,0]\n",
    "Y = umap_coord_2d[:,1]\n",
    "cmap = plt.cm.plasma\n",
    "cmap_r = plt.cm.get_cmap('plasma_r')\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c = SIMS_G_uniq,\n",
    "          cmap=cmap,\n",
    "          s=10\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly UMAP 2D\n",
    "umap_coord_2dscar = umap_2d.fit_transform(SIMS_scar_uniq)\n",
    "\n",
    "fig_2d = px.scatter(\n",
    "    umap_coord_2dscar, x=0, y=1,color=SIMS_G_uniq\n",
    ")\n",
    "fig_2d.update_traces(marker_size=3)\n",
    "fig_2d.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2d = px.scatter(\n",
    "    umap_coord_2d, x=0, y=1,color=SIMS_G_uniq\n",
    ")\n",
    "fig_2d.update_traces(marker_size=3)\n",
    "\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    umap_coord_3d, x=0, y=1, z=2,color=SIMS_G_uniq\n",
    ")\n",
    "\n",
    "fig_3d.update_traces(marker_size=2)\n",
    "\n",
    "fig_2d.show()\n",
    "fig_3d.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. t-SNE Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2d = px.scatter(\n",
    "    tsne_coord_2d, x=0, y=1,color=SIMS_G_uniq,\n",
    "    hover_data = {\"SIMS_G_uniq\":SIMS_G_uniq, \n",
    "                  \"SIMS_HT_uniq\":SIMS_HT_uniq,\n",
    "                  }\n",
    ")\n",
    "fig_2d.update_traces(marker_size=3)\n",
    "fig_2d.show()\n",
    "\n",
    "\n",
    "# fig_3d = px.scatter_3d(\n",
    "#     tsne_coord_3d, x=0, y=1, z=2,color=SIMS_G_uniq\n",
    "# )\n",
    "# fig_3d.update_traces(marker_size=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vida')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e442af4fad2330d8f4febe7e8e7250535e161341429a4f0b93cbf21b824330cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
