{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import h5py\n",
    "from misc import *\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import phate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import/generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load multiple simulated trajectory from Mulistrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multiple trajectories from multiple files\n",
    "folder_name = \"data/helix_assoc_PT3_new/assoc_PT3_1sim_20C\"\n",
    "# folder_name = \"data/helix_assoc_PT0_new/assoc_PT0_1sim_20C\"\n",
    "\n",
    "# define absorbing (final) state structure\n",
    "FINAL_STRUCTURE = \"(((((((((((((((((((((((((+)))))))))))))))))))))))))\"\n",
    "num_files = 100\n",
    "\n",
    "SIMS,SIMS_retrieve,SIMS_concat = load_multitrj(folder_name,FINAL_STRUCTURE,num_files)\n",
    "\n",
    "print(\"SIMS: \", len(SIMS))\n",
    "print(\"SIMS_retrieve: \", SIMS_retrieve.shape)\n",
    "print(\"SIMS_concat: \", len(SIMS_concat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load single simulated trajectory from Mulistrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load text file\n",
    "# # f = open('./data/helix_assos/assos_PT3_1sim_20C_21.txt', 'r') # PT3 \n",
    "# # STRAND_NAME = \"assos_PT3_1sim_20C_21\"\n",
    "\n",
    "# f = open('./data/helix_assos/assos_PT0_1sim_20C_51.txt', 'r') # PT0\n",
    "# STRAND_NAME = \"assos_PT0_1sim_20C_51\"\n",
    "\n",
    "# \"\"\" Dimenstions of SIM list \n",
    "# SIM: [[sim1], [sim2], ...]\n",
    "# sim: [[state1], [state2], ...]\n",
    "# state: [structure, time, energy]\n",
    "# \"\"\"\n",
    "# # define absorbing (final) state structure\n",
    "# FINAL_STRUCTURE = \"(((((((((((((((((((((((((+)))))))))))))))))))))))))\"\n",
    "\n",
    "# SIM = loadtrj(f,FINAL_STRUCTURE,type=\"Multiple\")\n",
    "# SIM_retrieve = np.array(SIM)\n",
    "# SIM_concat = concat_helix_structures(SIM) \n",
    "\n",
    "# print(\"SIM: \", len(SIM))\n",
    "# print(\"SIM_retrieve: \", SIM_retrieve.shape)\n",
    "# print(\"SIM_concat: \", len(SIM_concat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert dot-paren to adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dimenstions of SIM_adj list \n",
    "SIM_adj: N*m*m\n",
    "    N: number of states in the trajectory\n",
    "    m: number of nucleotides in the state (strand)\n",
    "\"\"\"\n",
    "# # get single trajectory's data\n",
    "# # get adjacency matrix, energy, and holding time for each state\n",
    "# SIM_adj,SIM_G,SIM_T,SIM_HT = sim_adj(SIM_concat)\n",
    "# print(SIM_adj.shape,SIM_G.shape,SIM_T.shape,SIM_HT.shape)\n",
    "\n",
    "\n",
    "# get multiple trajectories' data\n",
    "SIMS_adj, SIMS_G, SIMS_T, SIMS_HT = sim_adj(SIMS_concat)\n",
    "print(SIMS_adj.shape,SIMS_G.shape,SIMS_T.shape,SIMS_HT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique states adjacency matrix with their occupancy density, get unique energy, and time;\n",
    "# and their corresponding indices\n",
    "\n",
    "# # single trajectory\n",
    "# indices,occ_density,SIM_adj_uniq,SIM_G_uniq,SIM_T_uniq,SIM_HT_uniq \\\n",
    "#      = get_unique(SIM_concat,SIM_adj,SIM_G,SIM_T,SIM_HT)\n",
    "# print(indices.shape, occ_density.shape, SIM_adj_uniq.shape,SIM_G_uniq.shape,SIM_T_uniq.shape,SIM_HT_uniq.shape)\n",
    "\n",
    "\n",
    "# multiple trajectories\n",
    "indices_S,occ_density_S,SIMS_adj_uniq,SIMS_G_uniq,SIMS_T_uniq,SIMS_HT_uniq \\\n",
    "     = get_unique(SIMS_concat,SIMS_adj,SIMS_G,SIMS_T,SIMS_HT)\n",
    "print(indices_S.shape, occ_density_S.shape, SIMS_adj_uniq.shape,SIMS_G_uniq.shape,SIMS_T_uniq.shape,SIMS_HT_uniq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_adj_uniq.shape, (np.unique(SIMS_adj_uniq,axis=0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get labeled trajectory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # single trajectory\n",
    "# # get trajectory data with its corresponding labels \n",
    "# SIM_dict = label_structures(SIM_concat,indices) \n",
    "# coord_id = SIM_dict[:,3].astype(int)\n",
    "# print(SIM_dict.shape, coord_id.shape)\n",
    "# # find the structure having the largest occupancy density\n",
    "# print(SIM_retrieve[indices[occ_density.argmax()]])\n",
    "\n",
    "\n",
    "# multiple trajectories\n",
    "SIMS_dict = label_structures(SIMS_concat,indices_S)\n",
    "coord_id_S = SIMS_dict[:,3].astype(int)\n",
    "print(SIMS_dict.shape, coord_id_S.shape)\n",
    "# find the structure having the largest occupancy density\n",
    "print(SIMS_retrieve[indices_S[occ_density_S.argmax()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save SIMS_dict\n",
    "# fname_dict = \"data/helix_assoc_new/helix_assoc_new_PT3_multrj_100epoch_SIM_dict.npz\"\n",
    "# with open(fname_dict, 'wb') as f:\n",
    "#     np.savez(f,SIMS_dict = SIMS_dict)\n",
    "\n",
    "# # save SIMS\n",
    "# fname_sims = \"data/helix_assoc_new/helix_assoc_new_PT3_multrj_100epoch_SIMSpartial.npz\"\n",
    "# with open(fname_sims, 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#             SIMS_adj=SIMS_adj,SIMS_G=SIMS_G,SIMS_HT=SIMS_HT,\n",
    "#             SIMS_adj_uniq=SIMS_adj_uniq,SIMS_G_uniq=SIMS_G_uniq, SIMS_HT_uniq=SIMS_HT_uniq,\n",
    "#             occp_S=occ_density_S, coord_id_S=coord_id_S,SIMS_dict = SIMS_dict\n",
    "#             )\n",
    "    \n",
    "    \n",
    "fname_sims = \"data/helix_assoc_new/helix_assoc_new_PT3_multrj_100epoch_SIMSpartial.npz\"\n",
    "npzfile = np.load(fname_sims)\n",
    "\n",
    "SIMS_adj=npzfile[\"SIMS_adj\"];SIMS_G=npzfile[\"SIMS_G\"];SIMS_HT=npzfile[\"SIMS_HT\"];\n",
    "SIMS_adj_uniq=npzfile[\"SIMS_adj_uniq\"];SIMS_G_uniq=npzfile[\"SIMS_G_uniq\"];SIMS_HT_uniq=npzfile[\"SIMS_HT_uniq\"];\n",
    "occ_density_S=npzfile[\"occp_S\"];coord_id_S=npzfile[\"coord_id_S\"];SIMS_dict=npzfile[\"SIMS_dict\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert adjacency matrix scattering coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMS_scar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single trajectory\n",
    "# # convert all states\n",
    "# scat_coeff_array = transform_dataset(SIM_adj)\n",
    "# SIM_scar = get_normalized_moments(scat_coeff_array).squeeze()\n",
    "# print(SIM_scar.shape)\n",
    "\n",
    "# # convert only unique states to get unique scattering\n",
    "# scat_coeff_array = transform_dataset(SIM_adj_uniq)\n",
    "# SIM_scar_uniq = get_normalized_moments(scat_coeff_array).squeeze()\n",
    "# print(SIM_scar_uniq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# # Multiple trajectories\n",
    "scat_coeff_array_S = transform_dataset(SIMS_adj)\n",
    "SIMS_scar = get_normalized_moments(scat_coeff_array_S).squeeze()\n",
    "print(SIMS_scar.shape)\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large trajectories states\n",
    "SIMS_scar1 = get_normalized_moments(transform_dataset(SIMS_adj[:100000])).squeeze()\n",
    "SIMS_scar2 = get_normalized_moments(transform_dataset(SIMS_adj[100000:200000])).squeeze()\n",
    "SIMS_scar3 = get_normalized_moments(transform_dataset(SIMS_adj[200000:300000])).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_scar4 = get_normalized_moments(transform_dataset(SIMS_adj[300000:400000])).squeeze()\n",
    "SIMS_scar5 = get_normalized_moments(transform_dataset(SIMS_adj[400000:500000])).squeeze()\n",
    "SIMS_scar6 = get_normalized_moments(transform_dataset(SIMS_adj[500000:600000])).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_scar7 = get_normalized_moments(transform_dataset(SIMS_adj[600000:700000])).squeeze()\n",
    "SIMS_scar8 = get_normalized_moments(transform_dataset(SIMS_adj[700000:800000])).squeeze()\n",
    "SIMS_scar9 = get_normalized_moments(transform_dataset(SIMS_adj[800000:])).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_scar = np.concatenate((SIMS_scar1,SIMS_scar2,SIMS_scar3,SIMS_scar4,SIMS_scar5,SIMS_scar6,SIMS_scar7,SIMS_scar8,SIMS_scar9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save sim_scar data\n",
    "# fname_data = \"data/helix_assoc_new/helix_assoc_PT3_multrj_100epoch_SIMS_scar.npz\"\n",
    "# with open(fname_data, 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#             SIMS_scar=SIMS_scar,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_scar.shape, (np.unique(SIMS_scar,axis=0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMS_scar_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# convert only unique states to get unique scattering\n",
    "scat_coeff_array_S = transform_dataset(SIMS_adj_uniq)\n",
    "SIMS_scar_uniq = get_normalized_moments(scat_coeff_array_S).squeeze()\n",
    "print(SIMS_scar_uniq.shape)\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save sim_scar data\n",
    "# fname_data = \"data/helix_assoc_new/helix_assoc_PT3_multrj_100epoch_SIMS_scar_uniq.npz\"\n",
    "# with open(fname_data, 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#             SIMS_scar_uniq=SIMS_scar_uniq,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_scar_uniq.shape, (np.unique(SIMS_scar_uniq,axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save sim_scar data\n",
    "# fname_data = \"data/helix_assoc_new/helix_assoc_PT3_multrj_100epoch_SIMS_scarall.npz\"\n",
    "# with open(fname_data, 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#             SIMS_scar=SIMS_scar,\n",
    "#             SIMS_scar_uniq=SIMS_scar_uniq,\n",
    "#             )\n",
    "\n",
    "# # fname_data = \"data/helix_assos/helix_assos_PT0_multrj_100epoch__newSIMS.npz\"\n",
    "# # npzfile = np.load(fname_data)\n",
    "# # SIMS_adj=npzfile[\"SIMS_adj\"];SIMS_scar=npzfile[\"SIMS_scar\"];SIMS_G=npzfile[\"SIMS_G\"];SIMS_HT=npzfile[\"SIMS_HT\"];\n",
    "# # SIMS_adj_uniq=npzfile[\"SIMS_adj_uniq\"];SIMS_scar_uniq=npzfile[\"SIMS_scar_uniq\"];SIMS_G_uniq=npzfile[\"SIMS_G_uniq\"];SIMS_HT_uniq=npzfile[\"SIMS_HT_uniq\"];\n",
    "# # occp_S=npzfile[\"occp_S\"];coord_id_S=npzfile[\"coord_id_S\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Split data into tranning and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"load saved trajectories data\n",
    "# \"\"\"\n",
    "# # # single trajectory\n",
    "# # fname_data = \"/Users/chenwei/Desktop/Github/RPE/code/data/helix_assos/assos_PT0_1sim_20C_51_1985epoch.npz\"\n",
    "\n",
    "# # multiple trajectories\n",
    "# fname_data = \"/Users/chenwei/Desktop/Github/RPE/code/data/helix_assos/helix_assos_PT3_multrj_100epoch.h5\" \n",
    "\n",
    "# h5file = h5py.File(fname_data,'r') \n",
    "# h5file.keys()\n",
    "\n",
    "# SIMS_adj = h5file[\"SIM_adj\"][()] #\n",
    "# SIMS_scar = h5file[\"SIM_scar\"][()] #\n",
    "# SIMS_G = h5file[\"SIM_G\"][()] #\n",
    "# SIMS_HT = h5file[\"SIM_HT\"][()] #\n",
    "\n",
    "# SIMS_adj_uniq = h5file[\"SIM_adj_uniq\"][()] #\n",
    "# SIMS_scar_uniq = h5file[\"SIM_scar_uniq\"][()] #\n",
    "# SIMS_G_uniq = h5file[\"SIM_G_uniq\"][()] #\n",
    "# SIMS_HT_uniq = h5file[\"SIM_HT_uniq\"][()] #\n",
    "\n",
    "# coord_id_S = h5file[\"coord_id\"][()] #\n",
    "# data_embed = h5file[\"data_embed\"][()] #\n",
    "# occ_density_S = h5file[\"occp\"][()] #\n",
    "\n",
    "# indices_S = h5file[\"indices\"][()] \n",
    "# pca_coords = h5file[\"pca_coords\"][()]\n",
    "# pca_all_coords = h5file[\"pca_all_coords\"][()]\n",
    "\n",
    "# phate_coords = h5file[\"phate_coords\"][()] #\n",
    "# phate_all_coords = h5file[\"phate_all_coords\"][()] #\n",
    "\n",
    "# print(SIMS_adj.shape,SIMS_scar.shape,SIMS_G.shape,SIMS_HT.shape)\n",
    "# print(SIMS_adj_uniq.shape,SIMS_scar_uniq.shape,SIMS_G_uniq.shape,SIMS_HT_uniq.shape)   \n",
    "# print(coord_id_S.shape,data_embed.shape,occ_density_S.shape,indices_S.shape)\n",
    "# print(pca_coords.shape,pca_all_coords.shape,phate_coords.shape,phate_all_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shape of split data\n",
    "    train_data: [tr_adjs, tr_coeffs, tr_energies]\n",
    "    test_data: [te_adjs, te_coeffs, te_energies]\n",
    "\"\"\"\n",
    "# train_data,test_data = split_data(SIM_adj_uniq,SIM_scar_uniq,SIM_G_uniq)  # single trj\n",
    "train_data,test_data = split_data(SIMS_adj_uniq,SIMS_scar_uniq,SIMS_G_uniq)  # multiple trj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train and test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Structure of train_tup when gnn=False\n",
    "    train_tup: [train_coeffs,train_energy] \n",
    "\"\"\"\n",
    "train_loader, train_tup, test_tup, valid_loader,early_stop_callback = load_trte(train_data,test_data,\n",
    "                                              batch_size=64)\n",
    "train_tup[0].shape, test_tup[0].shape, train_loader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters\n",
    "\n",
    "input_dim = train_tup[0].shape[-1]\n",
    "len_epoch = len(train_loader)\n",
    "\n",
    "hparams = {\n",
    "    'input_dim':  input_dim,\n",
    "    'bottle_dim': 25,\n",
    "    'hidden_dim': 400, #not used in model\n",
    "    \n",
    "    'len_epoch': len_epoch,\n",
    "    'learning_rate': 0.0001,\n",
    "    'max_epochs': 100,  # PT0 --> 1985 epoch  # PT3 --> 60ï¼Œ 100, 150, 756(overtfit) epoch\n",
    "    'n_gpus': 0,\n",
    "    'batch_size': 64, #not used in model\n",
    "    \n",
    "    'alpha':1.0,\n",
    "    'beta':0.0001,\n",
    "\n",
    "}\n",
    "\n",
    "hparams = argparse.Namespace(**hparams)\n",
    "\n",
    "model = GSAE(hparams)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer.from_argparse_args(hparams,\n",
    "                                        max_epochs=hparams.max_epochs,\n",
    "                                        gpus=hparams.n_gpus,\n",
    "                                        # callbacks=[early_stop_callback],\n",
    "                                        )\n",
    "trainer.fit(model=model,\n",
    "            train_dataloader=train_loader,\n",
    "            val_dataloaders=valid_loader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/ --host localhost --port 8000\n",
    "#  http://localhost:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "# fname_model = \"models/{}_model_{}epoch.pickle\".format(STRAND_NAME,hparams.max_epochs) # single trj\n",
    "fname_model = \"models/helix_assoc_new_PT3_multrj_model_{}epoch.pickle\".format(hparams.max_epochs) # multiple trj\n",
    "\n",
    "pickle.dump(model, open(fname_model, 'wb'))\n",
    "print('Trained model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname_model = \"models/helix_assoc_new_PT0_multrj_model_100epoch.pickle\"\n",
    "fname_model = \"models/helix_assoc_new_PT3_multrj_model_100epoch.pickle\"\n",
    "\n",
    "model = pickle.load(open(fname_model, 'rb'))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # single trajectory\n",
    "# with torch.no_grad():\n",
    "#         data_embed = model.embed(torch.Tensor(SIM_scar_uniq))[0]\n",
    "\n",
    "# # multiple trajectories\n",
    "with torch.no_grad():\n",
    "        data_embed = model.embed(torch.Tensor(SIMS_scar_uniq))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do PCA for GSAE embeded data\n",
    "pca_coords = PCA(n_components=3).fit_transform(data_embed)\n",
    "\n",
    "# # get all pca embedded states coordinates\n",
    "# pca_all_coords = pca_coords[coord_id]  # single trj\n",
    "pca_all_coords = pca_coords[coord_id_S]  # multiple trj\n",
    "\n",
    "pca_coords.shape, pca_all_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do PHATE for GSAE embeded data\n",
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "phate_coords = phate_operator.fit_transform(data_embed)\n",
    "\n",
    "# # get all phate embedded states coordinates\n",
    "# phate_all_coords = phate_coords[coord_id]  # single trj\n",
    "phate_all_coords = phate_coords[coord_id_S]  # multiple trj\n",
    "\n",
    "phate_coords.shape, phate_all_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(pca_coords,axis=0)).shape, (np.unique(pca_all_coords,axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(phate_coords,axis=0)).shape, (np.unique(phate_all_coords,axis=0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all dats to h5 / npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NPZ file\n",
    "    Save all obtained data to npz file,\n",
    "    Single trajectory\n",
    "\"\"\"\n",
    "# fname_data = \"data/helix_assos/{}_{}epoch.npz\".format(STRAND_NAME,hparams.max_epochs)\n",
    "# with open(fname_data, 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#             SIM_adj=SIM_adj,SIM_scar=SIM_scar,SIM_G=SIM_G,SIM_HT=SIM_HT,\n",
    "#             SIM_adj_uniq=SIM_adj_uniq, SIM_scar_uniq=SIM_scar_uniq,\n",
    "#             SIM_G_uniq=SIM_G_uniq, SIM_HT_uniq=SIM_HT_uniq,\n",
    "#             # SIM_dict=SIM_dict, \n",
    "#             occp=occ_density,\n",
    "#             data_embed=data_embed, coord_id=coord_id,\n",
    "#             pca_coords=pca_coords, pca_all_coords=pca_all_coords,\n",
    "#             phate_coords=phate_coords, phate_all_coords=phate_all_coords,\n",
    "#             )\n",
    "\n",
    "\n",
    "\"\"\" Save all obtained data to npz file,\n",
    "    Multiple trajectories\n",
    "\"\"\"\n",
    "# # save for python\n",
    "# fname_data = \"data/helix_assos/helix_assos_PT0_multrj_100epoch_py.npz\"\n",
    "# with open(fname_data, 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#             SIMS_adj=SIMS_adj,SIMS_scar=SIMS_scar,SIMS_G=SIMS_G,SIMS_HT=SIMS_HT,\n",
    "#             SIMS_adj_uniq=SIMS_adj_uniq, SIMS_scar_uniq=SIMS_scar_uniq,\n",
    "#             SIMS_G_uniq=SIMS_G_uniq, SIMS_HT_uniq=SIMS_HT_uniq,\n",
    "#             occp_S=occ_density_S, coord_id_S=coord_id_S,\n",
    "#             data_embed=data_embed, \n",
    "#             pca_coords=pca_coords, pca_all_coords=pca_all_coords,\n",
    "#             phate_coords=phate_coords, phate_all_coords=phate_all_coords,\n",
    "#             )\n",
    "\n",
    "# # save for julia\n",
    "# fname_data = \"data/helix_assos/helix_assos_PT0_multrj_100epoch_jl.npz\"\n",
    "# with open(fname_data, 'wb') as f:\n",
    "#     np.savez(f,\n",
    "#             # SIMS_adj=SIMS_adj,  # no good for julia, too large\n",
    "#             # SIMS_scar=SIMS_scar,  # no good for julia, too large\n",
    "#             SIMS_G=SIMS_G,\n",
    "#             SIMS_HT=SIMS_HT,\n",
    "#             # SIMS_adj_uniq=SIMS_adj_uniq,  # no good for julia, too large\n",
    "#             SIMS_scar_uniq=SIMS_scar_uniq,\n",
    "#             SIMS_G_uniq=SIMS_G_uniq, \n",
    "#             SIMS_HT_uniq=SIMS_HT_uniq,\n",
    "#             occp_S=occ_density_S, coord_id_S=coord_id_S,\n",
    "#             data_embed=data_embed, \n",
    "#             pca_coords=pca_coords, pca_all_coords=pca_all_coords,\n",
    "#             phate_coords=phate_coords, phate_all_coords=phate_all_coords,\n",
    "#             )\n",
    "\n",
    "\n",
    "# print(npyfile[\"SIMS_adj\"].shape, npyfile[\"SIMS_scar\"].shape, npyfile[\"SIMS_G\"].shape, npyfile[\"SIMS_HT\"].shape,\"\\n\",\n",
    "# npyfile[\"SIMS_adj_uniq\"].shape, npyfile[\"SIMS_scar_uniq\"].shape, npyfile[\"SIMS_G_uniq\"].shape, npyfile[\"SIMS_HT_uniq\"].shape, \"\\n\",\n",
    "# npyfile[\"occp_S\"].shape, npyfile[\"data_embed\"].shape, npyfile[\"coord_id_S\"].shape, \"\\n\",\n",
    "# npyfile[\"pca_coords\"].shape, npyfile[\"pca_all_coords\"].shape, npyfile[\"phate_coords\"].shape, npyfile[\"phate_all_coords\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" HDF5 file\n",
    "    Single trajectory\n",
    "\"\"\"\n",
    "# fname_data_h5 = \"data/helix_assos/assos_PT3_1sim_20C_21_60epoch.h5\"\n",
    "# save_h5(fname_data_h5,\n",
    "#             SIM_adj, SIM_scar, SIM_G, SIM_HT,\n",
    "#             SIM_adj_uniq, SIM_scar_uniq, SIM_G_uniq, SIM_HT_uniq,\n",
    "#             occ_density, data_embed, coord_id,\n",
    "#             pca_coords, pca_all_coords,\n",
    "#             phate_coords, phate_all_coords)\n",
    "\n",
    "\"\"\" \n",
    "    Multiple trajectories\n",
    "\"\"\"\n",
    "# # fname_data_h5 = \"data/helix_assoc_new/helix_assoc_new_PT0_multrj_100epoch.h5\"\n",
    "# fname_data_h5 = \"data/helix_assoc_new/helix_assoc_new_PT3_multrj_100epoch.h5\"\n",
    "\n",
    "# save_h5(fname_data_h5,\n",
    "#             SIMS_adj, SIMS_scar, SIMS_G, SIMS_HT,\n",
    "#             SIMS_adj_uniq, SIMS_scar_uniq, SIMS_G_uniq, SIMS_HT_uniq,\n",
    "#             occ_density_S, data_embed, coord_id_S, indices_S,\n",
    "#             pca_coords, pca_all_coords,\n",
    "#             phate_coords, phate_all_coords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_data = \"/Users/chenwei/Desktop/Github/RPE/code/data/helix_assoc_new/helix_assoc_new_PT3_multrj_100epoch.h5\"\n",
    "# fname_data = \"/Users/chenwei/Desktop/Github/RPE/code/data/helix_assoc_new/helix_assoc_new_PT0_multrj_100epoch.h5\"\n",
    "\n",
    "h5file = h5py.File(fname_data,'r') \n",
    "print(SIMS_retrieve[h5file[\"indices\"][()][h5file[\"occp\"][()].argmax()]])\n",
    "h5file.keys()\n",
    "\n",
    "# print(h5file[\"SIM_adj\"].shape, h5file[\"SIM_scar\"].shape, h5file[\"SIM_G\"].shape, h5file[\"SIM_HT\"].shape,\"\\n\",\n",
    "# h5file[\"SIM_adj_uniq\"].shape, h5file[\"SIM_scar_uniq\"].shape, h5file[\"SIM_G_uniq\"].shape, h5file[\"SIM_HT_uniq\"].shape, \"\\n\",\n",
    "# h5file[\"coord_id\"].shape, h5file[\"data_embed\"].shape, h5file[\"occp\"].shape, h5file[\"indices\"].shape,\"\\n\",\n",
    "# h5file[\"pca_coords\"].shape, h5file[\"pca_all_coords\"].shape, h5file[\"phate_coords\"].shape, h5file[\"phate_all_coords\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = h5file[\"pca_all_coords\"][:,0]\n",
    "Y = h5file[\"pca_all_coords\"][:,1]\n",
    "Z = h5file[\"pca_all_coords\"][:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=h5file[\"SIM_G\"],\n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = h5file[\"pca_coords\"][:,0]\n",
    "Y = h5file[\"pca_coords\"][:,1]\n",
    "Z = h5file[\"pca_coords\"][:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=h5file[\"SIM_G_uniq\"], \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = h5file[\"pca_coords\"][:,0]\n",
    "Y = h5file[\"pca_coords\"][:,1]\n",
    "Z = h5file[\"pca_coords\"][:,2]\n",
    "\n",
    "# PCA: 3 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "im = ax.scatter3D(X,Y,Z,\n",
    "          c=h5file[\"SIM_G_uniq\"],      \n",
    "          cmap='plasma')\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "z = [Z[0], Z[-1]]\n",
    "ax.scatter(x,y,z,s=100,c=\"green\",alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try use PCA directly without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_coords1 = PCA(n_components=3).fit_transform(npyfile[\"SIM_scar_uniq\"])  # single trj\n",
    "pca_coords1 = PCA(n_components=3).fit_transform(h5file[\"SIM_scar_uniq\"])   # multiple trj\n",
    "\n",
    "X = pca_coords1[:,0]\n",
    "Y = pca_coords1[:,1]\n",
    "Z = pca_coords1[:,2]\n",
    "\n",
    "# PCA: 2 components\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X, Y, \n",
    "          c=h5file[\"SIM_G_uniq\"], \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X[0],X[-1]]\n",
    "y = [Y[0],Y[-1]]\n",
    "plt.scatter(x,y,s=150, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i]-0.3,y[i]-0.3),fontsize=15,c=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PHATE Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phate = h5file[\"phate_all_coords\"][:,0]\n",
    "Y_phate = h5file[\"phate_all_coords\"][:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X_phate,Y_phate,\n",
    "                c=h5file[\"SIM_G\"],   # multiple trj               \n",
    "                cmap='plasma',\n",
    "               )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X_phate[0],X_phate[-1]]\n",
    "y = [Y_phate[0],Y_phate[-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=30,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phate = h5file[\"phate_coords\"][:,0]\n",
    "Y_phate = h5file[\"phate_coords\"][:,1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(X_phate,Y_phate,\n",
    "                c=h5file[\"SIM_G_uniq\"],                 \n",
    "                cmap='plasma',\n",
    "               )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [X_phate[0],X_phate[-1]]\n",
    "y = [Y_phate[0],Y_phate[-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=30,c=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PHATE without AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "# phate1 = phate_operator.fit_transform(npyfile[\"SIM_scar_uniq\"])   # single trj\n",
    "phate1 = phate_operator.fit_transform(h5file[\"SIM_scar_uniq\"])   # multiple trj`\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.scatter(phate1[:,0],\n",
    "          phate1[:,1],\n",
    "          c=h5file[\"SIM_G_uniq\"], \n",
    "          cmap='plasma',\n",
    "        )\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "annotations=[\"I\",\"F\"]\n",
    "x = [phate1[:,0][0],phate1[:,0][-1]]\n",
    "y = [phate1[:,1][0],phate1[:,1][-1]]\n",
    "plt.scatter(x,y,s=50, c=\"green\", alpha=1)\n",
    "for i, label in enumerate(annotations):\n",
    "    plt.annotate(label, (x[i],y[i]),fontsize=20,c=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw helix structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from networkx.drawing.nx_pylab import draw_networkx\n",
    "from networkx.drawing.layout import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from misc import *\n",
    "\n",
    "f = open('./data/helix_assos/assos_PT0_1sim_20C_51.txt', 'r') # PT0\n",
    "STRAND_NAME = \"assos_PT0_1sim_20C_51\"\n",
    "\n",
    "# define absorbing (final) state structure\n",
    "FINAL_STRUCTURE = \"(((((((((((((((((((((((((+)))))))))))))))))))))))))\"\n",
    "\n",
    "SIM = loadtrj(f,FINAL_STRUCTURE,type=\"Multiple\")\n",
    "SIM_retrieve = np.array(SIM)\n",
    "SIM_concat = concat_helix_structures(SIM) \n",
    "\n",
    "print(\"SIM: \", len(SIM))\n",
    "print(\"SIM_retrieve: \", SIM_retrieve.shape)\n",
    "print(\"SIM_concat: \", len(SIM_concat))\n",
    "\n",
    "# get single trajectory's data\n",
    "# get adjacency matrix, energy, and holding time for each state\n",
    "SIM_adj,SIM_G,SIM_T,SIM_HT = sim_adj(SIM_concat)\n",
    "print(SIM_adj.shape,SIM_G.shape,SIM_T.shape,SIM_HT.shape)\n",
    "\n",
    "# single trajectory\n",
    "indices,occ_density,SIM_adj_uniq,SIM_G_uniq,SIM_T_uniq,SIM_HT_uniq \\\n",
    "     = get_unique(SIM_concat,SIM_adj,SIM_G,SIM_T,SIM_HT)\n",
    "print(indices.shape, occ_density.shape, SIM_adj_uniq.shape,SIM_G_uniq.shape,SIM_T_uniq.shape,SIM_HT_uniq.shape)\n",
    "\n",
    "print(np.unique(SIM_adj_uniq,axis=0).shape, SIM_adj_uniq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_neighborhood_graphs = [nx.convert_matrix.from_numpy_matrix(x) for x in SIM_adj[:5]]\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize=(20,5))\n",
    "\n",
    "for i, g in enumerate(ex_neighborhood_graphs):\n",
    "    ppos = kamada_kawai_layout(g)\n",
    "    nx.draw(g, ax=ax[i], node_size=5, pos=ppos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_neighborhood_graphs = [nx.convert_matrix.from_numpy_matrix(x) for x in SIM_adj[-3:]]\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "\n",
    "for i, g in enumerate(ex_neighborhood_graphs):\n",
    "    ppos = kamada_kawai_layout(g)\n",
    "    nx.draw(g, ax=ax[i], node_size=5, pos=ppos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_neighborhood_graphs = [nx.convert_matrix.from_numpy_matrix(x) for x in SIM_adj[-20:-15]]\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize=(20,5))\n",
    "\n",
    "for i, g in enumerate(ex_neighborhood_graphs):\n",
    "    ppos = kamada_kawai_layout(g)\n",
    "    nx.draw(g, ax=ax[i], node_size=5, pos=ppos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_neighborhood_graphs = [nx.convert_matrix.from_numpy_matrix(x) for x in SIM_adj[-16:-11]]\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize=(20,5))\n",
    "\n",
    "for i, g in enumerate(ex_neighborhood_graphs):\n",
    "    ppos = kamada_kawai_layout(g)\n",
    "    nx.draw(g, ax=ax[i], node_size=5, pos=ppos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in range(-20,0,5):\n",
    "    fig, ax = plt.subplots(1,5, figsize=(20,5))\n",
    "    ex_neighborhood_graphs = [nx.convert_matrix.from_numpy_matrix(x) for x in SIM_adj[j:j+5]]\n",
    "    if j+5 == 0:\n",
    "        ex_neighborhood_graphs = [nx.convert_matrix.from_numpy_matrix(x) for x in SIM_adj[j:]]\n",
    "\n",
    "    for i, g in enumerate(ex_neighborhood_graphs):\n",
    "        ppos = kamada_kawai_layout(g)\n",
    "        nx.draw(g, ax=ax[i], node_size=5, pos=ppos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '...(..............)......'\n",
    "s1 = '...((............))......'\n",
    "s2 = '.........................'\n",
    "\n",
    "s12 = s1+s2\n",
    "print(s12,len(s12))\n",
    "\n",
    "d_a2 = dot2adj(s12)\n",
    "print(d_a2, d_a2.shape)\n",
    "\n",
    "g = nx.convert_matrix.from_numpy_matrix(d_a2)\n",
    "\n",
    "# fig, ax = plt.plot(figsize=(20,5))\n",
    "\n",
    "nx.draw(g, node_size=5, pos=kamada_kawai_layout(g)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"....(((.....)))..((...)).\"\n",
    "s2 = \".(.(..............).)....\"\n",
    "s12 = s1+s2\n",
    "\n",
    "print(s12,len(s12))\n",
    "\n",
    "d_a2 = dot2adj(s12)\n",
    "print(d_a2, d_a2.shape)\n",
    "\n",
    "g = nx.convert_matrix.from_numpy_matrix(d_a2)\n",
    "\n",
    "# fig, ax = plt.plot(figsize=(20,5))\n",
    "\n",
    "nx.draw(g, node_size=50, pos=kamada_kawai_layout(g),with_labels=True,font_size=10)\n",
    "\n",
    "print(# 24, 32\n",
    "      d_a2[23,31], d_a2[31,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"....(((.....)))..((...)).\"\n",
    "s2 = \".(.(..............).)....\"\n",
    "s12 = s1+s2\n",
    "\n",
    "print(s12,len(s12))\n",
    "\n",
    "d_a2 = dot2adj(s12)\n",
    "print(d_a2, d_a2.shape)\n",
    "\n",
    "g = nx.convert_matrix.from_numpy_matrix(d_a2)\n",
    "\n",
    "# fig, ax = plt.plot(figsize=(20,5))\n",
    "\n",
    "nx.draw(g, node_size=50, pos=circular_layout(g),with_labels=True,font_size=10)\n",
    "\n",
    "print(# 24, 32\n",
    "      d_a2[23,31], d_a2[31,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.convert_matrix.from_numpy_matrix(SIM_adj[-3])\n",
    "nx.draw(g, node_size=50, pos=shell_layout(g),with_labels=True,font_size=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.convert_matrix.from_numpy_matrix(SIM_adj[11])\n",
    "nx.draw(g, node_size=50, pos=shell_layout(g),with_labels=True,font_size=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.convert_matrix.from_numpy_matrix(SIM_adj[-1])\n",
    "nx.draw(g, node_size=50, pos=shell_layout(g),with_labels=True,font_size=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '....(.....)..............'\n",
    "s2 = '...((............))......'\n",
    "s12 = s1+s2\n",
    "\n",
    "print(s12,len(s12))\n",
    "\n",
    "d_a2 = dot2adj(s12)\n",
    "print(d_a2, d_a2.shape)\n",
    "\n",
    "g = nx.convert_matrix.from_numpy_matrix(d_a2)\n",
    "\n",
    "# fig, ax = plt.plot(figsize=(20,5))\n",
    "\n",
    "nx.draw(g, node_size=5, pos=kamada_kawai_layout(g)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s12 = \"....(((.....))).(((...))..(.(..............).).)..\"\n",
    "\n",
    "print(s12,len(s12))\n",
    "\n",
    "d_a2 = dot2adj(s12)\n",
    "# print(d_a2, d_a2.shape)\n",
    "\n",
    "g = nx.convert_matrix.from_numpy_matrix(d_a2)\n",
    "\n",
    "# fig, ax = plt.plot(figsize=(20,5))\n",
    "\n",
    "nx.draw(g, node_size=5, pos=kamada_kawai_layout(g)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s12 = \"....(((.....))).(((...))..(.(..............).).)..\"\n",
    "\n",
    "print(s12,len(s12))\n",
    "\n",
    "d_a2 = dot2adj(s12)\n",
    "# print(d_a2, d_a2.shape)\n",
    "\n",
    "g = nx.convert_matrix.from_numpy_matrix(d_a2)\n",
    "\n",
    "# fig, ax = plt.plot(figsize=(20,5))\n",
    "\n",
    "nx.draw(g, node_size=50, pos=shell_layout(g),with_labels=True,font_size=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s12 = '.((..))..(..).'\n",
    "\n",
    "print(s12,len(s12))\n",
    "\n",
    "d_a2 = dot2adj(s12,hairpin=True,helix=False)\n",
    "# print(d_a2, d_a2.shape)\n",
    "\n",
    "g = nx.convert_matrix.from_numpy_matrix(d_a2)\n",
    "\n",
    "# fig, ax = plt.plot(figsize=(20,5))\n",
    "\n",
    "nx.draw(g, node_size=50, pos=shell_layout(g),with_labels=True,font_size=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s12 = \"....((((.....(.....).....))..))..(...........)....\"\n",
    "print(s12,len(s12))\n",
    "\n",
    "d_a2 = dot2adj(s12,helix=True)\n",
    "print(s12,len(s12),d_a2[24,25])\n",
    "\n",
    "# print(d_a2, d_a2.shape)\n",
    "\n",
    "g = nx.convert_matrix.from_numpy_matrix(d_a2)\n",
    "\n",
    "# fig, ax = plt.plot(figsize=(20,5))\n",
    "\n",
    "nx.draw(g, node_size=90, pos=kamada_kawai_layout(g),with_labels=True,font_size=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s12 = \"(.)...\"\n",
    "\n",
    "# d_a2 = dot2adj(s12,helix=False,hairpin=True)\n",
    "d_a2 = dot2adj(s12,helix=True)\n",
    "\n",
    "print(s12,len(s12),d_a2[int(len(s12)/2-1),int(len(s12)/2)])\n",
    "\n",
    "# print(d_a2, d_a2.shape)\n",
    "\n",
    "g = nx.convert_matrix.from_numpy_matrix(d_a2)\n",
    "\n",
    "# fig, ax = plt.plot(figsize=(20,5))\n",
    "\n",
    "nx.draw(g, node_size=90, pos=kamada_kawai_layout(g),with_labels=True,font_size=10) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('GSAE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "960be4cf653645892f8c5040dafd36ede801108a57618a05d934dc04f8033a0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
