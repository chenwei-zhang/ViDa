{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import yaml\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from vida.data_processing.load_data import *\n",
    "from vida.data_processing.convertor import *\n",
    "from vida.data_processing.misc import *\n",
    "from vida.data_processing.split_data import *\n",
    "\n",
    "from vida.model.scatter_transform import transform_dataset, get_normalized_moments\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import phate\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load First Step data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Jordan's fist step data \n",
    "rxn_name = \"npstates5m_39\"\n",
    "fnpz_data = f\"./data/jordan/multistrand_firststep/bins_{rxn_name}.npz\"\n",
    "\n",
    "trajs = np.load(fnpz_data,allow_pickle=True)\n",
    "print(trajs.files)\n",
    "\n",
    "trajs_states = trajs['trajs_states'] \n",
    "trajs_times = trajs['trajs_times']\n",
    "trajs_energies = trajs['trajs_energies']\n",
    "trajs_types = trajs['trajs_types'].item()\n",
    "trajs['trajs_states'].shape, len(trajs_types) # total number of trajectories, each trajectory is a list of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect successful and unsuccessful trajectories\n",
    "num_success_traj = 0\n",
    "success_threshold = 50 # 3 for npstates5m_46, 50 for others\n",
    "select_trajs = []\n",
    "select_trajs_id = []\n",
    "select_times = []\n",
    "select_energies = []\n",
    "final_states = '(((((((((((((((((((((((+)))))))))))))))))))))))'\n",
    "\n",
    "i = 0\n",
    "while num_success_traj < success_threshold:\n",
    "    select_trajs.append(trajs_states[i])\n",
    "    select_times.append(trajs_times[i])\n",
    "    select_energies.append(trajs_energies[i])\n",
    "    if trajs_states[i][-1] == final_states:\n",
    "        num_success_traj += 1\n",
    "        select_trajs_id.append(i)\n",
    "    i += 1\n",
    "    \n",
    "select_trajs = np.array(select_trajs, dtype=object)\n",
    "select_times = np.array(select_times, dtype=object)\n",
    "select_energies = np.array(select_energies, dtype=object)\n",
    "\n",
    "print(select_trajs.shape)\n",
    "print(len(select_trajs_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Define the filename for the saved data\n",
    "file_name = \"Hata-39.pkl.gz\"\n",
    "\n",
    "# Create a dictionary to store both the NumPy array and the set\n",
    "data_to_save = {\n",
    "    \"select_trajs\": select_trajs,\n",
    "    \"select_times\": select_times,\n",
    "    \"select_energies\": select_energies,\n",
    "    \"select_types\": SIMS_type_uniq,\n",
    "}\n",
    "\n",
    "# Save the data to the file using pickle\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(data_to_save, file)\n",
    "\n",
    "# # To load the data back later, you can use the following code:\n",
    "# with open(file_name, 'rb') as file:\n",
    "#     loaded_data = pickle.load(file)\n",
    "\n",
    "# # You can access the NumPy array and set as follows:\n",
    "# select_trajs = loaded_data[\"select_trajs\"]\n",
    "# select_times = loaded_data[\"select_times\"]\n",
    "# select_energies = loaded_data[\"select_energies\"]\n",
    "# SIMS_type_uniq = loaded_data[\"select_types\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# # for reaction 45\n",
    "## select 1000 failed trajectories randomly\n",
    "## then add 50 successful trajectories\n",
    "all_idx = np.arange(len(select_trajs))\n",
    "\n",
    "failed_idx = np.random.choice(np.delete(all_idx, select_trajs_id), size=1000, replace=False)\n",
    "\n",
    "# add 50 successful trajectories\n",
    "all_new_idx = np.concatenate((failed_idx, select_trajs_id))\n",
    "\n",
    "select_trajs = select_trajs[all_new_idx]\n",
    "select_times = select_times[all_new_idx]\n",
    "select_energies = select_energies[all_new_idx]\n",
    "\n",
    "print(select_trajs.shape)\n",
    "\n",
    "# make the new select_trajs_id\n",
    "num_success_traj = 0\n",
    "success_threshold = 50 \n",
    "select_trajs_id = []\n",
    "i = 0\n",
    "while num_success_traj < success_threshold:\n",
    "    if select_trajs[i][-1] == final_states:\n",
    "        num_success_traj += 1\n",
    "        select_trajs_id.append(i)\n",
    "    i += 1\n",
    "    \n",
    "# make the new select_trajs_id\n",
    "num_success_traj = 0\n",
    "success_threshold = 50 \n",
    "select_trajs_id = []\n",
    "i = 0\n",
    "while num_success_traj < success_threshold:\n",
    "    if select_trajs[i][-1] == final_states:\n",
    "        num_success_traj += 1\n",
    "        select_trajs_id.append(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMS_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert concantenate two individual structures to one structure\n",
    "def concat_helix_structures(dp):\n",
    "    \"\"\"concatenate two individual structures to one structure\n",
    "    Args:\n",
    "        SIM: list of individual structures\n",
    "    Returns:\n",
    "        SIM_concat: concatenated structure\n",
    "    \"\"\"\n",
    "    dp_concat = copy.deepcopy(dp)\n",
    "    dp_pair = []\n",
    "    for i in range(len(dp_concat)):\n",
    "        if \"&\" in dp_concat[i]:\n",
    "            dp_concat[i] = dp_concat[i].replace(\"&\",\"\")\n",
    "            dp_pair.append(0)\n",
    "            \n",
    "        if \"+\" in dp_concat[i]:\n",
    "            dp_concat[i] = dp_concat[i].replace(\"+\",\"\")\n",
    "            dp_pair.append(1)\n",
    "            \n",
    "    return np.array(dp_concat), np.array(dp_pair)\n",
    "\n",
    "\n",
    "# cooncatanate all sturcutres: \n",
    "def concat_all(states, times, energies):\n",
    "    SIMS_dp = []\n",
    "    SIMS_dp_og = []\n",
    "    SIMS_pair = []\n",
    "    SIMS_G = []\n",
    "    SIMS_T = []\n",
    "    \n",
    "    for i in range(len(states)):\n",
    "    # for i in range(100):\n",
    "        sims_dp, sims_pair = concat_helix_structures(states[i])\n",
    "\n",
    "        SIMS_dp.append(sims_dp)\n",
    "        SIMS_dp_og.append(states[i])\n",
    "        SIMS_pair.append(sims_pair)\n",
    "        SIMS_G.append(energies[i])\n",
    "        SIMS_T.append(times[i])\n",
    "    \n",
    "    SIMS_dp = np.concatenate(SIMS_dp)\n",
    "    SIMS_dp_og = np.concatenate(SIMS_dp_og)\n",
    "    SIMS_pair = np.concatenate(SIMS_pair)\n",
    "    SIMS_G = np.concatenate(SIMS_G)\n",
    "    SIMS_T = np.concatenate(SIMS_T)\n",
    "        \n",
    "    return SIMS_dp, SIMS_dp_og, SIMS_pair, SIMS_G, SIMS_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_dp, SIMS_dp_og, SIMS_pair, SIMS_G, SIMS_T = concat_all(select_trajs, select_times, select_energies)\n",
    "SIMS_dp.shape, SIMS_dp_og.shape, SIMS_pair.shape, SIMS_G.shape, SIMS_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remvoe duplicate data\n",
    "indices_S = np.unique(SIMS_dp,return_index=True)[1]\n",
    "\n",
    "SIMS_dp_uniq = SIMS_dp[indices_S]\n",
    "SIMS_dp_og_uniq = SIMS_dp_og[indices_S]\n",
    "SIMS_pair_uniq = SIMS_pair[indices_S]\n",
    "SIMS_G_uniq = SIMS_G[indices_S]\n",
    "\n",
    "SIMS_dp_uniq.shape, SIMS_pair_uniq.shape, SIMS_G_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index to recover to all data from unique data\n",
    "coord_id_S = np.empty(len(SIMS_dp))\n",
    "for i in range(len(SIMS_dp_uniq)):\n",
    "    temp = SIMS_dp == SIMS_dp_uniq[i]\n",
    "    indx = np.argwhere(temp==True)\n",
    "    coord_id_S[indx] = i\n",
    "coord_id_S = coord_id_S.astype(int)\n",
    "\n",
    "coord_id_S.shape, coord_id_S.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label data's type\n",
    "SIMS_type_uniq = []\n",
    "for i in range(len(SIMS_dp_og_uniq)):\n",
    "    SIMS_type_uniq.append(trajs_types[SIMS_dp_og_uniq[i]])\n",
    "SIMS_type_uniq = np.array(SIMS_type_uniq)\n",
    "SIMS_type_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dot-parenthesis notation to adjacency matrix\n",
    "def dot2adj(db_str,hairpin=False,helix=True):\n",
    "    \"\"\"converts DotBracket str to np adj matrix\n",
    "    \n",
    "    Args:\n",
    "        db_str (str): N-len dot bracket string\n",
    "    \n",
    "    Returns:\n",
    "        [np array]: NxN adjacency matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    dim = len(str(db_str))\n",
    "\n",
    "    # get pair tuples\n",
    "    pair_list = dot2pairs(db_str)\n",
    "    sym_pairs = symmetrized_edges(pair_list)\n",
    "\n",
    "\n",
    "    # initialize the NxN mat (N=len of RNA str)\n",
    "    adj_mat = np.zeros((dim,dim))\n",
    "\n",
    "    adj_mat[sym_pairs[0,:], sym_pairs[1,:]] = 1\n",
    "    \n",
    "    if hairpin == True:\n",
    "        True\n",
    "\n",
    "    if helix == True:\n",
    "        assert dim % 2 == 0, \"Not a valid helix sequence.\"\n",
    "        end2head = np.ceil(dim/2).astype(int)\n",
    "\n",
    "        if db_str[end2head-1:end2head+1] != \"()\":\n",
    "            adj_mat[end2head-1, end2head] = 0\n",
    "            adj_mat[end2head, end2head-1] = 0\n",
    "\n",
    "    return adj_mat\n",
    "\n",
    "def dot2pairs(dp_str):\n",
    "    \"\"\"converts a DotBracket str to adj matrix\n",
    "\n",
    "    uses a dual-checking method\n",
    "    - str1 = original str\n",
    "    - str2 = reversed str\n",
    "\n",
    "    iterates through both strings simult and collects indices\n",
    "    forward str iteration: collecting opening indicies - list1\n",
    "    backwards str iteration: collecting closing indices - list2\n",
    "    - as soon as a \"(\" is found in str2, the first(typo,last?) entry of list1 is paired\n",
    "      with the newly added index/entry of list2 \n",
    "    \n",
    "    Args:\n",
    "        dotbracket_str (str): dot bracket string (ex. \"((..))\")\n",
    "    \n",
    "    Returns:\n",
    "        [array]: numpy adjacency matrix\n",
    "    \"\"\" \n",
    "    dim = len(str(dp_str))\n",
    "\n",
    "    # pairing indices lists\n",
    "    l1_indcs = []\n",
    "    l2_indcs = []\n",
    "    pair_list = []\n",
    "\n",
    "    for indx in range(dim):\n",
    "        \n",
    "        # checking stage\n",
    "        # forward str\n",
    "        if dp_str[indx] == \"(\":\n",
    "\n",
    "            l1_indcs.append(indx)\n",
    " \n",
    "        if dp_str[indx] == \")\":\n",
    "            l2_indcs.append(indx)\n",
    "\n",
    "        # pairing stage\n",
    "        # check that either list is not empty\n",
    "        if len(l2_indcs) * len(l1_indcs) > 0:\n",
    "            pair = (l1_indcs[-1], l2_indcs[0])\n",
    "            pair_list.append(pair)\n",
    "        \n",
    "            # cleaning stage\n",
    "            l1_indcs.pop(-1)\n",
    "            l2_indcs.pop(0)\n",
    "    \n",
    "    # get path graph pairs\n",
    "    G = nx.path_graph(dim)\n",
    "    path_graph_pairs = G.edges()\n",
    "    \n",
    "    return pair_list + list(path_graph_pairs)\n",
    "\n",
    "def symmetrized_edges(pairs_list):\n",
    "    \n",
    "    # conver pairs to numpy array [2,-1]\n",
    "    edge_array = np.array(pairs_list)\n",
    " \n",
    "    # concatenate with opposite direction edges\n",
    "    # print(edge_array.T[[1,0]].T.shape)\n",
    "    reverse_edges = np.copy(edge_array)\n",
    "    reverse_edges[:, [0,1]] = reverse_edges[:, [1,0]]\n",
    "    full_edge_array = np.vstack((edge_array, reverse_edges))\n",
    "    \n",
    "    return full_edge_array.T\n",
    "\n",
    "# convert dot-parenthesis notation to adjacency matrix in a single trajectory\n",
    "def sim_adj_new(dps):\n",
    "    \"\"\"convert dot-parenthesis notation to adjacency matrix\n",
    "    Args:\n",
    "        sim: [list of sims] dot-parenthesis notation, energy floats\n",
    "            eg. ['...............', ...]\n",
    "    Returns:\n",
    "        (tuple): NxN adjacency np matrix\n",
    "    \"\"\"\n",
    "    adj_mtr = []\n",
    "        \n",
    "    for s in dps:\n",
    "        adj = dot2adj(s)\n",
    "        adj_mtr.append(adj)\n",
    "    adj_mtr = np.array(adj_mtr) # get adjacency matrix\n",
    "\n",
    "    return adj_mtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_adj_uniq = sim_adj_new(SIMS_dp_uniq)\n",
    "print(SIMS_adj_uniq.shape)\n",
    "\n",
    "# SIMS_adj = SIMS_adj_uniq[coord_id_S]\n",
    "# print(SIMS_adj.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected holding time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_ht_new(sim_T, sim_dp):\n",
    "    \"\"\"calculate holding time for each trajectory\n",
    "    \"\"\"\n",
    "    sim_HT = np.array([])\n",
    "    idx = np.where(sim_T==0)[0]\n",
    "    \n",
    "    for i in range(len(idx)):\n",
    "        if i < len(idx)-1:\n",
    "            temp_T = sim_T[idx[i]:idx[i+1]]\n",
    "            sim_HT = np.append(sim_HT,np.concatenate([np.diff(temp_T),[0]]))\n",
    "        else:\n",
    "            temp_T = sim_T[idx[i]:]\n",
    "            sim_HT = np.append(sim_HT,np.concatenate([np.diff(temp_T),[0]]))\n",
    "    \n",
    "    # get each individual trajectory's index\n",
    "    # trj_id = np.where(SIMS_dp == SIMS_dp[-1])[0]\n",
    "    trj_id = np.where(sim_HT==0)[0]\n",
    "\n",
    "    return sim_HT, trj_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_HT, trj_id = sim_ht_new(SIMS_T, SIMS_dp)\n",
    "print(SIMS_HT.shape)\n",
    "print(trj_id.shape)\n",
    "\n",
    "# get unique holding time of unique states\n",
    "SIMS_HT_uniq = mean_holdingtime(SIMS_HT, indices_S, coord_id_S)\n",
    "print(SIMS_HT_uniq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative holding time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_cumu_HT_uniq,cumu_account_uniq = cumu_holdingtime(SIMS_HT, indices_S, coord_id_S)\n",
    "print(SIMS_cumu_HT_uniq.shape, cumu_account_uniq.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scattering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multiple trajectories\n",
    "scat_coeff_array_S = transform_dataset(SIMS_adj_uniq)\n",
    "SIMS_scar_uniq = get_normalized_moments(scat_coeff_array_S).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_scar = SIMS_scar_uniq[coord_id_S]\n",
    "SIMS_scar.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# save npz file for shortest path\n",
    "with open(f'./data/jordan/pretraining/{rxn_name}.npz', 'wb') as f:\n",
    "    np.savez(f,\n",
    "             SIMS_dp_uniq = SIMS_dp_uniq,\n",
    "             SIMS_dp_og_uniq = SIMS_dp_og_uniq,\n",
    "             SIMS_pair_uniq = SIMS_pair_uniq,\n",
    "             SIMS_G_uniq = SIMS_G_uniq,\n",
    "             SIMS_T = SIMS_T,  \n",
    "             SIMS_HT = SIMS_HT,\n",
    "             SIMS_HT_uniq = SIMS_HT_uniq,\n",
    "             SIMS_cumu_HT_uniq = SIMS_cumu_HT_uniq,\n",
    "             SIMS_adj_uniq = SIMS_adj_uniq,\n",
    "             SIMS_scar_uniq = SIMS_scar_uniq,\n",
    "             SIMS_type_uniq=SIMS_type_uniq,\n",
    "             cumu_account_uniq=cumu_account_uniq,\n",
    "             trj_id = trj_id,\n",
    "             indices_S = indices_S,\n",
    "             coord_id_S = coord_id_S,\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "fnpz_data = f\"./data/jordan/pretraining/{rxn_name}.npz\"\n",
    "data_npz = np.load(fnpz_data)\n",
    "# # asssign data to variables\n",
    "for var in data_npz.files:\n",
    "     locals()[var] = data_npz[var]\n",
    "     print(var, locals()[var].shape)\n",
    "     \n",
    "# recover full data based on coord_id, indices, and unique data\n",
    "SIMS_dp = SIMS_dp_uniq[coord_id_S]\n",
    "SIMS_dp_og = SIMS_dp_og_uniq[coord_id_S]\n",
    "SIMS_pair = SIMS_pair_uniq[coord_id_S]\n",
    "SIMS_G = SIMS_G_uniq[coord_id_S]\n",
    "SIMS_type = SIMS_type_uniq[coord_id_S]\n",
    "SIMS_cumu_HT = SIMS_cumu_HT_uniq[coord_id_S]\n",
    "cumu_account = cumu_account_uniq[coord_id_S]\n",
    "\n",
    "SIMS_adj = SIMS_adj_uniq[coord_id_S]\n",
    "SIMS_scar = SIMS_scar_uniq[coord_id_S]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct weights (expected holding time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SIMS_HT_uniq.max(), SIMS_HT_uniq[SIMS_HT_uniq!=0].min(), SIMS_HT_uniq.std(), SIMS_HT_uniq.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = coord_id_S\n",
    "# print(\"Initial node:\", all_nodes[0], \"  Final node:\", all_nodes[-1])\n",
    "print(\"Initial node: not unique\", \"  Final node:\", all_nodes[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise nodes\n",
    "# note: this step connect final->initial node,\n",
    "# which will be remove later\n",
    "all_edges_temp = []\n",
    "for previous, current in zip(all_nodes, all_nodes[1:]):\n",
    "    all_edges_temp.append((previous, current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove all edges that connect final->initial node\n",
    "# all_edges = list(filter((all_nodes[-1],all_nodes[0]).__ne__, all_edges_temp))\n",
    "\n",
    "indices_to_delete = trj_id[:-1]\n",
    "# Sort the indices in reverse order so that deleting elements won't affect subsequent indices\n",
    "indices_to_delete = sorted(indices_to_delete, reverse=True)\n",
    "\n",
    "all_edges = copy.deepcopy(all_edges_temp)\n",
    "for index in indices_to_delete:\n",
    "    del all_edges[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "if (all_nodes[-1],all_nodes[0]) in all_edges:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(f\"There are no edges from final node {all_nodes[-1]} to initial node {all_nodes[0]}\")\n",
    "    \n",
    "print(\"before prune: \", len(all_edges))\n",
    "print(\"after prune: \",len(all_edges_temp))\n",
    "print(\"difference: \", len(all_edges_temp) - len(all_edges))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct modified undirected weight graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUG = nx.Graph()\n",
    "\n",
    "for  i in range(len(all_edges)):\n",
    "    idx0 = all_edges[i][0]\n",
    "    idx1 = all_edges[i][1]\n",
    "    \n",
    "    if SIMS_HT_uniq[idx0] < SIMS_HT_uniq[idx1]:\n",
    "        weight = SIMS_HT_uniq[idx0]\n",
    "    else:\n",
    "        weight = SIMS_HT_uniq[idx1]\n",
    "        \n",
    "    if SIMS_HT_uniq[idx0] == 0 or SIMS_HT_uniq[idx1] == 0:\n",
    "        weight = SIMS_HT_uniq[idx0] + SIMS_HT_uniq[idx1]\n",
    "        \n",
    "    MUG.add_edge(int(all_edges[i][0]), int(all_edges[i][1]), weight = float(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(MUG.edges), len(MUG.nodes), MUG.get_edge_data(0,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect and save KNN=100 neighbor's shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# # create a directory to store the shortest path\n",
    "directory = f'./data/jordan/graph/{rxn_name}'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(\"Directory created successfully!\")\n",
    "else :\n",
    "    print(\"Directory already exists\")\n",
    "\n",
    "# # collect the shortest path for each node\n",
    "knn = 100\n",
    "\n",
    "for i in range(len(SIMS_HT_uniq)):\n",
    "    length = nx.single_source_dijkstra_path_length(MUG, i)\n",
    "    length_arr = np.array(list(length.items()), dtype=object)\n",
    "    with open(f'./data/jordan/graph/{rxn_name}/{i}.npz', 'wb') as f:\n",
    "        np.savez(f,\n",
    "                 X_j = length_arr[1:knn+1,0],\n",
    "                 D_ij = length_arr[1:knn+1,1],\n",
    "             )\n",
    "    if i % 5000 == 0:\n",
    "        print(i)\n",
    "\n",
    "# # load saved distance matrix\n",
    "X_j = []\n",
    "D_ij = []\n",
    "for i in range(len(SIMS_HT_uniq)):\n",
    "    fpath = f'./data/jordan/graph/{rxn_name}/{i}.npz'\n",
    "    X_j.append(np.load(fpath,allow_pickle=True)[\"X_j\"])\n",
    "    D_ij.append(np.load(fpath,allow_pickle=True)[\"D_ij\"])\n",
    "\n",
    "# # padd the X_j and D_ij to the same length\n",
    "for i in range(len(X_j)):\n",
    "    if len(X_j[i]) < knn:\n",
    "        X_j[i] = np.pad(X_j[i], (0, knn-len(X_j[i])), 'constant', constant_values=i)\n",
    "        D_ij[i] = np.pad(D_ij[i], (0, knn-len(D_ij[i])), 'constant', constant_values=0)\n",
    "\n",
    "# # convert to numpy array\n",
    "X_j = np.array(X_j, dtype=int)\n",
    "D_ij = np.array(D_ij, dtype=float)\n",
    "\n",
    "# # save npz file for shortest path\n",
    "with open(f'./data/jordan/graph/{rxn_name}_shortestpath_knn={knn}.npz', 'wb') as f:\n",
    "    np.savez(f,\n",
    "             X_j = X_j,\n",
    "             D_ij = D_ij,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved graph info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j = np.array(np.load(f'./data/jordan/graph/{rxn_name}_shortestpath_knn=100.npz',allow_pickle=True)[\"X_j\"], dtype=int)\n",
    "D_ij = np.array(np.load(f'./data/jordan/graph/{rxn_name}_shortestpath_knn=100.npz',allow_pickle=True)[\"D_ij\"], dtype=float)\n",
    "\n",
    "X_j.shape, D_ij.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,3))  # range [1,3]\n",
    "# scaler = MinMaxScaler(feature_range=(1,10))  # range [1,10]\n",
    "\n",
    "norm_Dij = scaler.fit_transform(D_ij) \n",
    "print(norm_Dij.max(), norm_Dij.min(), norm_Dij.mean(), norm_Dij.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the importance weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability of being visited during a simulated trajectory \n",
    "# from the initial state\n",
    "split_id = trj_id + 1 # index for split to each trajectory\n",
    "P_tot = np.zeros(len(SIMS_dp_uniq))\n",
    "\n",
    "for i in range(len(split_id)):\n",
    "    if i == 0:\n",
    "        trj = set(coord_id_S[0:split_id[i]])\n",
    "    else:\n",
    "        trj = set(coord_id_S[split_id[i-1]:split_id[i]])\n",
    "\n",
    "    P_tot[list(trj)] += 1\n",
    "\n",
    "P_tot = P_tot / 100\n",
    "\n",
    "P_tot.shape, P_tot.max(), P_tot.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "def edit_distance_old(str1, str2):\n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "    \n",
    "    # Initialize a 2D array to store edit distances\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Initialize the first row and column\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Calculate edit distance using dynamic programming\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,      # Deletion\n",
    "                dp[i][j - 1] + 1,      # Insertion\n",
    "                dp[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "    \n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(adj1, adj2):\n",
    "    # Calculate edit distance based on its adjacency matrix\n",
    "    edit_dist = np.sum(np.abs(adj1-adj2),dtype=int)\n",
    "    \n",
    "    return edit_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the edit distance between X_i and X_j'\n",
    "ED_ij = []\n",
    "for i in range(X_j.shape[0]):\n",
    "    ed_ij = []\n",
    "    for j in range(X_j.shape[1]):\n",
    "        ed_ij.append(edit_distance(SIMS_adj_uniq[i], SIMS_adj_uniq[X_j[i,j]]))\n",
    "    ED_ij.append(ed_ij)\n",
    "ED_ij = np.array(ED_ij, dtype=int)\n",
    "\n",
    "with open(f'./data/jordan/graph/{rxn_name}_edit_distance.npz', 'wb') as f:\n",
    "    np.savez(f,\n",
    "             ED_ij = ED_ij,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ED_ij.max(), ED_ij.min(), ED_ij.mean(), ED_ij.std(), ED_ij.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ED_ij = np.array(np.load(f'./data/jordan/graph/{rxn_name}_edit_distance.npz',allow_pickle=True)[\"ED_ij\"], dtype=float)\n",
    "print(ED_ij.max(), ED_ij.min(), ED_ij.mean(), ED_ij.std(), ED_ij.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViDa Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tup = (torch.Tensor(SIMS_scar_uniq),\n",
    "            torch.Tensor(SIMS_G_uniq),\n",
    "            torch.arange(len(SIMS_scar_uniq)))\n",
    "data_dataset = torch.utils.data.TensorDataset(*data_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and validation\n",
    "data_size = len(data_dataset)\n",
    "train_size = int(0.7 * data_size)\n",
    "val_size = data_size - train_size\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(data_dataset, [train_size, val_size], \n",
    "                                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(data_size, len(train_data), len(val_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters\n",
    "input_dim = data_tup[0].shape[-1]\n",
    "\n",
    "config = Namespace(\n",
    "    reaction = rxn_name,\n",
    "    select_trajs_id = select_trajs_id,\n",
    "    \n",
    "    type = 'vida',\n",
    "    knn = 100,\n",
    "    edit_distance = \"100_neighbors\",\n",
    "    device = 'mps', # change to cuda if using Nvida GPU\n",
    "    log_dir = f'{time.strftime(\"%m%d-%H%M\")}', # log directory\n",
    "    batch_size = 256,\n",
    "    input_dim = input_dim,\n",
    "    output_dim = input_dim,\n",
    "    latent_dim = 25, # bottleneck dimension\n",
    "    hidden_dim = 400,\n",
    "    n_epochs = 60,\n",
    "    # learning_rate = 1e-4, # learning rate\n",
    "    learning_rate = 5e-5, # learning rate\n",
    "    \n",
    "    log_interval = 10, # how many batches to wait before logging training status    \n",
    "    # patience = 5, # how many epochs to wait before early stopping #3\n",
    "    patience = 10, # how many epochs to wait before early stopping #3\n",
    "    \n",
    "    \n",
    "    # hyperparameters for loss function\n",
    "    alpha = 1, # reconstruction loss\n",
    "    \n",
    "    beta = 1e-4, # kl divergence\n",
    "    \n",
    "    gamma = 0.3, # energy loss\n",
    "    \n",
    "    # delta = 0.04, # distance loss\n",
    "    delta = 0.0004, # distance loss\n",
    "    # delta = 1e-4, # distance loss\n",
    "    \n",
    "    # delta = 0, # distance loss\n",
    "    \n",
    "    epsilon = 1e-4, # edit distance loss\n",
    "    # epsilon = 5e-5, # edit distance loss\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make datalaoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(data_dataset, batch_size=config.batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=config.batch_size, \n",
    "                                           shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=config.batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_loader: ', len(data_loader.dataset), len(data_loader), data_loader.batch_size)\n",
    "print('train_loader: ', len(train_loader.dataset), len(train_loader), train_loader.batch_size)\n",
    "print('val_loader: ', len(val_loader.dataset), len(val_loader), val_loader.batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        '''\n",
    "        Args:\n",
    "        ----\n",
    "            - input_dim: the dimension of the input node feature\n",
    "            - hiddent_dim: the dimension of the hidden layer\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        \n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution, note how we only output\n",
    "        # diagonal values of covariance matrix. Here we assume\n",
    "        # they are conditionally independent\n",
    "        self.hid2mu = nn.Linear(400, self.latent_dim)\n",
    "        self.hid2logvar = nn.Linear(400, self.latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.fc1(x)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        mu = self.hid2mu(x)\n",
    "        logvar = self.hid2logvar(x)\n",
    "        return mu, logvar\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        '''\n",
    "        Args:\n",
    "        ----\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "            - hiddent_dim: the dimension of the hidden layer\n",
    "            - output_dim: the dimension of the output node feature\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.latent_dim, 400)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        self.fc3 = nn.Linear(400, self.output_dim)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.bn1(F.relu(self.fc1(z)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        '''\n",
    "        The regressor is used to predict the energy of the node\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "        '''\n",
    "        super(Regressor, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.regfc1 = nn.Linear(self.latent_dim, 15)\n",
    "        self.regfc2 = nn.Linear(15, 1)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        y = F.relu(self.regfc1(z))\n",
    "        y = self.regfc2(y)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIDA(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, regressor):\n",
    "        '''\n",
    "        Args:\n",
    "        ----\n",
    "            - input_dim: the dimension of the input node feature\n",
    "            - hiddent_dim: the dimension of the hidden layer\n",
    "            - latent_dim: the dimension of the latent space (bottleneck layer)\n",
    "            - output_dim: the dimension of the output node feature (same as input_dim)\n",
    "        '''\n",
    "        super(VIDA, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.regressor = regressor\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps*std\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        y_pred = self.regressor(z)\n",
    "        return x_recon, y_pred, z, mu, logvar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_recon, x, mu, logvar):\n",
    "    '''\n",
    "    Compute the VAE loss\n",
    "    \n",
    "    Args:\n",
    "        - x_recon: the reconstructed node feature\n",
    "        - x: the original node feature\n",
    "        - mu: the mean of the latent space\n",
    "        - logvar: the log variance of the latent space\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing (scalar) the loss for the VAE\n",
    "    '''\n",
    "    BCE = F.mse_loss(x_recon.flatten(), x.flatten()) # L2 loss\n",
    "    # BCE = F.l1_loss(x_recon.flatten(), x.flatten()) # L1 loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE, KLD\n",
    "\n",
    "\n",
    "def pred_loss(y_pred, y):\n",
    "    '''\n",
    "    Compute the energy prediction loss\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        - y_pred: the predicted energy of the node\n",
    "        - y: the true energy of the node\n",
    "    \n",
    "    Returns:\n",
    "        - loss: PyTorch Tensor containing (scalar) the loss for the prediction\n",
    "    '''\n",
    "    return F.mse_loss(y_pred.flatten(), y.flatten())\n",
    "\n",
    "\n",
    "def distance_knn_loss_torch(config, zi, zj, Dij, P_tot, idx, batchXj_id):\n",
    "    '''\n",
    "    Compute the distance loss between embeddings \n",
    "    and the minimum expected holding time\n",
    "    \n",
    "    Args:\n",
    "        - zi: the embedding of the node i\n",
    "        - zj: the embedding of the nodes j's\n",
    "        - Dij: the post-processing distance between nodes i and j's\n",
    "        - P_tot: the total probability of the nodes i and j's\n",
    "        - idx: the index of the node i\n",
    "        - batchXj_id: the index of the nodes j's\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing (scalar) the loss for the embedding distance\n",
    "    '''\n",
    "    zi = zi.reshape(-1,1,zi.shape[-1])\n",
    "    l2_zizj = torch.sqrt(torch.sum((zi-zj)**2, dim=-1))\n",
    "    dist_diff = (l2_zizj - (Dij[idx]).to(config.device))**2\n",
    "\n",
    "    wij = (P_tot[idx].reshape(-1,1) * P_tot[batchXj_id]).to(config.device) # importance weight of nodes i and j       \n",
    "    dist_loss = torch.sum(dist_diff * wij)\n",
    "    return dist_loss\n",
    "\n",
    "def edit_distance_loss(config, zi, zj, ED_ij, idx):\n",
    "    '''\n",
    "    Compute the edit distance loss between embeddings \n",
    "    \n",
    "    Args:\n",
    "        - zi: the embedding of the node i\n",
    "        - zj: the embedding of the nodes j's\n",
    "        - ED_ij: the post-processing distance between nodes i and j's\n",
    "        - idx: the index of the node i\n",
    "        - batchXj_id: the index of the nodes j's\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing (scalar) the loss for the embedding distance\n",
    "    '''\n",
    "    zi = zi.reshape(-1,1,zi.shape[-1])\n",
    "    l2_zizj = torch.sqrt(torch.sum((zi-zj)**2, dim=-1))\n",
    "    dist_diff = (l2_zizj - (ED_ij[idx]).to(config.device))**2\n",
    "    # wij = (P_tot[idx].reshape(-1,1) * P_tot[batchXj_id]).to(config.device) # importance weight of nodes i and j       \n",
    "    # dist_loss = torch.sum(dist_diff * wij)\n",
    "    editdist_loss = torch.sum(dist_diff)\n",
    "    return editdist_loss\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early_stopping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop(val_loss, epoch, patience):\n",
    "    \"\"\"\n",
    "    Check if validation loss has not improved for a certain number of epochs\n",
    "    \n",
    "    Args:\n",
    "        val_loss (float): current validation loss\n",
    "        epoch (int): current epoch number\n",
    "        patience (int): number of epochs to wait before stopping if validation loss does not improve\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if validation loss has not improved for the last `patience` epochs, False otherwise\n",
    "    \"\"\"\n",
    "    if epoch == 0:\n",
    "        # First epoch, don't stop yet\n",
    "        return False\n",
    "    else:\n",
    "        # Check if validation loss has not improved for `patience` epochs\n",
    "        if val_loss >= early_stop.best_loss:\n",
    "            early_stop.num_epochs_without_improvement += 1\n",
    "            if early_stop.num_epochs_without_improvement >= patience:\n",
    "                print(\"Stopping early\")\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            # Validation loss improved, reset counter\n",
    "            early_stop.best_loss = val_loss\n",
    "            early_stop.num_epochs_without_improvement = 0\n",
    "            return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function\n",
    "def validate(config, model, data_loader, val_loader, P_tot, Dij, ED_ij, X_j,\n",
    "             vae_loss, pred_loss, distance_knn_loss, edit_distance_loss):\n",
    "    model.to(config.device)\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0; val_bce = 0; val_kld = 0; val_pred = 0; val_dist = 0; val_edit = 0\n",
    "    \n",
    "    # Disable gradient calculation to speed up inference\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_loader:\n",
    "            \n",
    "            # Configure input\n",
    "            x = x.to(config.device)\n",
    "            y = y.to(config.device)\n",
    "            \n",
    "            # forward X_j\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                batchXj_id = X_j[idx]\n",
    "                neighbor_input = data_loader.dataset.tensors[0][batchXj_id].reshape(-1, config.input_dim).to(config.device)\n",
    "                _, _, neighbor_embed, _, _ = model(neighbor_input)\n",
    "                neighbor_embed = neighbor_embed.reshape(-1, config.knn, neighbor_embed.shape[-1])\n",
    "                \n",
    "            # embedding\n",
    "            x_recon, y_pred, z, mu, logvar = model(x)\n",
    "            \n",
    "            # # compute the total loss\n",
    "            # vae loss\n",
    "            recon_loss, kl_loss = vae_loss(x_recon, x, mu, logvar)\n",
    "            recon_loss = recon_loss.item()\n",
    "            kl_loss = kl_loss.item()\n",
    "            \n",
    "            # energy prediction loss\n",
    "            p_loss = pred_loss(y_pred, y).item()\n",
    "                \n",
    "            # distance loss\n",
    "            dist_loss = distance_knn_loss(config, z, neighbor_embed, Dij, P_tot, idx, batchXj_id).item()\n",
    "            \n",
    "            # edit distance loss\n",
    "            edit_loss = edit_distance_loss(config, z, neighbor_embed, ED_ij, idx).item()\n",
    "            \n",
    "            # scaling the loss\n",
    "            recon_loss = config.alpha * recon_loss\n",
    "            kl_loss = config.beta * kl_loss\n",
    "            p_loss = config.gamma * p_loss\n",
    "            dist_loss = config.delta * dist_loss\n",
    "            edit_loss = config.epsilon * edit_loss\n",
    "            \n",
    "            # total loss\n",
    "            loss = recon_loss + kl_loss + p_loss + dist_loss + edit_loss\n",
    "            \n",
    "            val_loss += loss\n",
    "            val_bce += recon_loss\n",
    "            val_kld += kl_loss\n",
    "            val_pred += p_loss\n",
    "            val_dist += dist_loss\n",
    "            val_edit += edit_loss\n",
    "                        \n",
    "    print('Validation Loss: {:.4f}'.format(val_loss/len(val_loader.dataset)))\n",
    "    \n",
    "    # Clear the cache\n",
    "    torch.cuda.empty_cache()\n",
    "    # torch.mps.empty_cache()\n",
    "    \n",
    "    return val_loss/len(val_loader.dataset), val_bce/len(val_loader.dataset), val_kld/len(val_loader.dataset), val_pred/len(val_loader.dataset), val_dist/len(val_loader.dataset), val_edit/len(val_loader.dataset)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, data_loader, train_loader, val_loader, P_tot, Dij, ED_ij, X_j,\n",
    "          optimizer, vae_loss, pred_loss, distance_knn_loss, edit_distance_loss, early_stop,):\n",
    "    '''\n",
    "    Train VIDA!\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        - config: Experiment configurations\n",
    "        - model: Pytorch VIDA model\n",
    "        - data_loader: Pytorch DataLoader for all data\n",
    "        - train_loader: Pytorch DataLoader for training set\n",
    "        - val_loader: Pytorch DataLoader for validation set\n",
    "        - P_tot: the total probability of each node\n",
    "        - Dij: the shortest path distance between k nearest pairs of nodes\n",
    "        - X_j: the index of k nearest neighbours of each node\n",
    "        - optimizer: Pytorch optimizer\n",
    "        - vae_loss: the VAE loss function\n",
    "        - pred_loss: the energy prediction loss function\n",
    "        - distant_knn_loss: the k nearest neighbour distance loss function\n",
    "    '''\n",
    "    \n",
    "    model.to(config.device)\n",
    "    \n",
    "    log_dir = f'./model_config/{config.log_dir}'\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    # save the config file\n",
    "    with open(f'{log_dir}/hparams.yaml','w') as f:\n",
    "        yaml.dump(config,f)\n",
    "    \n",
    "    # Initialize early stop object\n",
    "    early_stop.best_loss = np.Inf\n",
    "    early_stop.num_epochs_without_improvement = 0\n",
    "\n",
    "    # convert the numpy array to tensor\n",
    "    P_tot = torch.from_numpy(P_tot.astype(np.float32))\n",
    "    Dij = torch.from_numpy(Dij.astype(np.float32))\n",
    "    ED_ij = torch.from_numpy(ED_ij.astype(int))\n",
    "    X_j = torch.from_numpy(X_j)\n",
    "    \n",
    "    print('\\n ------- Start Training -------')\n",
    "    for epoch in range(config.n_epochs):\n",
    "        start_time = time.time()\n",
    "        training_loss = 0\n",
    "        \n",
    "        for batch_idx, (x, y, idx) in enumerate(train_loader):  # mini batch\n",
    "            \n",
    "            # Configure input\n",
    "            x = x.to(config.device)\n",
    "            y = y.to(config.device)\n",
    "            \n",
    "            # forward X_j\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                batchXj_id = X_j[idx]\n",
    "                neighbor_input = data_loader.dataset.tensors[0][batchXj_id].reshape(-1, config.input_dim).to(config.device)\n",
    "                _, _, neighbor_embed, _, _ = model(neighbor_input)\n",
    "                neighbor_embed = neighbor_embed.reshape(-1, config.knn, neighbor_embed.shape[-1])\n",
    "                \n",
    "            # ------------------------------------------\n",
    "            #  Train VIDA\n",
    "            # ------------------------------------------\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get the reconstructed nodes, predicted energy, and the embeddings\n",
    "            x_recon, y_pred, z, mu, logvar = model(x)\n",
    "        \n",
    "            ## compute the total loss\n",
    "            # vae loss\n",
    "            recon_loss, kl_loss = vae_loss(x_recon, x, mu, logvar)\n",
    "            \n",
    "            # energy prediction loss\n",
    "            p_loss = pred_loss(y_pred, y)\n",
    "\n",
    "            # distance loss\n",
    "            dist_loss = distance_knn_loss(config, z, neighbor_embed, Dij, P_tot, idx, batchXj_id)\n",
    "\n",
    "            # edit distance loss\n",
    "            edit_loss = edit_distance_loss(config, z, neighbor_embed, ED_ij, idx)\n",
    "            \n",
    "            # scaling the loss\n",
    "            recon_loss = config.alpha * recon_loss\n",
    "            kl_loss = config.beta * kl_loss\n",
    "            p_loss = config.gamma * p_loss\n",
    "            dist_loss = config.delta * dist_loss\n",
    "            edit_loss = config.epsilon * edit_loss\n",
    "            \n",
    "            # total loss\n",
    "            loss = recon_loss + kl_loss + p_loss + dist_loss + edit_loss\n",
    "            \n",
    "            # backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item()\n",
    "            \n",
    "            # ------------------------------------------\n",
    "            # Log Progress\n",
    "            # ------------------------------------------\n",
    "            if batch_idx % config.log_interval == 0:\n",
    "                print('Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, config.n_epochs, batch_idx * len(x), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item()))\n",
    "                \n",
    "                writer.add_scalar('training loss',\n",
    "                                  loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('recon loss',\n",
    "                                  recon_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('kl loss',\n",
    "                                  kl_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('pred loss',\n",
    "                                  p_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('dist loss',\n",
    "                                  dist_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('edit loss',\n",
    "                                  edit_loss.item(),\n",
    "                                  epoch * len(train_loader) + batch_idx)\n",
    "    \n",
    "        print ('====> Epoch: {} Average loss: {:.4f}'.format(epoch, training_loss/len(train_loader.dataset)))\n",
    "        writer.add_scalar('epoch training loss', training_loss/len(train_loader.dataset), epoch)\n",
    "        \n",
    "        # validation\n",
    "        val_loss, val_bce, val_kld, val_pred, val_dist, val_edit = validate(config, model, data_loader, val_loader, P_tot, Dij, ED_ij, X_j, \n",
    "                                                                  vae_loss, pred_loss, distance_knn_loss, edit_distance_loss)\n",
    "        writer.add_scalar('validation loss', val_loss, epoch)\n",
    "        writer.add_scalar('val_recon loss', val_bce, epoch)\n",
    "        writer.add_scalar('val_kl loss', val_kld, epoch)\n",
    "        writer.add_scalar('val_pred loss', val_pred, epoch)\n",
    "        writer.add_scalar('val_dist loss', val_dist, epoch)\n",
    "        writer.add_scalar('val_edit loss', val_edit, epoch)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        print (f'Epoch {epoch} train+val time: {epoch_time:.2f} seconds \\n')\n",
    "        \n",
    "        # Check if validation loss has not improved for `patience` epochs\n",
    "        if early_stop(val_loss, epoch, config.patience):\n",
    "            break\n",
    "    \n",
    "        # Clear the cache\n",
    "        # torch.mps.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "    writer.close()\n",
    "    print('\\n ------- Finished Training -------')\n",
    "    \n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), f'{log_dir}/model.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "encoder = Encoder(input_dim=config.input_dim, hidden_dim=config.hidden_dim, latent_dim=config.latent_dim)\n",
    "decoder = Decoder(latent_dim=config.latent_dim, hidden_dim=config.hidden_dim, output_dim=config.output_dim)\n",
    "regressor = Regressor(latent_dim=config.latent_dim)\n",
    "\n",
    "vida = VIDA(encoder, decoder, regressor)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(vida.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train VIDA\n",
    "train(config, vida, data_loader, train_loader, val_loader, P_tot, norm_Dij, ED_ij, X_j,\n",
    "      optimizer, vae_loss, pred_loss, distance_knn_loss_torch, edit_distance_loss, early_stop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VIDA(encoder, decoder, regressor)\n",
    "# model.load_state_dict(torch.load('./model_config/0805-0325/model.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VIDA(encoder, decoder, regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do inference\n",
    "model.to(config.device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "        _, _, z, _, _ = model(data_loader.dataset.tensors[0].to(config.device))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed = z.to('cpu').numpy()\n",
    "data_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_embed.max(), data_embed.min(), data_embed.mean(), data_embed.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do PCA for GSAE embeded data\n",
    "pca_coords = PCA(n_components=2).fit_transform(data_embed)\n",
    "\n",
    "pca_coords.shape, (np.unique(pca_coords,axis=0)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_embed = scaler.fit_transform(data_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do PHATE for GSAE embeded data\n",
    "phate_operator = phate.PHATE(n_jobs=-2)\n",
    "phate_coords = phate_operator.fit_transform(data_embed)\n",
    "\n",
    "phate_coords.shape, (np.unique(phate_coords,axis=0)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "\"\"\" Save all DRs\n",
    "\"\"\"\n",
    "# save for python\n",
    "fnpz_data = f\"data/jordan/plotdata/plot_{config.log_dir}_{rxn_name}.npz\"\n",
    "with open(fnpz_data, 'wb') as f:\n",
    "    np.savez(f,\n",
    "            data_embed=data_embed,\n",
    "            # plotting data\n",
    "            pca_coords=pca_coords,\n",
    "            phate_coords=phate_coords,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load plot data\n",
    "# fnpz_data = f\"data/jordan/plotdata/plot_{config.log_dir}_{rxn_name}.npz\"\n",
    "fnpz_data = f\"data/jordan/plotdata/plot_0825-2043_{rxn_name}.npz\"\n",
    "\n",
    "data_npz = np.load(fnpz_data)\n",
    "\n",
    "# asssign data to variables\n",
    "for var in data_npz.files:\n",
    "    locals()[var] = data_npz[var]\n",
    "    print(var, locals()[var].shape)\n",
    "\n",
    "pca_all_coords = pca_coords[coord_id_S]\n",
    "phate_all_coords = phate_coords[coord_id_S]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(SIMS_cumu_HT[SIMS_cumu_HT!=0]), SIMS_cumu_HT.max())\n",
    "print(np.min(cumu_account), cumu_account.max())\n",
    "\n",
    "pca_all_coords = pca_coords[coord_id_S]\n",
    "phate_all_coords = phate_coords[coord_id_S]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assign the state with time=0 to the minimum value\n",
    "SIMS_cumu_HT_temp = copy.deepcopy(SIMS_cumu_HT)\n",
    "SIMS_cumu_HT_temp[np.where(SIMS_cumu_HT==0)[0]] = np.min(SIMS_cumu_HT[SIMS_cumu_HT!=0])\n",
    "# # log transform the data\n",
    "exponent = np.ceil(np.log10(1) - np.log10(np.min(SIMS_cumu_HT_temp))).astype(int)\n",
    "SIMS_cumu_HT_log = np.log(SIMS_cumu_HT_temp * 10**exponent)\n",
    "\n",
    "# # add 1 to the min(cumu_account_uniq) to make sure log transform doesn't have 0\n",
    "cumu_account_temp = copy.deepcopy(cumu_account)\n",
    "cumu_account_temp[np.where(cumu_account==1)[0]] = 2\n",
    "# # log transform the data\n",
    "cumu_account_log = np.log(cumu_account_temp)\n",
    "\n",
    "print((SIMS_cumu_HT_log).min(), SIMS_cumu_HT_log.max())\n",
    "print((cumu_account_log).min(), cumu_account_log.max())\n",
    "\n",
    "SIMS_cumu_HT_uniq_log = SIMS_cumu_HT_log[indices_S]\n",
    "cumu_account_uniq_log = cumu_account_log[indices_S]\n",
    "\n",
    "SIMS_cumu_HT_uniq_log.shape, cumu_account_uniq_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each trajectory\n",
    "\n",
    "# List of arrays to split\n",
    "arrays_to_split = [SIMS_dp_og, SIMS_T, SIMS_HT, SIMS_G, SIMS_pair, SIMS_type, SIMS_cumu_HT, cumu_account, SIMS_cumu_HT_log, cumu_account_log, pca_all_coords, phate_all_coords]\n",
    "\n",
    "# Get each trajectory using a single loop\n",
    "subtrj_id = (trj_id+1)[:-1]\n",
    "sub_arrays = [np.split(arr, subtrj_id) for arr in arrays_to_split]\n",
    "\n",
    "# Use zip to unpack the sub-arrays into separate variables if needed\n",
    "sub_SIMS_dp_og, sub_SIMS_T, sub_SIMS_HT, sub_SIMS_G, sub_SIMS_pair, sub_SIMS_type, sub_SIMS_cumu_HT, sub_cumu_account, sub_SIMS_cumu_HT_log, sub_cumu_account_log, sub_pca_all_coords, sub_phate_all_coords = sub_arrays\n",
    "\n",
    "# Assert the lengths to ensure correctness\n",
    "assert len(sub_SIMS_T) == len(sub_SIMS_G) == len(sub_SIMS_dp_og) == len(sub_SIMS_HT) == len(sub_SIMS_pair) == len(sub_SIMS_type) == len(sub_SIMS_cumu_HT) == len(sub_cumu_account) == len(sub_SIMS_cumu_HT_log) == len(sub_SIMS_cumu_HT_log) == len(sub_pca_all_coords) == len(sub_phate_all_coords)\n",
    "\n",
    "# Print the length of sub_SIMS_T for verification\n",
    "print(len(sub_SIMS_T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # separate the successful and failed trajectories\n",
    "\n",
    "# List of arrays to extract successful trajectories from\n",
    "arrays_to_extract = [sub_SIMS_dp_og, sub_SIMS_T, sub_SIMS_HT, sub_SIMS_G, sub_SIMS_pair, sub_SIMS_type, sub_SIMS_cumu_HT, sub_cumu_account, sub_SIMS_cumu_HT_log, sub_cumu_account_log, sub_pca_all_coords, sub_phate_all_coords]\n",
    "\n",
    "succ_id = select_trajs_id\n",
    "# Extract successful trajectories for each array using a single loop and list comprehension\n",
    "succ_sub_arrays = [[arr[i] for i in succ_id] for arr in arrays_to_extract]\n",
    "# Unpack the extracted successful trajectories into separate variables\n",
    "succ_sub_SIMS_dp_og, succ_sub_SIMS_T, succ_sub_SIMS_HT, succ_sub_SIMS_G, succ_sub_SIMS_pair, succ_sub_SIMS_type, succ_sub_SIMS_cumu_HT, succ_sub_cumu_account, succ_sub_SIMS_cumu_HT_log, succ_sub_cumu_account_log, succ_sub_pca_all_coords, succ_sub_phate_all_coords = succ_sub_arrays\n",
    "\n",
    "fail_id = np.setdiff1d(np.arange(len(sub_SIMS_T)), succ_id)\n",
    "# Extract failed trajectories for each array using a single loop and list comprehension\n",
    "fail_sub_arrays = [[arr[i] for i in fail_id] for arr in arrays_to_extract]\n",
    "# Unpack the extracted failed trajectories into separate variables\n",
    "fail_sub_SIMS_dp_og, fail_sub_SIMS_T, fail_sub_SIMS_HT, fail_sub_SIMS_G, fail_sub_SIMS_pair, fail_sub_SIMS_type, fail_sub_SIMS_cumu_HT, fail_sub_cumu_account, fail_sub_SIMS_cumu_HT_log, fail_sub_cumu_account_log, fail_sub_pca_all_coords, fail_sub_phate_all_coords = fail_sub_arrays\n",
    "\n",
    "assert len(succ_sub_SIMS_dp_og) +  len(fail_sub_SIMS_dp_og) == len(sub_SIMS_dp_og)\n",
    "print(len(succ_sub_SIMS_dp_og), len(fail_sub_SIMS_dp_og))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the successful and fail trajectories by reaction time\n",
    "sorted_succ_indices = np.argsort([sub_array[-1] for sub_array in succ_sub_SIMS_T])[::-1]\n",
    "sorted_fail_indices = np.argsort([sub_array[-1] for sub_array in fail_sub_SIMS_T])[::-1]\n",
    "\n",
    "# List of arrays to sort in descending order based on sorted_succ_indices or sorted_fail_indices\n",
    "succ_arrays_to_sort = [succ_sub_SIMS_dp_og, succ_sub_SIMS_T, succ_sub_SIMS_HT, succ_sub_SIMS_G, succ_sub_SIMS_pair, succ_sub_SIMS_type, succ_sub_SIMS_cumu_HT, succ_sub_cumu_account, succ_sub_SIMS_cumu_HT_log, succ_sub_cumu_account_log, succ_sub_pca_all_coords, succ_sub_phate_all_coords]\n",
    "fail_arrays_to_sort = [fail_sub_SIMS_dp_og, fail_sub_SIMS_T, fail_sub_SIMS_HT, fail_sub_SIMS_G, fail_sub_SIMS_pair, fail_sub_SIMS_type, fail_sub_SIMS_cumu_HT, fail_sub_cumu_account, fail_sub_SIMS_cumu_HT_log, fail_sub_cumu_account_log, fail_sub_pca_all_coords, fail_sub_phate_all_coords]\n",
    "\n",
    "# Use list comprehension and zip to sort all arrays simultaneously\n",
    "sorted_succ_arrays = [np.array(arr,dtype=object)[sorted_succ_indices] for arr in succ_arrays_to_sort] \n",
    "sorted_fail_arrays = [np.array(arr,dtype=object)[sorted_fail_indices] for arr in fail_arrays_to_sort]\n",
    "\n",
    "# Unpack the sorted arrays into separate variables\n",
    "sorted_succ_sub_SIMS_dp_og, sorted_succ_sub_SIMS_T, sorted_succ_sub_SIMS_HT, sorted_succ_sub_SIMS_G, sorted_succ_sub_SIMS_pair, sorted_succ_sub_SIMS_type, sorted_succ_sub_SIMS_cumu_HT, sorted_succ_sub_cumu_account, sorted_succ_sub_SIMS_cumu_HT_log, sorted_succ_sub_cumu_account_log, sorted_succ_sub_pca_all_coords, sorted_succ_sub_phate_all_coords = sorted_succ_arrays\n",
    "sorted_fail_sub_SIMS_dp_og, sorted_fail_sub_SIMS_T, sorted_fail_sub_SIMS_HT, sorted_fail_sub_SIMS_G, sorted_fail_sub_SIMS_pair, sorted_fail_sub_SIMS_type, sorted_fail_sub_SIMS_cumu_HT, sorted_fail_sub_cumu_account, sorted_fail_sub_SIMS_cumu_HT_log, sorted_fail_sub_cumu_account_log, sorted_fail_sub_pca_all_coords, sorted_fail_sub_phate_all_coords = sorted_fail_arrays\n",
    "\n",
    "sorted_succ_sub_SIMS_dp_og.shape, sorted_fail_sub_SIMS_dp_og.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for traj 42\n",
    "# print(sorted_succ_sub_SIMS_HT[0][sorted_succ_sub_SIMS_type[0]==\"SMH\"].sum())\n",
    "# print(sorted_succ_sub_SIMS_HT[0][sorted_succ_sub_SIMS_type[0]==\"0MH\"].sum())\n",
    "# print(sorted_succ_sub_SIMS_HT[0][sorted_succ_sub_SIMS_type[0]==\"SM0\"].sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe for plotting   \n",
    "df = pd.DataFrame(data={\n",
    "            \"Energy\": SIMS_G_uniq, \"Pair\": SIMS_pair_uniq, \"DP\": SIMS_dp_og_uniq, \n",
    "            \"HT\": SIMS_HT_uniq, \"Type\": SIMS_type_uniq,\n",
    "            \"HT_cumu\": SIMS_cumu_HT_uniq, \"Account_cumu\": cumu_account_uniq,\n",
    "            \"HT_cumu_log\": SIMS_cumu_HT_uniq_log, \"Account_cumu_log\": cumu_account_uniq_log,\n",
    "            \"PCA 1\": pca_coords[:,0], \"PCA 2\": pca_coords[:,1],\n",
    "            \"PHATE 1\": phate_coords[:,0], \"PHATE 2\": phate_coords[:,1]\n",
    "            }\n",
    "            )\n",
    "\n",
    "dfsucc = pd.DataFrame(data={\n",
    "        \"Energy\": sorted_succ_sub_SIMS_G, \"Pair\": sorted_succ_sub_SIMS_pair,\"DP\": sorted_succ_sub_SIMS_dp_og,\n",
    "        \"HT\": sorted_succ_sub_SIMS_HT, \"TotalT\": sorted_succ_sub_SIMS_T,\n",
    "        \"HT_cumu\": sorted_succ_sub_SIMS_cumu_HT, \"Account_cumu\": sorted_succ_sub_cumu_account,\n",
    "        \"HT_cumu_log\": sorted_succ_sub_SIMS_cumu_HT_log, \"Account_cumu_log\": sorted_succ_sub_cumu_account_log,\n",
    "        \"PCA\": sorted_succ_sub_pca_all_coords, \"PHATE\": sorted_succ_sub_phate_all_coords,\n",
    "        \"IDX\": sorted_succ_indices, \"Type\": sorted_succ_sub_SIMS_type\n",
    "        }\n",
    "        )\n",
    "\n",
    "dffail = pd.DataFrame(data={\n",
    "        \"Energy\": sorted_fail_sub_SIMS_G, \"Pair\": sorted_fail_sub_SIMS_pair,\"DP\": sorted_fail_sub_SIMS_dp_og,\n",
    "        \"HT\": sorted_fail_sub_SIMS_HT, \"TotalT\": sorted_fail_sub_SIMS_T,\n",
    "        \"HT_cumu\": sorted_fail_sub_SIMS_cumu_HT, \"Account_cumu\": sorted_fail_sub_cumu_account,\n",
    "        \"HT_cumu_log\": sorted_fail_sub_SIMS_cumu_HT_log, \"Account_cumu_log\": sorted_fail_sub_cumu_account_log,\n",
    "        \"PCA\": sorted_fail_sub_pca_all_coords, \"PHATE\": sorted_fail_sub_phate_all_coords,\n",
    "        \"IDX\": sorted_fail_indices, \"Type\": sorted_fail_sub_SIMS_type\n",
    "        }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# plot 2D energy landscape\n",
    "###############################################################################\n",
    "\n",
    "def interactive_plotly_2D(df,dfsucc,dffail,rxn_name,vis):\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # plot energy landscape background\n",
    "    fig.add_trace(go.Scattergl(\n",
    "            x=df[\"{} 1\".format(vis)], \n",
    "            y=df[\"{} 2\".format(vis)], \n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                sizemode='area',\n",
    "                # size=4,\n",
    "                size=df[\"HT\"],\n",
    "                sizeref=1e-5,\n",
    "                color=df[\"Energy\"],\n",
    "                colorscale=\"Plasma\",\n",
    "                showscale=True,\n",
    "                # colorbar_x=-0.2,\n",
    "                colorbar=dict(\n",
    "                    title=\"Free energy (kcal/mol)\",  \n",
    "                    x=-0.2,\n",
    "                    titleside=\"top\",  \n",
    "                    len=1.065,\n",
    "                    y=0.5,\n",
    "                ),\n",
    "                line=dict(width=0),\n",
    "            ),\n",
    "            text=df['DP'],\n",
    "            customdata=np.stack((df[\"Type\"],\n",
    "                        ),axis=-1),\n",
    "            hovertemplate=\n",
    "                \"DP notation: <br> <b>%{text}</b><br>\" +\n",
    "                \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "                \"Energy:  %{marker.color:.3f} kcal/mol<br>\"+\n",
    "                \"Expected holding time:  %{marker.size:.3e} s<br>\"+\n",
    "                \"Type: %{customdata[0]}\",\n",
    "            name=\"Energy landscape\",\n",
    "            # showlegend=False,\n",
    "            visible='legendonly',\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # plot type background\n",
    "    # Define a color mapping for each unique value in the \"Types\" column\n",
    "    color_mapping = {\n",
    "        \"000\": \"green\",\n",
    "        \"00H\": \"blue\",\n",
    "        \"0M0\": \"pink\",\n",
    "        \"0MH\": \"purple\",\n",
    "        \"SMH\": \"red\",\n",
    "        \"SM0\": \"orange\",\n",
    "        \"S0H\": \"yellow\",\n",
    "        \"S00\": \"grey\",\n",
    "    }\n",
    "    fig.add_trace(go.Scattergl(\n",
    "            x=df[\"{} 1\".format(vis)], \n",
    "            y=df[\"{} 2\".format(vis)], \n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                sizemode='area',\n",
    "                size=4,\n",
    "                color=[color_mapping[type_val] for type_val in df[\"Type\"]],\n",
    "                showscale=False,\n",
    "                line=dict(width=0),\n",
    "                ),\n",
    "            text=df['DP'],\n",
    "            customdata=np.stack((df[\"HT_cumu\"],\n",
    "                                 df[\"Energy\"],\n",
    "                                 df[\"Type\"],\n",
    "                                 df[\"Account_cumu\"],\n",
    "                                     ),axis=-1),\n",
    "            hovertemplate=\n",
    "                \"DP notation: <br> <b>%{text}</b><br>\" +\n",
    "                \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "                \"Energy:  %{customdata[1]:.3f} kcal/mol<br>\"+\n",
    "                \"Cumulative holding time:  %{customdata[0]:.3e} s<br>\"+\n",
    "                \"Appearance frequency:  %{customdata[3]:d} <br>\"+\n",
    "                \"Type: %{customdata[2]}\",\n",
    "            name=\"Type\",\n",
    "            # showlegend=False,\n",
    "            visible='legendonly',\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    # plot type with cumu_time size as background\n",
    "    fig.add_trace(go.Scattergl(\n",
    "            x=df[\"{} 1\".format(vis)], \n",
    "            y=df[\"{} 2\".format(vis)], \n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                sizemode='area',\n",
    "                # size=df[\"HT_cumu_log\"],\n",
    "                size=df[\"HT_cumu\"],\n",
    "                sizeref=5e-5, #1.3 for log\n",
    "                color=[color_mapping[type_val] for type_val in df[\"Type\"]],\n",
    "                # colorscale=[[type_val, color_mapping[type_val]] for type_val in df[\"Type\"].unique()],\n",
    "                showscale=False,\n",
    "                line=dict(width=0),\n",
    "                ),\n",
    "            text=df['DP'],\n",
    "            customdata=np.stack((df[\"HT_cumu\"],\n",
    "                                 df[\"Energy\"],\n",
    "                                 df[\"Type\"],\n",
    "                                 df[\"Account_cumu\"],\n",
    "                                     ),axis=-1),\n",
    "            hovertemplate=\n",
    "                \"DP notation: <br> <b>%{text}</b><br>\" +\n",
    "                \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "                \"Energy:  %{customdata[1]:.3f} kcal/mol<br>\"+\n",
    "                \"Cumulative holding time:  %{customdata[0]:.3e} s<br>\"+\n",
    "                \"Appearance frequency:  %{customdata[3]:d} <br>\"+\n",
    "                \"Type: %{customdata[2]}\",\n",
    "            name=\"Type-cumuTime\",\n",
    "            # showlegend=False,\n",
    "            visible='legendonly',\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    # plot type with cumu_count size as background\n",
    "    fig.add_trace(go.Scattergl(\n",
    "            x=df[\"{} 1\".format(vis)], \n",
    "            y=df[\"{} 2\".format(vis)], \n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                sizemode='area',\n",
    "                # size=df[\"Account_cumu_log\"],\n",
    "                size=df[\"Account_cumu\"],\n",
    "                sizeref=1,\n",
    "                color=[color_mapping[type_val] for type_val in df[\"Type\"]],\n",
    "                # colorscale=[[type_val, color_mapping[type_val]] for type_val in df[\"Type\"].unique()],\n",
    "                showscale=False,\n",
    "                line=dict(width=0),\n",
    "                ),\n",
    "            text=df['DP'],\n",
    "            customdata=np.stack((df[\"HT_cumu\"],\n",
    "                                 df[\"Energy\"],\n",
    "                                 df[\"Type\"],\n",
    "                                 df[\"Account_cumu\"],\n",
    "                                     ),axis=-1),\n",
    "            hovertemplate=\n",
    "                \"DP notation: <br> <b>%{text}</b><br>\" +\n",
    "                \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "                \"Energy:  %{customdata[1]:.3f} kcal/mol<br>\"+\n",
    "                \"Cumulative holding time:  %{customdata[0]:.3e} s<br>\"+\n",
    "                \"Appearance frequency:  %{customdata[3]:d} <br>\"+\n",
    "                \"Type: %{customdata[2]}\",\n",
    "            name=\"Type-freq\",\n",
    "            # showlegend=False,\n",
    "            visible='legendonly',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # plot bounded/unbounded background\n",
    "    fig.add_trace(go.Scattergl(\n",
    "            x=df[\"{} 1\".format(vis)], \n",
    "            y=df[\"{} 2\".format(vis)], \n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                sizemode='area',\n",
    "                size=4,\n",
    "                color=df[\"Pair\"],\n",
    "                showscale=True,\n",
    "                # colorbar_x=-0.2,\n",
    "                colorbar=dict(\n",
    "                    title=\"Free energy (kcal/mol)\",  \n",
    "                    x=-0.2,\n",
    "                    titleside=\"top\",  \n",
    "                    len=1.065,\n",
    "                    y=0.5,\n",
    "                ),\n",
    "                line=dict(width=0),\n",
    "            ),\n",
    "            text=df['DP'],\n",
    "            customdata=np.stack((df[\"HT_cumu\"],\n",
    "                                 df[\"Energy\"],\n",
    "                                 df[\"Type\"],\n",
    "                                 df[\"Account_cumu\"],\n",
    "                                     ),axis=-1),\n",
    "            hovertemplate=\n",
    "                \"DP notation: <br> <b>%{text}</b><br>\" +\n",
    "                \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "                \"Energy:  %{customdata[1]:.3f} kcal/mol<br>\"+\n",
    "                \"Cumulative holding time:  %{customdata[0]:.3e} s<br>\"+\n",
    "                \"Appearance frequency:  %{customdata[3]:d} <br>\"+\n",
    "                \"Type: %{customdata[2]}\",          \n",
    "            name=\"Bound/Unbound\",\n",
    "            # showlegend=False,\n",
    "            visible='legendonly',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # plot background\n",
    "    fig.add_trace(go.Scattergl(\n",
    "            x=df[\"{} 1\".format(vis)], \n",
    "            y=df[\"{} 2\".format(vis)], \n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                sizemode='area',\n",
    "                size=4,\n",
    "                color='lightgrey',\n",
    "                showscale=True,\n",
    "                line=dict(width=0),\n",
    "            ),\n",
    "            text=df['DP'],\n",
    "            hovertemplate= \"DP notation: <br> <b>%{text}</b><br>\",\n",
    "            name=\"Background\",\n",
    "            visible='legendonly',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # layout successful trajectory\n",
    "    # for i in range(len(dfsucc)):\n",
    "    idx=0\n",
    "    colors=[\"darkorchid\", \"goldenrod\", \"black\"]\n",
    "    for i in [0, 47, 49]:\n",
    "        Step = []\n",
    "        if len(dfsucc[\"DP\"][i]) < 2000:\n",
    "            Step = np.arange(len(dfsucc[\"DP\"][i]))\n",
    "        else:\n",
    "            Step = np.full(len(dfsucc[\"DP\"][i]), None, dtype=object)  \n",
    "        fig.add_trace(\n",
    "            go.Scattergl(\n",
    "                x=dfsucc[f\"{vis}\"][i][:,0],\n",
    "                y=dfsucc[f\"{vis}\"][i][:,1],\n",
    "                mode='lines+markers',\n",
    "                # line=dict(\n",
    "                #     color=\"rgba(0, 0, 0, 0.1)\",\n",
    "                #     width=0.3,\n",
    "                # ),\n",
    "                line=dict(\n",
    "                    color=colors[idx],\n",
    "                    width=2,\n",
    "                ),\n",
    "                marker=dict(\n",
    "                    sizemode='area',\n",
    "                    size=5,\n",
    "                    # size=dfsucc[\"HT\"][i],\n",
    "                    # sizeref=5e-6,\n",
    "                    # color=[color_mapping[type_val] for type_val in dfsucc[\"Type\"][i]],\n",
    "                    color=[color_mapping[type_val] for type_val in dfsucc[\"Type\"][i]],\n",
    "                    opacity=1,\n",
    "                    # showscale=False,\n",
    "                    colorbar=dict(\n",
    "                        x=-0.2,\n",
    "                        y=0.5,\n",
    "                        tickvals=[],\n",
    "                        len=1,\n",
    "                    ),\n",
    "                    line=dict(width=0),\n",
    "                ),            \n",
    "                text=Step,\n",
    "                customdata=np.stack((dfsucc['Energy'][i],\n",
    "                                     dfsucc['TotalT'][i],\n",
    "                                     dfsucc['DP'][i],\n",
    "                                     dfsucc['Type'][i],\n",
    "                                     dfsucc['HT_cumu'][i],\n",
    "                                     dfsucc['Account_cumu'][i],\n",
    "                                     ),axis=-1),\n",
    "                hovertemplate=\n",
    "                    \"Step:  <b>%{text}</b><br><br>\"+\n",
    "                    \"DP notation: <br> <b>%{customdata[2]}</b><br>\" +\n",
    "                    \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "                    \"Energy:  %{customdata[0]} kcal/mol<br>\"+\n",
    "                    \"Holding time for last step:  %{marker.size:.3e} s<br>\"+\n",
    "                    \"Total time until current state:  %{customdata[1]:.3e} s<br>\"+\n",
    "                    # \"Cumulative holding time:  %{customdata[4]:.3e} s<br>\"+\n",
    "                    # \"Appearance frequency:  %{customdata[5]:d} <br>\"+\n",
    "                    \"Type: %{customdata[3]}\", \n",
    "                visible='legendonly',\n",
    "                name = \"Succ {}\".format(dfsucc[\"IDX\"][i]+1),\n",
    "            )\n",
    "        )\n",
    "        idx+=1\n",
    "        \n",
    "    # # layout failed trajectory on top of energy landscape\n",
    "    # for i in range(len(dffail)):\n",
    "    #     Step = []\n",
    "    #     if len(dffail[\"DP\"][i]) < 2000:\n",
    "    #         Step = np.arange(len(dffail[\"DP\"][i]))\n",
    "    #     else:\n",
    "    #         Step = np.full(len(dffail[\"DP\"][i]), None, dtype=object)\n",
    "    #     fig.add_trace(\n",
    "    #         go.Scattergl(\n",
    "    #             x=dffail[f\"{vis}\"][i][:,0],\n",
    "    #             y=dffail[f\"{vis}\"][i][:,1],\n",
    "    #             mode='lines+markers',\n",
    "    #             line=dict(\n",
    "    #                 color=\"rgba(0, 0, 0, 0.5)\",\n",
    "    #                 width=0.5,\n",
    "    #             ),\n",
    "    #             marker=dict(\n",
    "    #                 sizemode='area',\n",
    "    #                 # size=6,\n",
    "    #                 size=dffail[\"TotalT\"][i],\n",
    "    #                 sizeref=5e-3,\n",
    "    #                 color=[color_mapping[type_val] for type_val in dffail[\"Type\"][i]],\n",
    "    #                 opacity=1,\n",
    "    #                 # showscale=False,\n",
    "    #                 colorbar=dict(\n",
    "    #                     x=-0.2,\n",
    "    #                     y=0.5,\n",
    "    #                     tickvals=[],\n",
    "    #                     len=1,\n",
    "    #                 ),\n",
    "    #                 line=dict(width=0),\n",
    "    #             ),    \n",
    "    #             text=Step,\n",
    "    #             customdata=np.stack((dffail['Energy'][i],\n",
    "    #                                  dffail['TotalT'][i],\n",
    "    #                                  dffail['DP'][i],\n",
    "    #                                  dffail['Type'][i],\n",
    "    #                                  dffail['HT_cumu'][i],\n",
    "    #                                  dffail['Account_cumu'][i],\n",
    "    #                                  ),axis=-1),\n",
    "    #             hovertemplate=\n",
    "    #                 \"Step:  <b>%{text}</b><br><br>\"+\n",
    "    #                 \"DP notation: <br> <b>%{customdata[2]}</b><br>\" +\n",
    "    #                 \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "    #                 \"Energy:  %{customdata[0]} kcal/mol<br>\"+\n",
    "    #                 \"Holding time for last step:  %{marker.size:.3e} s<br>\"+\n",
    "    #                 \"Total time until current state:  %{customdata[1]:.3e} s<br>\"+\n",
    "    #                 # \"Cumulative holding time:  %{customdata[4]:.3e} s<br>\"+\n",
    "    #                 # \"Appearance frequency:  %{customdata[5]:d} <br>\"+\n",
    "    #                 \"Type: %{customdata[3]}\",\n",
    "    #             visible='legendonly',\n",
    "    #             name = \"Fail {}\".format(dffail[\"IDX\"][i]+1),\n",
    "    #         )\n",
    "    #     )\n",
    "    \n",
    "           \n",
    "    # label final states\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            x=[dfsucc[f\"{vis}\"][0][-1,0]],\n",
    "            y=[dfsucc[f\"{vis}\"][0][-1,1]],\n",
    "            mode='markers+text',\n",
    "            marker_color=\"lime\", \n",
    "            # marker_size=15,\n",
    "            marker_size=20,\n",
    "            text=[\"F\"],\n",
    "            textposition=\"middle center\",\n",
    "            textfont=dict(\n",
    "            family=\"sans serif\",\n",
    "            size=16,\n",
    "            color=\"black\"\n",
    "        ),\n",
    "            hoverinfo='skip',\n",
    "            showlegend=False,\n",
    "                        )\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(\n",
    "        range=[min(df[\"{} 1\".format(vis)])*1.1,max(df[\"{} 1\".format(vis)])*1.1]\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        range=[min(df[\"{} 2\".format(vis)])*1.1,max(df[\"{} 2\".format(vis)])*1.1]\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        # autosize=True,\n",
    "        # width=700,\n",
    "        # height=700,\n",
    "        # margin=dict(\n",
    "        #     l=50,\n",
    "        #     r=50,\n",
    "        #     b=100,\n",
    "        #     t=100,\n",
    "        #     pad=4\n",
    "        # ),\n",
    "        title=\"{}: {} Vis\".format(rxn_name,vis),\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(\n",
    "                title=\"{} 1\".format(vis),\n",
    "            ),\n",
    "        yaxis=dict(\n",
    "                title=\"{} 2\".format(vis),\n",
    "            ),\n",
    "        legend=dict(\n",
    "            # title=\"Single Trajectory\",\n",
    "            title_font=dict(size=10),\n",
    "            font=dict(\n",
    "                # family=\"Courier\",\n",
    "                size=10,\n",
    "                color=\"black\"\n",
    "        )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIS_METHOD = [\"PCA\", \"PHATE\"]\n",
    "VIS_METHOD = [\"PHATE\"]\n",
    "\n",
    "for vis in VIS_METHOD: \n",
    "    fig = interactive_plotly_2D(df,dfsucc,dffail,rxn_name,vis)\n",
    "    # pio.write_html(fig, file=f\"./data/jordan/plots/FirstStep_{vis}_{rxn_name}_{config.log_dir}.html\", auto_open=True)\n",
    "    # pio.write_html(fig, file=f\"./data/jordan/plots/FirstStep_{vis}_{rxn_name}_0825-2043.html\", auto_open=True)\n",
    "    pio.write_html(fig, file=f\"../output_files/saved_ViDa_plots/plot_dna29/FirstStep_{vis}_{rxn_name}_0825-2043_trj.html\", auto_open=True)\n",
    "    print(\"DONE: \", vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering to Find Kinetic Traps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out data points with low frequency\n",
    "filter_threshold = 0.1 # reaction 8\n",
    "\n",
    "filter_idx = np.where(P_tot>=filter_threshold)[0]\n",
    "filter_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load PCA for DBSCAN\n",
    "filter_comb_pca_coords = pca_coords[filter_idx]\n",
    "filter_comb_pca_coords.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elbow method to find eps for DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "n_neighbors = 4  # Number of neighbors to find\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(filter_comb_pca_coords)\n",
    "distances, indices = nbrs.kneighbors(filter_comb_pca_coords)\n",
    "four_dist = np.sum(distances,axis=1)\n",
    "sorted_four_dist = np.sort(four_dist)[::-1]\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "# Add a line trace\n",
    "fig.add_trace(go.Scatter(x=indices[:,0], y=sorted_four_dist, \n",
    "                         mode='lines', name='Line Plot'))\n",
    "# Set labels and title\n",
    "fig.update_layout(xaxis_title='points', yaxis_title='4-dist', title='Elbow')\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# 0.71\n",
    "\n",
    "X = filter_comb_pca_coords\n",
    "clusters = DBSCAN(eps = 2.2, min_samples = 4).fit(X)\n",
    "# get cluster labels\n",
    "labels = clusters.labels_\n",
    "\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "# # check unique clusters\n",
    "set(clusters.labels_)\n",
    "# # -1 value represents noisy points could not assigned to any cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove no trap clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels = labels.copy()\n",
    "for k_clust in np.unique(labels):\n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(labels==k_clust)[0]])\n",
    "    # print(\"For cluster {}:\".format(k_clust))\n",
    "    plausible_trap = SIMS_dp_og_uniq[filter_idx][np.where(labels==k_clust)[0]][min_index]\n",
    "    if \"(\"*10 in plausible_trap:\n",
    "        real_labels = [-1 if x==k_clust else x for x in real_labels]\n",
    "        print(\"Cluster {} is NOT a trap\".format(k_clust))\n",
    "    else:\n",
    "        print(\"Cluster {} is a trap\".format(k_clust))\n",
    "        \n",
    "real_labels = np.array(real_labels)\n",
    "\n",
    "print(\"\\nClusters with trap are: {}\".format(np.unique(real_labels)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = filter_comb_pca_coords[:,0]\n",
    "Y = filter_comb_pca_coords[:,1]\n",
    "clusters = real_labels  # Cluster real labels\n",
    "\n",
    "# Get unique cluster labels\n",
    "unique_clusters = np.unique(clusters)\n",
    "\n",
    "# Define colors for each cluster\n",
    "noise  = 'grey'\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'cyan', 'magenta', 'lime', 'teal']  # Add more colors as needed\n",
    "name_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "trap_shape = ['star', 'x', 'triangle-up', 'cross', 'pentagon', 'diamond', 'square', 'triangle-down', 'triangle-left', 'triangle-right']\n",
    "\n",
    "# Create a scatter trace for each cluster\n",
    "traces = []\n",
    "i = 0\n",
    "for cluster_label in unique_clusters:\n",
    "    mask = clusters == cluster_label\n",
    "    if cluster_label == -1:\n",
    "        # Assign a color for cluster label -1\n",
    "        color = noise\n",
    "        # name = 'Cluster -1'\n",
    "        name = 'Noise'\n",
    "        \n",
    "    else:\n",
    "        # Assign a color for other cluster labels\n",
    "        color = colors[cluster_label]\n",
    "        name = f'Cluster {name_list[i]}'\n",
    "        i += 1\n",
    "    \n",
    "    trace = go.Scattergl(\n",
    "        x=X[mask],\n",
    "        y=Y[mask],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "            # size = SIMS_HT_uniq[filter_idx][mask],\n",
    "            # sizeref=3e-6,\n",
    "            size=5,\n",
    "            sizemode='diameter',\n",
    "            ),\n",
    "        name=name,\n",
    "        showlegend=True,\n",
    "        \n",
    "        customdata = np.stack((SIMS_G_uniq[filter_idx][mask],\n",
    "                           SIMS_HT_uniq[filter_idx][mask],\n",
    "                           P_tot[filter_idx][mask],\n",
    "                           ),axis=-1),\n",
    "        text = SIMS_dp_og_uniq[filter_idx][mask],\n",
    "        hovertemplate=\n",
    "            \"X: %{x}   \" + \"   Y: %{y} <br>\"+\n",
    "            \"DP notation: <br> <b>%{text}</b><br>\" +  \n",
    "            \"Energy:  %{customdata[0]:.3f} kcal/mol<br>\"+\n",
    "            \"Average holding time:  %{customdata[1]:.5g} s<br>\"+\n",
    "            \"Probability:  %{customdata[2]:.2g} <br>\",\n",
    "    )\n",
    "\n",
    "    traces.append(trace)\n",
    "\n",
    "# label final states\n",
    "final_idx = np.where(SIMS_dp_og_uniq == final_states)[0][0]\n",
    "trace = go.Scattergl(\n",
    "        x = [pca_coords[final_idx][0]],\n",
    "        y = [pca_coords[final_idx][1]],\n",
    "        mode='markers+text',\n",
    "        \n",
    "        marker_color=\"lime\", \n",
    "        marker_size=12,\n",
    "        text=[\"F\"],\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(\n",
    "        family=\"sans serif\",\n",
    "        size=10,\n",
    "        color=\"black\"\n",
    "    ),\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False,\n",
    "                    )\n",
    "\n",
    "traces.append(trace)\n",
    "\n",
    "# label kinetic traps\n",
    "i = 0\n",
    "for k_clust in np.unique(clusters):\n",
    "    if k_clust == -1:\n",
    "        continue\n",
    "    \n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(clusters==k_clust)[0]])\n",
    "    trace = go.Scattergl(\n",
    "        x = np.array(X[np.where(clusters==k_clust)[0]][min_index]),\n",
    "        y = np.array(Y[np.where(clusters==k_clust)[0]][min_index]),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=\"black\",\n",
    "            symbol=trap_shape[i],\n",
    "            size=10,\n",
    "        ),\n",
    "        name = f\"Trap {name_list[i]}\",\n",
    "        showlegend=True,\n",
    "    )\n",
    "    i += 1\n",
    "    traces.append(trace)\n",
    "\n",
    "# legend setting\n",
    "layout = go.Layout(\n",
    "    legend=dict(\n",
    "        # x=0.5,  # Adjust the x position of the legend\n",
    "        # y=0.5,  # Adjust the y position of the legend\n",
    "        # font=dict(\n",
    "        #     size=10  # Adjust the font size of the legend\n",
    "        # ),\n",
    "        itemsizing='constant',\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        range = [min(X)*1.1,max(X)*1.1],\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range = [min(Y)*1.1,max(Y)*1.1],\n",
    "    ),\n",
    "    title=f\"DBSCAN finding Kinetic Traps for sample {rxn_name}\",\n",
    ")\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the kinetic trap in each cluster\n",
    "for k_clust in np.unique(real_labels):\n",
    "    if k_clust == -1:\n",
    "        continue\n",
    "    min_index = np.argmin(SIMS_G_uniq[filter_idx][np.where(labels==k_clust)[0]])\n",
    "    print(\"Kinetic trap in cluster {} is:\".format(k_clust))\n",
    "    print(SIMS_dp_og_uniq[filter_idx][np.where(labels==k_clust)[0]][min_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exact each trajectory\n",
    "split_id = trj_id + 1 # index for split to each trajectory\n",
    "traj_in_clust = np.zeros(len(np.unique(labels)), dtype=int)\n",
    "avg_time_in_clust = np.zeros(len(np.unique(labels)), dtype=float)\n",
    "\n",
    "for i in range(len(split_id)):\n",
    "    if i == 0:\n",
    "        trj_dp = SIMS_dp_og[0:split_id[i]]\n",
    "    else:\n",
    "        trj_dp = SIMS_dp_og[split_id[i-1]:split_id[i]]\n",
    "\n",
    "    for j, k_clust in enumerate(np.unique(labels)):\n",
    "        mask = labels == k_clust\n",
    "        if np.size(np.intersect1d(trj_dp, SIMS_dp_og_uniq[filter_idx][mask])) != 0:\n",
    "            traj_in_clust[j] += 1\n",
    "            avg_time_in_clust[j] += SIMS_T[trj_id[i]]\n",
    "\n",
    "print(f\"{rxn_name}:\")\n",
    "for i in range(len(traj_in_clust)):\n",
    "    print(\"{} trajs in cluster {}. Average time: {:.3e}.\".format(traj_in_clust[i], np.unique(labels)[i], avg_time_in_clust[i]/traj_in_clust[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_scar_uniq[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMS_dp_og_uniq[:3], SIMS_dp_og_uniq[-3:], SIMS_dp_og_uniq[1000], SIMS_dp_og_uniq[1050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse01 = mean_squared_error(SIMS_scar_uniq[0], SIMS_scar_uniq[1])\n",
    "print(\"Mean Squared Error:\", mse01)\n",
    "\n",
    "mse02 = mean_squared_error(SIMS_scar_uniq[0], SIMS_scar_uniq[2])\n",
    "print(\"Mean Squared Error:\", mse02)\n",
    "\n",
    "mse12 = mean_squared_error(SIMS_scar_uniq[1], SIMS_scar_uniq[2])\n",
    "print(\"Mean Squared Error:\", mse12)\n",
    "\n",
    "mse01000 = mean_squared_error(SIMS_scar_uniq[0], SIMS_scar_uniq[1000])\n",
    "print(\"Mean Squared Error:\", mse01000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse0m1 = mean_squared_error(SIMS_scar_uniq[0], SIMS_scar_uniq[-1])\n",
    "print(\"Mean Squared Error:\", mse0m1)\n",
    "\n",
    "msem1m2 = mean_squared_error(SIMS_scar_uniq[-1], SIMS_scar_uniq[-2])\n",
    "print(\"Mean Squared Error:\", msem1m2)\n",
    "\n",
    "mse0m2 = mean_squared_error(SIMS_scar_uniq[0], SIMS_scar_uniq[-2])\n",
    "print(\"Mean Squared Error:\", mse0m2)\n",
    "\n",
    "mse0m3 = mean_squared_error(SIMS_scar_uniq[0], SIMS_scar_uniq[-3])\n",
    "print(\"Mean Squared Error:\", mse0m3)\n",
    "\n",
    "mse10001050 = mean_squared_error(SIMS_scar_uniq[1000], SIMS_scar_uniq[1050])\n",
    "print(\"Mean Squared Error:\", mse10001050)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vida')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e442af4fad2330d8f4febe7e8e7250535e161341429a4f0b93cbf21b824330cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
